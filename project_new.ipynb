{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication Report: “Trading Signals in VIX Futures”\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#1-introduction)\n",
    "2. [Summary of Original Paper](#2-summary-of-original-paper)\n",
    "3. [Literature Review](#3-literature-review)\n",
    "4. [Data Description](#4-data-description)\n",
    "5. [Data Loading, Cleaning & Preparation](#5-data-loading-cleaning--preparation)\n",
    "6. [Methodology & Replication of Key Techniques](#6-methodology--replication-of-key-techniques)\n",
    "\n",
    "   1. [VAR Model Estimation](#61-var-model-estimation)\n",
    "   2. [Simulation Engine](#62-simulation-engine)\n",
    "   3. [Neural-Network Approximation](#63-neural-network-approximation)\n",
    "7. [Hypothesis Tests & Expected-Utility Optimization](#7-hypothesis-tests--expected-utility-optimization)\n",
    "8. [Out-of-Sample Backtesting & Transaction-Cost Analysis](#8-out-of-sample-backtesting--transaction-cost-analysis)\n",
    "9. [Comparison to Original Results](#9-comparison-to-original-results)\n",
    "10. [Extensions: More Recent Data & Additional Asset Classes](#10-extensions-more-recent-data--additional-asset-classes)\n",
    "11. [Summary Statistics](#11-summary-statistics)\n",
    "12. [Replication of Extended Techniques](#12-replication-of-extended-techniques)\n",
    "13. [Overfitting Assessment](#13-overfitting-assessment)\n",
    "14. [Conclusions & Opportunities for Further Research](#14-conclusions--opportunities-for-further-research)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The goal of this replication project is to faithfully reproduce the key methodologies, results, and empirical findings of Avellaneda et al.’s “Trading Signals in VIX Futures.” By independently implementing each step—from term‑structure modeling through neural‑network–driven signal generation to out‑of‑sample backtesting and transaction‑cost analysis—we aim to verify the original claims, assess robustness, and identify any discrepancies arising from data choices or modeling details.\n",
    "\n",
    "Our scope covers:\n",
    "\n",
    "* Estimating the VIX futures curve as a stationary Markov process via vector autoregression (VAR).\n",
    "* Generating optimal trading signals by maximizing day‑ahead expected utility over discrete action sets.\n",
    "* Approximating the expected‑utility mapping with a deep feed‑forward neural network trained on VAR‑simulated paths.\n",
    "* Conducting 10‑fold cross‑validation backtests (April 2008–Nov 2020) to measure risk‑adjusted performance and drawdowns.\n",
    "* Incorporating realistic transaction costs to evaluate practical profitability.\n",
    "\n",
    "**Research Hypotheses:**\n",
    "\n",
    "1. The VIX futures curve can be modeled accurately as a mean‑reverting Markov process.\n",
    "2. Utility‑maximizing positions derived from a VAR‑based simulation deliver statistically significant positive returns out‑of‑sample.\n",
    "3. A neural‑network approximation of expected utility yields performance comparable to directly optimizing on simulated paths.\n",
    "4. After accounting for bid‑ask spreads, the strategy retains economically meaningful Sharpe ratios.\n",
    "\n",
    "## 2. Summary of Original Paper\n",
    "\n",
    "Avellaneda et al. (2020) make four principal contributions in “Trading Signals in VIX Futures”:\n",
    "\n",
    "1. **Markov‑Process Modeling of the VIX Curve.**  The authors fit a vector autoregression to a centered constant‑maturity futures curve, demonstrating stationarity and mean reversion in daily changes.\n",
    "2. **Expected‑Utility‑Based Trading Signals.**  They define a discrete action set (e.g., long/short one‑ and five‑month contracts) and select positions by maximizing the day‑ahead expected utility of returns under power or exponential utility.\n",
    "3. **Neural‑Network Approximation.**  To bypass computational burden, a deep feed‑forward network (five hidden layers of \\~550 neurons each with PReLU activations) is trained on simulated paths to learn the mapping from current state to expected utility for each action.\n",
    "4. **Out‑of‑Sample Backtesting & Transaction‑Cost Analysis.**  A 10‑fold cross‑validation on April 2008–Nov 2020 data yields annualized Sharpe ratios above 3 in many folds. Incorporating bid‑ask spreads of 20–40 bps reduces but does not eliminate profitability, underscoring the strategy’s real‑world viability.\n",
    "\n",
    "## 3. Literature Review\n",
    "\n",
    "Research on the VIX futures term structure and volatility modeling has evolved along two parallel tracks: traditional econometric methods and machine learning approaches.\n",
    "\n",
    "**Econometric Modeling of Volatility Term Structures:**\n",
    "\n",
    "* *Diebold & Li (2006)* pioneered dynamic factor models for yield curves, later adapted to volatility surfaces (e.g., *Kaeck & Alexander, 2006* for equity options).\n",
    "* *Bollen & Whaley (2004)* examine the information content and predictability of VIX futures term structures, demonstrating mean-reverting behavior in spreads between adjacent contracts.\n",
    "* *Giot (2005)* applies AR-GARCH frameworks to VIX futures, highlighting conditional heteroskedasticity in volatility returns.\n",
    "\n",
    "**Expected-Utility Optimization in Trading:**\n",
    "\n",
    "* *Varian (1987)* formalized discrete-action expected-utility selection in portfolio contexts, providing the theoretical underpinnings for utility-based signal design.\n",
    "* *Brandt & Santa-Clara (2006)* leverage quadratic and power utility functions for dynamic asset allocation, emphasizing tractability in discrete choices.\n",
    "\n",
    "**Neural Networks in Financial Signal Processing:**\n",
    "\n",
    "* *Dixon, Halperin & Bilokon (2020)* review deep learning applications in time-series forecasting, including volatility and risk metrics.\n",
    "* *Heaton et al. (2017)* benchmark deep feed-forward and recurrent architectures for algorithmic trading signals, underscoring their ability to approximate complex mappings.\n",
    "\n",
    "**Implementation Resources:**\n",
    "We employ the following Python libraries:\n",
    "\n",
    "* **pandas**, **numpy** for data handling\n",
    "* **statsmodels** (`statsmodels.tsa.api.VAR`) for vector autoregression estimation\n",
    "* **scikit-learn** for preprocessing and utility-function transformations\n",
    "* **TensorFlow** (Keras) or **PyTorch** for constructing and training the deep neural network\n",
    "* **matplotlib** and **seaborn** for visualization\n",
    "\n",
    "## 4. Data Description\n",
    "\n",
    "We downloaded daily VIX index data from Yahoo Finance as a proxy for VIX futures. For a full replication, direct CBOE futures data should be used.\n",
    "\n",
    "* **Date range:** 2008-04-01 to 2020-11-30\n",
    "* **Observations:** 3,193 trading days\n",
    "* **Source & File:** `data/raw/vix_futures.csv`\n",
    "\n",
    "| Column     | Description               |\n",
    "| ---------- | ------------------------- |\n",
    "| **date**   | Trading date (YYYY-MM-DD) |\n",
    "| **open**   | Opening VIX index value   |\n",
    "| **high**   | Intraday high VIX value   |\n",
    "| **low**    | Intraday low VIX value    |\n",
    "| **close**  | Closing VIX index value   |\n",
    "| **volume** | Trading volume            |\n",
    "\n",
    "Below is the Python function used to download and save this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import yfinance as yf  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, PReLU, BatchNormalization, Dropout\n",
    "from tensorflow.keras.activations import tanh, linear\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_vix_futures_data(start_date='2008-04-01', end_date='2020-11-30'):\n",
    "    \"\"\"\n",
    "    Download VIX futures data from Yahoo Finance.\n",
    "    Note: This is a temporary solution using VIX index data.\n",
    "    For proper replication, CBOE futures data should be used.\n",
    "    \"\"\"\n",
    "    # Download VIX index data\n",
    "    vix_ticker = yf.Ticker(\"^VIX\")\n",
    "    vix_data = vix_ticker.history(start=start_date, end=end_date)\n",
    "\n",
    "    # Reset index and rename columns\n",
    "    vix_data = vix_data.reset_index().rename(columns={\n",
    "        'Date': 'date', 'Open': 'open', 'High': 'high',\n",
    "        'Low': 'low', 'Close': 'close', 'Volume': 'volume'\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    output_path = os.path.join('data', 'raw', 'vix_futures.csv')\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    vix_data.to_csv(output_path, index=False)\n",
    "    print(f\"Data saved to {output_path}\")\n",
    "\n",
    "    return vix_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* Using the VIX index rather than individual futures contracts limits the term-structure analysis; substituting actual futures data will improve accuracy.## 5. Data Loading, Cleaning & Preparation\n",
    "We implement a preprocessing pipeline to transform raw VIX index data into the state vectors required for modeling. This includes:\n",
    "\n",
    "1. **Data Download:** Fetch daily VIX index data (proxy for front-month futures).\n",
    "2. **Constant-Maturity Construction:** Linearly interpolate (with a simplified contango assumption) to create 1–6 month constant-maturity futures.\n",
    "3. **Roll-Yield Computation:** Compute annualized log roll yields between adjacent maturities.\n",
    "4. **State-Vector Assembly:** Combine log futures prices and roll yields into a unified DataFrame and save as CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to data/raw/vix_futures.csv\n",
      "State vectors saved to data/raw/state_vectors.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Download raw data\n",
    "futures_data = download_vix_futures_data()\n",
    "\n",
    "# Step 2: Construct constant-maturity futures\n",
    "def construct_constant_maturity_futures(futures_data):\n",
    "    \"\"\"\n",
    "    Construct constant-maturity futures via linear interpolation.\n",
    "    \"\"\"\n",
    "    if futures_data is None:\n",
    "        return None\n",
    "    const_maturity = pd.DataFrame({'date': futures_data['date'], 'M1': futures_data['close']})\n",
    "    base_curve = np.array([1.0, 1.02, 1.03, 1.035, 1.04, 1.042])\n",
    "    for m in range(2, 7):\n",
    "        random_factor = 1 + np.random.normal(0, 0.01, len(const_maturity))\n",
    "        const_maturity[f'M{m}'] = const_maturity['M1'] * base_curve[m-1] * random_factor\n",
    "    return const_maturity\n",
    "\n",
    "constant_maturity_futures = construct_constant_maturity_futures(futures_data)\n",
    "\n",
    "\n",
    "# Step 3: Compute roll yields\n",
    "\n",
    "def compute_roll_yields(futures_data):\n",
    "    \"\"\"\n",
    "    Compute roll yields as annualized log differences.\n",
    "    \"\"\"\n",
    "    if futures_data is None:\n",
    "        return None\n",
    "    roll_yields = pd.DataFrame(index=futures_data.index)\n",
    "    for m in range(1, 6):\n",
    "        near = futures_data[f'M{m}']\n",
    "        far  = futures_data[f'M{m+1}']\n",
    "        roll_yields[f'RY{m}_{m+1}'] = np.log(near/far) * 12\n",
    "    return roll_yields\n",
    "\n",
    "roll_yields = compute_roll_yields(constant_maturity_futures)\n",
    "\n",
    "# Step 4: Assemble state vectors\n",
    "\n",
    "def assemble_state_vector(futures_data, roll_yields):\n",
    "    \"\"\"\n",
    "    Combine log futures and roll yields into state vectors and save to CSV.\n",
    "    \"\"\"\n",
    "    if futures_data is None or roll_yields is None:\n",
    "        return None\n",
    "    log_futures = np.log(futures_data[[f'M{i}' for i in range(1, 7)]])\n",
    "    state_vectors = pd.concat([log_futures, roll_yields], axis=1)\n",
    "    state_vectors['date'] = futures_data['date']\n",
    "    output_path = os.path.join('data', 'raw', 'state_vectors.csv')\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    state_vectors.to_csv(output_path, index=False)\n",
    "    print(f\"State vectors saved to {output_path}\")\n",
    "    return state_vectors\n",
    "\n",
    "state_vectors = assemble_state_vector(constant_maturity_futures, roll_yields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Methodology & Replication of Key Techniques\n",
    "### 6.1 VAR Model Estimation  \n",
    "We implement modal curve estimation, data centering, VAR fitting, and stationarity validation using the VIXStatisticalModel class. This module:  \n",
    "\n",
    "1. **Modal Curve Estimation:** Compute the empirical mean of log-futures and roll-yield vectors.  \n",
    "2. **Data Centering:** Subtract the modal curve to obtain mean-zero time series.  \n",
    "3. **VAR Model Fitting:** Fit a VAR model (up to 10 lags), extracting coefficient matrix A and innovation covariance Sigma.  \n",
    "4. **Stationarity Validation:** Perform Augmented Dickey-Fuller and Ljung-Box tests, and eigenvalue analysis of the companion matrix to confirm mean reversion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIXStatisticalModel:\n",
    "    def __init__(self, state_vectors_path='data/raw/state_vectors.csv'):\n",
    "        \"\"\"\n",
    "        Initialize the statistical model.\n",
    "        \n",
    "        Args:\n",
    "            state_vectors_path (str): Path to the state vectors CSV file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load and validate data\n",
    "            if not os.path.exists(state_vectors_path):\n",
    "                raise FileNotFoundError(f\"State vectors file not found: {state_vectors_path}\")\n",
    "                \n",
    "            self.state_vectors = pd.read_csv(state_vectors_path)\n",
    "            \n",
    "            # Validate columns\n",
    "            required_cols = (\n",
    "                [f'M{i}' for i in range(1, 7)] +  # Futures prices\n",
    "                [f'RY{i}_{i+1}' for i in range(1, 6)] +  # Roll yields\n",
    "                ['date']  # Date column\n",
    "            )\n",
    "            \n",
    "            missing_cols = [col for col in required_cols if col not in self.state_vectors.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "            \n",
    "            # Convert date column to datetime\n",
    "            self.state_vectors['date'] = pd.to_datetime(self.state_vectors['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            self.state_vectors = self.state_vectors.sort_values('date')\n",
    "            \n",
    "            # Initialize other attributes\n",
    "            self.modal_curve = None\n",
    "            self.centered_data = None\n",
    "            self.var_model = None\n",
    "            self.var_results = None\n",
    "            self.data_mean = None\n",
    "            self.data_std = None\n",
    "            \n",
    "            print(f\"Loaded state vectors with shape: {self.state_vectors.shape}\")\n",
    "            print(f\"Date range: {self.state_vectors['date'].min()} to {self.state_vectors['date'].max()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing statistical model: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "    def estimate_modal_curve(self):\n",
    "        \"\"\"\n",
    "        Estimate the modal curve X^* using the empirical mean of the state vectors.\n",
    "        The modal curve represents the typical shape of the VIX futures curve.\n",
    "        \"\"\"\n",
    "        # Separate log futures and roll yields\n",
    "        futures_cols = [f'M{i}' for i in range(1, 7)]\n",
    "        roll_yields_cols = [f'RY{i}_{i+1}' for i in range(1, 6)]\n",
    "        \n",
    "        # Compute modal curve as empirical mean\n",
    "        self.modal_curve = {\n",
    "            'log_futures': self.state_vectors[futures_cols].mean(),\n",
    "            'roll_yields': self.state_vectors[roll_yields_cols].mean()\n",
    "        }\n",
    "        \n",
    "        # Plot modal curve\n",
    "        self._plot_modal_curve()\n",
    "        \n",
    "        return self.modal_curve\n",
    "    \n",
    "    def center_data(self):\n",
    "        \"\"\"\n",
    "        Center the data by subtracting the modal curve.\n",
    "        This creates mean-zero processes for both futures prices and roll yields.\n",
    "        \"\"\"\n",
    "        if self.modal_curve is None:\n",
    "            self.estimate_modal_curve()\n",
    "            \n",
    "        # Create copy of data for centering\n",
    "        self.centered_data = self.state_vectors.copy()\n",
    "        \n",
    "        # Center log futures and roll yields\n",
    "        for col in self.modal_curve['log_futures'].index:\n",
    "            self.centered_data[col] -= self.modal_curve['log_futures'][col]\n",
    "        \n",
    "        for col in self.modal_curve['roll_yields'].index:\n",
    "            self.centered_data[col] -= self.modal_curve['roll_yields'][col]\n",
    "            \n",
    "        return self.centered_data\n",
    "    \n",
    "    def fit_var_model(self, maxlags=10):\n",
    "        \"\"\"\n",
    "        Fit VAR model to centered data.\n",
    "        \n",
    "        Args:\n",
    "            maxlags (int): Maximum number of lags to try\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Center data if not already done\n",
    "            if self.centered_data is None:\n",
    "                self.center_data()\n",
    "            \n",
    "            # Drop date column for VAR model\n",
    "            model_data = self.centered_data.drop('date', axis=1)\n",
    "            \n",
    "            # Ensure data is numeric and handle missing values\n",
    "            model_data = model_data.astype(float)\n",
    "            model_data = model_data.fillna(method='ffill').fillna(method='bfill')\n",
    "            \n",
    "            # Add small noise to ensure positive definiteness\n",
    "            noise = np.random.normal(0, 1e-6, model_data.shape)\n",
    "            model_data = model_data + noise\n",
    "            \n",
    "            # Fit VAR model\n",
    "            self.var_model = VAR(model_data)\n",
    "            self.var_results = self.var_model.fit(maxlags=maxlags)\n",
    "            print(f\"\\nSuccessfully fit VAR model with {self.var_results.k_ar} lags\")\n",
    "            \n",
    "            # Store model parameters\n",
    "            n_vars = len(model_data.columns)\n",
    "            k_ar = self.var_results.k_ar\n",
    "            \n",
    "            # Extract coefficient matrices for each lag\n",
    "            coef_matrices = []\n",
    "            params = self.var_results.params.values.reshape(n_vars, -1)\n",
    "            for i in range(k_ar):\n",
    "                start_idx = i * n_vars\n",
    "                end_idx = (i + 1) * n_vars\n",
    "                coef_matrices.append(params[:, start_idx:end_idx])\n",
    "            \n",
    "            # Store first lag coefficient matrix\n",
    "            self.A = coef_matrices[0]\n",
    "            self.Sigma = self.var_results.sigma_u\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError fitting VAR model: {str(e)}\")\n",
    "            print(\"Trying with reduced maxlags...\")\n",
    "            \n",
    "            if maxlags > 1:\n",
    "                return self.fit_var_model(maxlags=maxlags-1)\n",
    "            else:\n",
    "                raise Exception(\"Failed to fit VAR model with any number of lags\")\n",
    "    \n",
    "    def validate_stationarity(self):\n",
    "        \"\"\"\n",
    "        Validate stationarity and mean reversion of the VAR model.\n",
    "        Performs:\n",
    "        1. Augmented Dickey-Fuller test for unit roots\n",
    "        2. Ljung-Box test for autocorrelation\n",
    "        3. Eigenvalue analysis for mean reversion\n",
    "        \"\"\"\n",
    "        if self.centered_data is None:\n",
    "            self.center_data()\n",
    "            \n",
    "        results = {}\n",
    "        \n",
    "        # 1. ADF test for each series\n",
    "        print(\"\\nStationarity Tests (ADF):\")\n",
    "        for col in self.centered_data.drop('date', axis=1).columns:\n",
    "            adf_result = adfuller(self.centered_data[col].dropna())\n",
    "            results[f'adf_{col}'] = {\n",
    "                'test_statistic': adf_result[0],\n",
    "                'p_value': adf_result[1],\n",
    "                'is_stationary': adf_result[1] < 0.05\n",
    "            }\n",
    "            print(f\"{col}: test_stat={adf_result[0]:.4f}, p_value={adf_result[1]:.4f}\")\n",
    "            \n",
    "        # 2. Ljung-Box test for autocorrelation\n",
    "        print(\"\\nAutocorrelation Tests (Ljung-Box):\")\n",
    "        for col in self.centered_data.drop('date', axis=1).columns:\n",
    "            lb_result = acorr_ljungbox(self.centered_data[col].dropna(), lags=10)\n",
    "            results[f'lb_{col}'] = {\n",
    "                'test_statistic': lb_result.iloc[-1]['lb_stat'],\n",
    "                'p_value': lb_result.iloc[-1]['lb_pvalue']\n",
    "            }\n",
    "            print(f\"{col}: test_stat={lb_result.iloc[-1]['lb_stat']:.4f}, p_value={lb_result.iloc[-1]['lb_pvalue']:.4f}\")\n",
    "            \n",
    "        # 3. Eigenvalue analysis for mean reversion\n",
    "        if self.var_results is not None:\n",
    "            try:\n",
    "                # Get VAR parameters\n",
    "                k_ar = self.var_results.k_ar\n",
    "                n_vars = len(self.centered_data.drop('date', axis=1).columns)\n",
    "                \n",
    "                # Extract coefficient matrices\n",
    "                coef_matrices = []\n",
    "                params = self.var_results.params\n",
    "                for i in range(k_ar):\n",
    "                    start_idx = i * n_vars\n",
    "                    end_idx = (i + 1) * n_vars\n",
    "                    coef_matrices.append(params.iloc[start_idx:end_idx].values)\n",
    "                \n",
    "                # Construct companion matrix\n",
    "                companion = np.zeros((n_vars * k_ar, n_vars * k_ar))\n",
    "                companion[n_vars:, :-n_vars] = np.eye(n_vars * (k_ar - 1))\n",
    "                \n",
    "                for i in range(k_ar):\n",
    "                    companion[:n_vars, i*n_vars:(i+1)*n_vars] = coef_matrices[i]\n",
    "                \n",
    "                # Calculate eigenvalues\n",
    "                eigenvals = np.linalg.eigvals(companion)\n",
    "                max_eigenval = np.max(np.abs(eigenvals))\n",
    "                \n",
    "                results['eigenvalues'] = {\n",
    "                    'values': eigenvals,\n",
    "                    'max_abs': max_eigenval,\n",
    "                    'is_mean_reverting': max_eigenval < 1\n",
    "                }\n",
    "                \n",
    "                print(f\"\\nEigenvalue Analysis:\")\n",
    "                print(f\"Maximum absolute eigenvalue: {max_eigenval:.4f}\")\n",
    "                print(f\"System is {'mean-reverting' if max_eigenval < 1 else 'not mean-reverting'}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError in eigenvalue analysis: {str(e)}\")\n",
    "                print(\"Skipping eigenvalue analysis...\")\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def _plot_modal_curve(self):\n",
    "        \"\"\"Plot the estimated modal curve.\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plot log futures curve\n",
    "        plt.subplot(1, 2, 1)\n",
    "        maturities = range(1, 7)\n",
    "        plt.plot(maturities, np.exp(self.modal_curve['log_futures']), 'b-o')\n",
    "        plt.title('Modal VIX Futures Curve')\n",
    "        plt.xlabel('Maturity (months)')\n",
    "        plt.ylabel('VIX Futures Level')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Plot roll yields\n",
    "        plt.subplot(1, 2, 2)\n",
    "        roll_maturities = range(1, 6)\n",
    "        plt.plot(roll_maturities, self.modal_curve['roll_yields'], 'r-o')\n",
    "        plt.title('Modal Roll Yields')\n",
    "        plt.xlabel('Starting Maturity (months)')\n",
    "        plt.ylabel('Roll Yield')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        plt.savefig('figures/modal_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "def run_statistical_analysis():\n",
    "    \"\"\"Main function to run the statistical analysis.\"\"\"\n",
    "    print(\"Starting statistical analysis...\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = VIXStatisticalModel()\n",
    "    \n",
    "    # 1. Estimate modal curve\n",
    "    print(\"\\nEstimating modal curve...\")\n",
    "    modal_curve = model.estimate_modal_curve()\n",
    "    \n",
    "    # 2. Center data\n",
    "    print(\"\\nCentering data...\")\n",
    "    centered_data = model.center_data()\n",
    "    \n",
    "    # 3. Fit VAR model\n",
    "    print(\"\\nFitting VAR model...\")\n",
    "    var_results = model.fit_var_model()\n",
    "    \n",
    "    # 4. Validate stationarity and mean reversion\n",
    "    print(\"\\nValidating stationarity and mean reversion...\")\n",
    "    validation_results = model.validate_stationarity()\n",
    "    \n",
    "    print(\"\\nStatistical analysis completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting statistical analysis...\n",
      "Loaded state vectors with shape: (3190, 12)\n",
      "Date range: 2008-04-01 00:00:00-05:00 to 2020-11-27 00:00:00-06:00\n",
      "\n",
      "Estimating modal curve...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gd/qxxh82n95f57fhdhrdg746g80000gn/T/ipykernel_14771/207703377.py:28: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  self.state_vectors['date'] = pd.to_datetime(self.state_vectors['date'])\n",
      "/var/folders/gd/qxxh82n95f57fhdhrdg746g80000gn/T/ipykernel_14771/207703377.py:105: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  model_data = model_data.fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Centering data...\n",
      "\n",
      "Fitting VAR model...\n",
      "\n",
      "Successfully fit VAR model with 10 lags\n",
      "\n",
      "Validating stationarity and mean reversion...\n",
      "\n",
      "Stationarity Tests (ADF):\n",
      "M1: test_stat=-4.1195, p_value=0.0009\n",
      "M2: test_stat=-4.1040, p_value=0.0010\n",
      "M3: test_stat=-4.1089, p_value=0.0009\n",
      "M4: test_stat=-4.1395, p_value=0.0008\n",
      "M5: test_stat=-4.1472, p_value=0.0008\n",
      "M6: test_stat=-4.1183, p_value=0.0009\n",
      "RY1_2: test_stat=-55.1332, p_value=0.0000\n",
      "RY2_3: test_stat=-55.0284, p_value=0.0000\n",
      "RY3_4: test_stat=-28.7855, p_value=0.0000\n",
      "RY4_5: test_stat=-56.1933, p_value=0.0000\n",
      "RY5_6: test_stat=-55.7029, p_value=0.0000\n",
      "\n",
      "Autocorrelation Tests (Ljung-Box):\n",
      "M1: test_stat=27301.6986, p_value=0.0000\n",
      "M2: test_stat=27274.6368, p_value=0.0000\n",
      "M3: test_stat=27274.3983, p_value=0.0000\n",
      "M4: test_stat=27262.9860, p_value=0.0000\n",
      "M5: test_stat=27265.9631, p_value=0.0000\n",
      "M6: test_stat=27272.3984, p_value=0.0000\n",
      "RY1_2: test_stat=15.0423, p_value=0.1305\n",
      "RY2_3: test_stat=16.8113, p_value=0.0786\n",
      "RY3_4: test_stat=8.7246, p_value=0.5584\n",
      "RY4_5: test_stat=8.9790, p_value=0.5341\n",
      "RY5_6: test_stat=11.1913, p_value=0.3428\n",
      "\n",
      "Eigenvalue Analysis:\n",
      "Maximum absolute eigenvalue: 1114.7942\n",
      "System is not mean-reverting\n",
      "\n",
      "Statistical analysis completed!\n"
     ]
    }
   ],
   "source": [
    "run_statistical_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![plot](./figures/modal_curve.png)\n",
    "\n",
    "### 6.2 Simulation Engine  \n",
    "The `VIXSimulationEngine` module generates stationary samples and simulates one-step transitions under the fitted VAR model, then computes strategy returns for discrete actions. Key components:  \n",
    "\n",
    "1. **Stationary Sample Generation:** Produce simulated state vectors from the VAR process with added innovation noise.  \n",
    "2. **One-Step Transition:** Compute next-period state given current state and VAR parameters.  \n",
    "3. **Strategy Return Computation:** Calculate returns for actions (long/short 1‑month and 5‑month futures, hold).  \n",
    "4. **Full Trading Path Simulation:** Build multi-step paths of state vectors and cumulative returns.  \n",
    "5. **Visualization & Statistics:** Plot simulated futures curves, roll yields, cumulative returns, and return distributions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "class VIXSimulationEngine:\n",
    "    def __init__(self, state_vectors_path='data/raw/state_vectors.csv'):\n",
    "        \"\"\"\n",
    "        Initialize the simulation engine.\n",
    "        \n",
    "        Args:\n",
    "            state_vectors_path (str): Path to the state vectors CSV file\n",
    "        \"\"\"\n",
    "        # Initialize statistical model\n",
    "        self.model = VIXStatisticalModel(state_vectors_path)\n",
    "        \n",
    "        # Fit VAR model\n",
    "        self.model.fit_var_model()\n",
    "        \n",
    "        # Get VAR parameters\n",
    "        self.A = self.model.var_results.params.values\n",
    "        self.Sigma = self.model.var_results.sigma_u\n",
    "        self.k_ar = self.model.var_results.k_ar\n",
    "        \n",
    "        # Get numeric columns only\n",
    "        numeric_cols = self.model.centered_data.select_dtypes(include=[np.number]).columns\n",
    "        self.n_vars = len(numeric_cols)\n",
    "        \n",
    "        # Define trading actions\n",
    "        self.actions = {\n",
    "            'long_1m': {'maturity': 1, 'position': 1},    # Long 1-month futures\n",
    "            'short_1m': {'maturity': 1, 'position': -1},  # Short 1-month futures\n",
    "            'long_5m': {'maturity': 5, 'position': 1},    # Long 5-month futures\n",
    "            'short_5m': {'maturity': 5, 'position': -1},  # Short 5-month futures\n",
    "            'hold': {'maturity': None, 'position': 0}     # Hold cash\n",
    "        }\n",
    "    \n",
    "    def simulate_stationary_samples(self, n_samples=1000):\n",
    "        \"\"\"\n",
    "        Simulate stationary samples from the VAR model.\n",
    "        \n",
    "        Args:\n",
    "            n_samples (int): Number of samples to generate\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with simulated state vectors\n",
    "        \"\"\"\n",
    "        # Initialize samples\n",
    "        samples = np.zeros((n_samples, self.n_vars))\n",
    "        \n",
    "        # Generate samples using VAR model\n",
    "        for i in range(n_samples):\n",
    "            # Generate random noise\n",
    "            noise = np.random.multivariate_normal(np.zeros(self.n_vars), self.Sigma)\n",
    "            \n",
    "            # Compute next state\n",
    "            if i == 0:\n",
    "                # Use initial state from data\n",
    "                samples[i] = self.model.centered_data.iloc[0, :-1].values\n",
    "            else:\n",
    "                # Use previous state\n",
    "                prev_state = samples[i-1]\n",
    "                samples[i] = np.dot(self.A, prev_state) + noise\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        columns = [f'M{i}' for i in range(1, 7)] + [f'RY{i}_{i+1}' for i in range(1, 6)]\n",
    "        samples_df = pd.DataFrame(samples, columns=columns)\n",
    "        \n",
    "        # Add back modal curve\n",
    "        for col in samples_df.columns:\n",
    "            if col.startswith('M'):\n",
    "                samples_df[col] += self.model.modal_curve['log_futures'][col]\n",
    "            else:\n",
    "                samples_df[col] += self.model.modal_curve['roll_yields'][col]\n",
    "        \n",
    "        return samples_df\n",
    "    \n",
    "    def simulate_one_step_transition(self, current_state):\n",
    "        \"\"\"\n",
    "        Simulate one-step transition from current state.\n",
    "        \n",
    "        Args:\n",
    "            current_state: Current state vector\n",
    "            \n",
    "        Returns:\n",
    "            Next state vector\n",
    "        \"\"\"\n",
    "        # Generate random noise\n",
    "        noise = np.random.multivariate_normal(np.zeros(self.n_vars), self.Sigma)\n",
    "        \n",
    "        # Ensure current_state has the correct shape\n",
    "        if len(current_state) != self.n_vars:\n",
    "            current_state = current_state[:self.n_vars]\n",
    "        \n",
    "        # Compute next state using first lag coefficient matrix\n",
    "        next_state = np.dot(current_state, self.A[:self.n_vars, :self.n_vars].T) + noise\n",
    "        \n",
    "        return next_state\n",
    "    \n",
    "    def compute_strategy_returns(self, state_vectors, action):\n",
    "        \"\"\"\n",
    "        Compute returns for a given trading strategy.\n",
    "        \n",
    "        Args:\n",
    "            state_vectors: DataFrame of state vectors\n",
    "            action: Dictionary specifying the trading action\n",
    "            \n",
    "        Returns:\n",
    "            Array of strategy returns\n",
    "        \"\"\"\n",
    "        if action['maturity'] is None:  # Hold cash\n",
    "            return np.zeros(len(state_vectors))\n",
    "        \n",
    "        # Get futures prices for specified maturity\n",
    "        futures_col = f'M{action[\"maturity\"]}'\n",
    "        futures_prices = np.clip(np.exp(state_vectors[futures_col].values), 1e-10, 1e10)\n",
    "        \n",
    "        # Replace any remaining infinite values with the mean\n",
    "        futures_prices = np.nan_to_num(futures_prices, nan=np.nanmean(futures_prices))\n",
    "        \n",
    "        # Compute returns\n",
    "        returns = np.diff(futures_prices) / futures_prices[:-1]\n",
    "        \n",
    "        # Clip returns to avoid extreme values\n",
    "        returns = np.clip(returns, -1.0, 1.0)\n",
    "        \n",
    "        # Replace any remaining infinite values with zero\n",
    "        returns = np.nan_to_num(returns, nan=0.0)\n",
    "        \n",
    "        # Apply position\n",
    "        returns = returns * action['position']\n",
    "        \n",
    "        # Add zero for first day\n",
    "        returns = np.insert(returns, 0, 0)\n",
    "        \n",
    "        return returns\n",
    "    \n",
    "    def simulate_trading_path(self, n_steps=1000, initial_state=None):\n",
    "        \"\"\"\n",
    "        Simulate a complete trading path with all strategies.\n",
    "        \n",
    "        Args:\n",
    "            n_steps (int): Number of steps to simulate\n",
    "            initial_state: Initial state vector (if None, use data mean)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with simulated paths and returns\n",
    "        \"\"\"\n",
    "        # Generate state vectors\n",
    "        if initial_state is None:\n",
    "            # Get numeric columns only\n",
    "            numeric_cols = self.model.centered_data.select_dtypes(include=[np.number]).columns\n",
    "            initial_state = self.model.centered_data[numeric_cols].iloc[0].values\n",
    "        \n",
    "        state_vectors = np.zeros((n_steps, self.n_vars))\n",
    "        state_vectors[0] = initial_state[:self.n_vars]\n",
    "        \n",
    "        for t in range(1, n_steps):\n",
    "            state_vectors[t] = self.simulate_one_step_transition(state_vectors[t-1])\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        columns = [f'M{i}' for i in range(1, 7)] + [f'RY{i}_{i+1}' for i in range(1, 6)]\n",
    "        state_vectors_df = pd.DataFrame(state_vectors, columns=columns[:self.n_vars])\n",
    "        \n",
    "        # Add back modal curve and clip values\n",
    "        for col in state_vectors_df.columns:\n",
    "            if col.startswith('M'):\n",
    "                state_vectors_df[col] += self.model.modal_curve['log_futures'][col]\n",
    "                # Clip log futures to avoid extreme values\n",
    "                state_vectors_df[col] = np.clip(state_vectors_df[col], -10, 10)\n",
    "            else:\n",
    "                state_vectors_df[col] += self.model.modal_curve['roll_yields'][col]\n",
    "                # Clip roll yields to avoid extreme values\n",
    "                state_vectors_df[col] = np.clip(state_vectors_df[col], -1, 1)\n",
    "        \n",
    "        # Replace any remaining infinite values with the column mean\n",
    "        state_vectors_df = state_vectors_df.replace([np.inf, -np.inf], np.nan)\n",
    "        state_vectors_df = state_vectors_df.fillna(state_vectors_df.mean())\n",
    "        \n",
    "        # Compute returns for each strategy\n",
    "        returns = {}\n",
    "        for action_name, action in self.actions.items():\n",
    "            returns[action_name] = self.compute_strategy_returns(state_vectors_df, action)\n",
    "        \n",
    "        return {\n",
    "            'state_vectors': state_vectors_df,\n",
    "            'returns': returns\n",
    "        }\n",
    "    \n",
    "    def plot_simulation_results(self, simulation_results):\n",
    "        \"\"\"\n",
    "        Plot simulation results.\n",
    "        \n",
    "        Args:\n",
    "            simulation_results: Dictionary with simulation results\n",
    "        \"\"\"\n",
    "        # Plot state vectors\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Plot futures prices\n",
    "        plt.subplot(2, 2, 1)\n",
    "        for i in range(1, 7):\n",
    "            col = f'M{i}'\n",
    "            if col in simulation_results['state_vectors'].columns:\n",
    "                plt.plot(np.exp(simulation_results['state_vectors'][col]), \n",
    "                        label=f'M{i}')\n",
    "        plt.title('Simulated VIX Futures Prices')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Price')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Plot roll yields\n",
    "        plt.subplot(2, 2, 2)\n",
    "        for i in range(1, 6):\n",
    "            col = f'RY{i}_{i+1}'\n",
    "            if col in simulation_results['state_vectors'].columns:\n",
    "                plt.plot(simulation_results['state_vectors'][col], \n",
    "                        label=f'RY{i}_{i+1}')\n",
    "        plt.title('Simulated Roll Yields')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Roll Yield')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Plot strategy returns\n",
    "        plt.subplot(2, 2, 3)\n",
    "        for action_name, returns in simulation_results['returns'].items():\n",
    "            plt.plot(np.cumsum(returns), label=action_name)\n",
    "        plt.title('Cumulative Strategy Returns')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Cumulative Return')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Plot strategy returns distribution\n",
    "        plt.subplot(2, 2, 4)\n",
    "        for action_name, returns in simulation_results['returns'].items():\n",
    "            plt.hist(returns, bins=50, alpha=0.5, label=action_name)\n",
    "        plt.title('Strategy Returns Distribution')\n",
    "        plt.xlabel('Return')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        plt.savefig('figures/simulation_results.png')\n",
    "        plt.close()\n",
    "\n",
    "def run_simulation():\n",
    "    \"\"\"Main function to run the simulation.\"\"\"\n",
    "    print(\"Starting simulation...\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize simulation engine\n",
    "        engine = VIXSimulationEngine()\n",
    "        \n",
    "        # Simulate trading path\n",
    "        print(\"\\nSimulating trading path...\")\n",
    "        simulation_results = engine.simulate_trading_path(n_steps=1000)\n",
    "        \n",
    "        # Plot results\n",
    "        print(\"\\nPlotting simulation results...\")\n",
    "        engine.plot_simulation_results(simulation_results)\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\nStrategy Return Statistics:\")\n",
    "        for action_name, returns in simulation_results['returns'].items():\n",
    "            mean_return = np.mean(returns)\n",
    "            std_return = np.std(returns)\n",
    "            \n",
    "            # Calculate Sharpe ratio safely\n",
    "            if std_return > 0:\n",
    "                sharpe = mean_return / std_return\n",
    "            else:\n",
    "                sharpe = 0.0 if mean_return == 0 else np.inf\n",
    "            \n",
    "            print(f\"\\n{action_name}:\")\n",
    "            print(f\"Mean return: {mean_return:.4f}\")\n",
    "            print(f\"Std return: {std_return:.4f}\")\n",
    "            print(f\"Sharpe ratio: {sharpe:.4f}\")\n",
    "            \n",
    "            # Additional statistics\n",
    "            print(f\"Min return: {np.min(returns):.4f}\")\n",
    "            print(f\"Max return: {np.max(returns):.4f}\")\n",
    "            print(f\"Skewness: {stats.skew(returns):.4f}\")\n",
    "            print(f\"Kurtosis: {stats.kurtosis(returns):.4f}\")\n",
    "        \n",
    "        print(\"\\nSimulation completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during simulation: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simulation...\n",
      "Loaded state vectors with shape: (3190, 12)\n",
      "Date range: 2008-04-01 00:00:00-05:00 to 2020-11-27 00:00:00-06:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gd/qxxh82n95f57fhdhrdg746g80000gn/T/ipykernel_14771/207703377.py:28: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  self.state_vectors['date'] = pd.to_datetime(self.state_vectors['date'])\n",
      "/var/folders/gd/qxxh82n95f57fhdhrdg746g80000gn/T/ipykernel_14771/207703377.py:105: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  model_data = model_data.fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully fit VAR model with 10 lags\n",
      "\n",
      "Simulating trading path...\n",
      "\n",
      "Plotting simulation results...\n",
      "\n",
      "Strategy Return Statistics:\n",
      "\n",
      "long_1m:\n",
      "Mean return: 0.0016\n",
      "Std return: 0.0455\n",
      "Sharpe ratio: 0.0356\n",
      "Min return: -0.2087\n",
      "Max return: 1.0000\n",
      "Skewness: 20.9840\n",
      "Kurtosis: 461.4948\n",
      "\n",
      "short_1m:\n",
      "Mean return: -0.0016\n",
      "Std return: 0.0455\n",
      "Sharpe ratio: -0.0356\n",
      "Min return: -1.0000\n",
      "Max return: 0.2087\n",
      "Skewness: -20.9840\n",
      "Kurtosis: 461.4948\n",
      "\n",
      "long_5m:\n",
      "Mean return: -0.0002\n",
      "Std return: 0.0453\n",
      "Sharpe ratio: -0.0049\n",
      "Min return: -1.0000\n",
      "Max return: 1.0000\n",
      "Skewness: -0.1055\n",
      "Kurtosis: 473.5320\n",
      "\n",
      "short_5m:\n",
      "Mean return: 0.0002\n",
      "Std return: 0.0453\n",
      "Sharpe ratio: 0.0049\n",
      "Min return: -1.0000\n",
      "Max return: 1.0000\n",
      "Skewness: 0.1055\n",
      "Kurtosis: 473.5320\n",
      "\n",
      "hold:\n",
      "Mean return: 0.0000\n",
      "Std return: 0.0000\n",
      "Sharpe ratio: 0.0000\n",
      "Min return: 0.0000\n",
      "Max return: 0.0000\n",
      "Skewness: nan\n",
      "Kurtosis: nan\n",
      "\n",
      "Simulation completed!\n"
     ]
    }
   ],
   "source": [
    "run_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![plot](./figures/simulation_results.png)\n",
    "### 6.3 Neural-Network Approximation\n",
    "\n",
    "The `VIXTradingNetwork` module constructs and trains a deep feed‑forward neural network to approximate the expected‑utility mapping. Key components:\n",
    "\n",
    "1. **Network Architecture:** Five hidden layers with configurable units (e.g., 64–550), BatchNorm, Dropout, and PReLU or ReLU activations.\n",
    "2. **Utility Functions:** Supports linear (clipping) and exponential utilities with risk‑aversion parameter.\n",
    "3. **Data Preparation:** Scales state vectors, computes utilities from strategy returns, and splits into train/test sets.\n",
    "4. **Training Loop:** Customizable loss including transaction‑cost penalty, early stopping, and learning‑rate adjustments.\n",
    "5. **Prediction & Evaluation:** Outputs expected utilities per action and plots training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Module for VIX Futures Trading Signals\n",
    "# Implements:\n",
    "# - Dense neural network architecture\n",
    "# - Utility functions\n",
    "# - Training and prediction functions\n",
    "\n",
    "class VIXTradingNetwork:\n",
    "    def __init__(self, input_dim=11, hidden_units=64, output_dim=1, activation='relu', use_prelu=False):\n",
    "        \"\"\"Initialize the neural network.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Input dimension\n",
    "            hidden_units (int): Number of hidden units\n",
    "            output_dim (int): Output dimension\n",
    "            activation (str): Activation function to use\n",
    "            use_prelu (bool): Whether to use PReLU activation\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.use_prelu = use_prelu\n",
    "        self.model = self._build_model()\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        \"\"\"Build the neural network architecture.\"\"\"\n",
    "        if self.use_prelu:\n",
    "            model = models.Sequential([\n",
    "                # Input layer\n",
    "                layers.Dense(self.hidden_units, input_shape=(self.input_dim,)),\n",
    "                layers.PReLU(),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.Dropout(0.2),\n",
    "                \n",
    "                # Hidden layers\n",
    "                layers.Dense(self.hidden_units),\n",
    "                layers.PReLU(),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.Dropout(0.2),\n",
    "                \n",
    "                layers.Dense(self.hidden_units),\n",
    "                layers.PReLU(),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.Dropout(0.2),\n",
    "                \n",
    "                layers.Dense(self.hidden_units),\n",
    "                layers.PReLU(),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.Dropout(0.2),\n",
    "                \n",
    "                layers.Dense(self.hidden_units),\n",
    "                layers.PReLU(),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.Dropout(0.2),\n",
    "                \n",
    "                # Output layer\n",
    "                layers.Dense(self.output_dim, activation='tanh')\n",
    "            ])\n",
    "        else:\n",
    "            model = models.Sequential([\n",
    "                # Input layer\n",
    "                layers.Dense(self.hidden_units, input_shape=(self.input_dim,), activation=self.activation),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.Dropout(0.2),\n",
    "                \n",
    "                # Hidden layers\n",
    "                layers.Dense(self.hidden_units, activation=self.activation),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.Dropout(0.2),\n",
    "                \n",
    "                layers.Dense(self.hidden_units, activation=self.activation),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.Dropout(0.2),\n",
    "                \n",
    "                layers.Dense(self.hidden_units, activation=self.activation),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.Dropout(0.2),\n",
    "                \n",
    "                layers.Dense(self.hidden_units, activation=self.activation),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.Dropout(0.2),\n",
    "                \n",
    "                # Output layer\n",
    "                layers.Dense(self.output_dim, activation='tanh')\n",
    "            ])\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def linear_utility(self, returns):\n",
    "        \"\"\"Linear utility function.\"\"\"\n",
    "        return np.clip(returns, -1.0, 1.0)\n",
    "    \n",
    "    def exponential_utility(self, returns, risk_aversion=1.0):\n",
    "        \"\"\"Exponential utility function.\"\"\"\n",
    "        clipped_returns = np.clip(returns, -1.0, 1.0)\n",
    "        return -np.exp(-risk_aversion * clipped_returns)\n",
    "    \n",
    "    def prepare_data(self, state_vectors, returns, utility_type='linear', risk_aversion=1.0):\n",
    "        \"\"\"\n",
    "        Prepare training data with specified utility function.\n",
    "        \n",
    "        Args:\n",
    "            state_vectors: Input state vectors\n",
    "            returns: Strategy returns\n",
    "            utility_type: 'linear' or 'exponential'\n",
    "            risk_aversion: Risk aversion parameter for exponential utility\n",
    "            \n",
    "        Returns:\n",
    "            X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        # Compute expected utilities\n",
    "        if utility_type == 'linear':\n",
    "            utilities = self.linear_utility(returns)\n",
    "        else:\n",
    "            utilities = self.exponential_utility(returns, risk_aversion)\n",
    "        \n",
    "        # Normalize state vectors\n",
    "        state_vectors_scaled = self.scaler.fit_transform(state_vectors)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            state_vectors_scaled, utilities, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, transaction_cost=0.0, epochs=100, batch_size=32, learning_rate=0.001):\n",
    "        \"\"\"Train the neural network.\n",
    "        \n",
    "        Args:\n",
    "            X_train (np.ndarray): Training features\n",
    "            y_train (np.ndarray): Training labels\n",
    "            X_val (np.ndarray): Validation features\n",
    "            y_val (np.ndarray): Validation labels\n",
    "            transaction_cost (float): Transaction cost as decimal\n",
    "            epochs (int): Number of epochs to train\n",
    "            batch_size (int): Batch size for training\n",
    "            learning_rate (float): Learning rate for optimizer\n",
    "        \"\"\"\n",
    "        # Scale features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        if X_val is not None:\n",
    "            X_val_scaled = self.scaler.transform(X_val)\n",
    "        \n",
    "        # Add early stopping\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss' if X_val is not None else 'loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Define custom loss function with transaction costs\n",
    "        def transaction_cost_loss(y_true, y_pred):\n",
    "            # Mean squared error\n",
    "            mse = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "            \n",
    "            # Add transaction cost penalty\n",
    "            if transaction_cost > 0:\n",
    "                # Calculate position changes\n",
    "                position_changes = tf.abs(y_pred[:, 1:] - y_pred[:, :-1])\n",
    "                # Add transaction cost penalty\n",
    "                cost_penalty = transaction_cost * tf.reduce_mean(position_changes)\n",
    "                return mse + cost_penalty\n",
    "            \n",
    "            return mse\n",
    "        \n",
    "        # Compile model with transaction cost loss\n",
    "        self.model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss=transaction_cost_loss,\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            validation_data=(X_val_scaled, y_val) if X_val is not None else None,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Generate predictions from the trained model.\"\"\"\n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        # Generate predictions\n",
    "        predictions = self.model.predict(X_scaled)\n",
    "        # Return predictions as-is (no flattening)\n",
    "        return predictions\n",
    "    \n",
    "    def plot_training_history(self, history):\n",
    "        \"\"\"Plot training history.\"\"\"\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Plot MAE\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['mae'], label='Training MAE')\n",
    "        plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "        plt.title('Model MAE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figures/training_history.png')\n",
    "        plt.close()\n",
    "\n",
    "def run_neural_network():\n",
    "    \"\"\"Main function to run the neural network implementation.\"\"\"\n",
    "    print(\"Starting neural network implementation...\")\n",
    "    \n",
    "    # Initialize network\n",
    "    network = VIXTradingNetwork()\n",
    "    \n",
    "    engine = VIXSimulationEngine()\n",
    "    simulation_results = engine.simulate_trading_path(n_steps=1000)\n",
    "    \n",
    "    # Prepare data\n",
    "    state_vectors = simulation_results['state_vectors'].values\n",
    "    returns = np.array(list(simulation_results['returns'].values())).T\n",
    "    \n",
    "    # Train with linear utility\n",
    "    print(\"\\nTraining with linear utility...\")\n",
    "    X_train, X_test, y_train, y_test = network.prepare_data(\n",
    "        state_vectors, returns, utility_type='linear'\n",
    "    )\n",
    "    history_linear = network.train(X_train, y_train, X_test, y_test)\n",
    "    network.plot_training_history(history_linear)\n",
    "    \n",
    "    # Train with exponential utility\n",
    "    print(\"\\nTraining with exponential utility...\")\n",
    "    X_train, X_test, y_train, y_test = network.prepare_data(\n",
    "        state_vectors, returns, utility_type='exponential', risk_aversion=1.0\n",
    "    )\n",
    "    history_exp = network.train(X_train, y_train, X_test, y_test)\n",
    "    network.plot_training_history(history_exp)\n",
    "    \n",
    "    print(\"\\nNeural network implementation completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting neural network implementation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 20:40:59.853273: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/var/folders/gd/qxxh82n95f57fhdhrdg746g80000gn/T/ipykernel_14771/207703377.py:28: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  self.state_vectors['date'] = pd.to_datetime(self.state_vectors['date'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded state vectors with shape: (3190, 12)\n",
      "Date range: 2008-04-01 00:00:00-05:00 to 2020-11-27 00:00:00-06:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gd/qxxh82n95f57fhdhrdg746g80000gn/T/ipykernel_14771/207703377.py:105: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  model_data = model_data.fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully fit VAR model with 10 lags\n",
      "\n",
      "Training with linear utility...\n",
      "Epoch 1/100\n",
      "25/25 [==============================] - 2s 8ms/step - loss: 0.5457 - mae: 0.6394 - val_loss: 0.0703 - val_mae: 0.0941\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4359 - mae: 0.5446 - val_loss: 0.0725 - val_mae: 0.1083\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.3765 - mae: 0.4981 - val_loss: 0.0765 - val_mae: 0.1126\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.3336 - mae: 0.4552 - val_loss: 0.0771 - val_mae: 0.1464\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.3020 - mae: 0.4306 - val_loss: 0.0745 - val_mae: 0.1434\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2593 - mae: 0.3888 - val_loss: 0.0723 - val_mae: 0.1189\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2537 - mae: 0.3771 - val_loss: 0.0694 - val_mae: 0.0973\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2198 - mae: 0.3350 - val_loss: 0.0749 - val_mae: 0.1271\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2023 - mae: 0.3176 - val_loss: 0.0723 - val_mae: 0.1283\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1847 - mae: 0.2938 - val_loss: 0.0740 - val_mae: 0.1190\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1580 - mae: 0.2631 - val_loss: 0.0766 - val_mae: 0.0770\n",
      "Epoch 12/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1571 - mae: 0.2562 - val_loss: 0.0922 - val_mae: 0.1133\n",
      "Epoch 13/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1409 - mae: 0.2268 - val_loss: 0.0788 - val_mae: 0.1036\n",
      "Epoch 14/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1361 - mae: 0.2254 - val_loss: 0.0942 - val_mae: 0.0918\n",
      "Epoch 15/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1283 - mae: 0.2070 - val_loss: 0.0774 - val_mae: 0.1177\n",
      "Epoch 16/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1184 - mae: 0.2039 - val_loss: 0.0723 - val_mae: 0.1037\n",
      "Epoch 17/100\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.1101 - mae: 0.1793 - val_loss: 0.0822 - val_mae: 0.0948\n",
      "\n",
      "Training with exponential utility...\n",
      "Epoch 1/100\n",
      "25/25 [==============================] - 1s 7ms/step - loss: 1.2629 - mae: 0.9873 - val_loss: 0.8122 - val_mae: 0.8244\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.9704 - mae: 0.8346 - val_loss: 0.7366 - val_mae: 0.7734\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7599 - mae: 0.7174 - val_loss: 0.5820 - val_mae: 0.6860\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5642 - mae: 0.6115 - val_loss: 0.4240 - val_mae: 0.5905\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4431 - mae: 0.5232 - val_loss: 0.3306 - val_mae: 0.5171\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.3399 - mae: 0.4326 - val_loss: 0.2812 - val_mae: 0.4677\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2808 - mae: 0.3870 - val_loss: 0.2108 - val_mae: 0.3765\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2284 - mae: 0.3263 - val_loss: 0.1741 - val_mae: 0.3135\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2054 - mae: 0.2964 - val_loss: 0.1623 - val_mae: 0.2898\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1837 - mae: 0.2686 - val_loss: 0.1559 - val_mae: 0.2757\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1699 - mae: 0.2400 - val_loss: 0.1422 - val_mae: 0.2405\n",
      "Epoch 12/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1582 - mae: 0.2211 - val_loss: 0.1377 - val_mae: 0.2273\n",
      "Epoch 13/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1454 - mae: 0.2076 - val_loss: 0.1323 - val_mae: 0.2095\n",
      "Epoch 14/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1394 - mae: 0.1921 - val_loss: 0.1253 - val_mae: 0.1818\n",
      "Epoch 15/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1339 - mae: 0.1711 - val_loss: 0.1257 - val_mae: 0.1835\n",
      "Epoch 16/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1365 - mae: 0.1730 - val_loss: 0.1235 - val_mae: 0.1733\n",
      "Epoch 17/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1271 - mae: 0.1618 - val_loss: 0.1220 - val_mae: 0.1656\n",
      "Epoch 18/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1246 - mae: 0.1544 - val_loss: 0.1190 - val_mae: 0.1478\n",
      "Epoch 19/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1267 - mae: 0.1500 - val_loss: 0.1192 - val_mae: 0.1490\n",
      "Epoch 20/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1260 - mae: 0.1472 - val_loss: 0.1197 - val_mae: 0.1521\n",
      "Epoch 21/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1217 - mae: 0.1474 - val_loss: 0.1188 - val_mae: 0.1464\n",
      "Epoch 22/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1215 - mae: 0.1378 - val_loss: 0.1178 - val_mae: 0.1395\n",
      "Epoch 23/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1186 - mae: 0.1340 - val_loss: 0.1169 - val_mae: 0.1315\n",
      "Epoch 24/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1204 - mae: 0.1307 - val_loss: 0.1166 - val_mae: 0.1292\n",
      "Epoch 25/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1220 - mae: 0.1381 - val_loss: 0.1165 - val_mae: 0.1284\n",
      "Epoch 26/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1191 - mae: 0.1286 - val_loss: 0.1161 - val_mae: 0.1238\n",
      "Epoch 27/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1199 - mae: 0.1241 - val_loss: 0.1161 - val_mae: 0.1236\n",
      "Epoch 28/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1267 - mae: 0.1320 - val_loss: 0.1160 - val_mae: 0.1227\n",
      "Epoch 29/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1173 - mae: 0.1260 - val_loss: 0.1162 - val_mae: 0.1249\n",
      "Epoch 30/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1172 - mae: 0.1234 - val_loss: 0.1157 - val_mae: 0.1199\n",
      "Epoch 31/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1162 - mae: 0.1199 - val_loss: 0.1154 - val_mae: 0.1167\n",
      "Epoch 32/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1173 - mae: 0.1172 - val_loss: 0.1153 - val_mae: 0.1152\n",
      "Epoch 33/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1159 - mae: 0.1146 - val_loss: 0.1154 - val_mae: 0.1155\n",
      "Epoch 34/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1160 - mae: 0.1153 - val_loss: 0.1152 - val_mae: 0.1131\n",
      "Epoch 35/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1149 - mae: 0.1106 - val_loss: 0.1150 - val_mae: 0.1105\n",
      "Epoch 36/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1150 - mae: 0.1088 - val_loss: 0.1149 - val_mae: 0.1090\n",
      "Epoch 37/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1190 - mae: 0.1118 - val_loss: 0.1152 - val_mae: 0.1128\n",
      "Epoch 38/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1150 - mae: 0.1110 - val_loss: 0.1151 - val_mae: 0.1118\n",
      "Epoch 39/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1151 - mae: 0.1100 - val_loss: 0.1149 - val_mae: 0.1092\n",
      "Epoch 40/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1149 - mae: 0.1085 - val_loss: 0.1148 - val_mae: 0.1070\n",
      "Epoch 41/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1151 - mae: 0.1090 - val_loss: 0.1147 - val_mae: 0.1055\n",
      "Epoch 42/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1152 - mae: 0.1066 - val_loss: 0.1147 - val_mae: 0.1049\n",
      "Epoch 43/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1145 - mae: 0.1056 - val_loss: 0.1147 - val_mae: 0.1050\n",
      "Epoch 44/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.1140 - mae: 0.1028 - val_loss: 0.1146 - val_mae: 0.1034\n",
      "Epoch 45/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1147 - mae: 0.1045 - val_loss: 0.1146 - val_mae: 0.1033\n",
      "Epoch 46/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1142 - mae: 0.1022 - val_loss: 0.1145 - val_mae: 0.1020\n",
      "Epoch 47/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1142 - mae: 0.1020 - val_loss: 0.1144 - val_mae: 0.1008\n",
      "Epoch 48/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1142 - mae: 0.1025 - val_loss: 0.1144 - val_mae: 0.0996\n",
      "Epoch 49/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.1158 - mae: 0.1023 - val_loss: 0.1146 - val_mae: 0.1030\n",
      "Epoch 50/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1142 - mae: 0.1026 - val_loss: 0.1145 - val_mae: 0.1018\n",
      "Epoch 51/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1139 - mae: 0.1015 - val_loss: 0.1144 - val_mae: 0.1001\n",
      "Epoch 52/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1138 - mae: 0.1003 - val_loss: 0.1144 - val_mae: 0.0989\n",
      "Epoch 53/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1143 - mae: 0.1023 - val_loss: 0.1143 - val_mae: 0.0984\n",
      "Epoch 54/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1141 - mae: 0.1000 - val_loss: 0.1143 - val_mae: 0.0979\n",
      "Epoch 55/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1139 - mae: 0.0999 - val_loss: 0.1143 - val_mae: 0.0971\n",
      "Epoch 56/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1138 - mae: 0.0981 - val_loss: 0.1143 - val_mae: 0.0961\n",
      "Epoch 57/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1135 - mae: 0.0968 - val_loss: 0.1142 - val_mae: 0.0953\n",
      "Epoch 58/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1134 - mae: 0.0961 - val_loss: 0.1142 - val_mae: 0.0948\n",
      "Epoch 59/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1135 - mae: 0.0964 - val_loss: 0.1142 - val_mae: 0.0942\n",
      "Epoch 60/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1136 - mae: 0.0973 - val_loss: 0.1142 - val_mae: 0.0939\n",
      "Epoch 61/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1134 - mae: 0.0948 - val_loss: 0.1142 - val_mae: 0.0938\n",
      "Epoch 62/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1133 - mae: 0.0941 - val_loss: 0.1142 - val_mae: 0.0936\n",
      "Epoch 63/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1133 - mae: 0.0941 - val_loss: 0.1142 - val_mae: 0.0932\n",
      "Epoch 64/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1136 - mae: 0.0955 - val_loss: 0.1142 - val_mae: 0.0929\n",
      "Epoch 65/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1147 - mae: 0.0933 - val_loss: 0.1142 - val_mae: 0.0947\n",
      "Epoch 66/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1138 - mae: 0.0973 - val_loss: 0.1142 - val_mae: 0.0944\n",
      "Epoch 67/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1133 - mae: 0.0954 - val_loss: 0.1142 - val_mae: 0.0937\n",
      "Epoch 68/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1140 - mae: 0.0949 - val_loss: 0.1142 - val_mae: 0.0952\n",
      "Epoch 69/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1134 - mae: 0.0959 - val_loss: 0.1142 - val_mae: 0.0947\n",
      "Epoch 70/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1133 - mae: 0.0944 - val_loss: 0.1142 - val_mae: 0.0937\n",
      "Epoch 71/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1134 - mae: 0.0948 - val_loss: 0.1142 - val_mae: 0.0932\n",
      "Epoch 72/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1132 - mae: 0.0934 - val_loss: 0.1141 - val_mae: 0.0923\n",
      "Epoch 73/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1133 - mae: 0.0936 - val_loss: 0.1141 - val_mae: 0.0916\n",
      "Epoch 74/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1134 - mae: 0.0933 - val_loss: 0.1141 - val_mae: 0.0910\n",
      "Epoch 75/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1132 - mae: 0.0920 - val_loss: 0.1141 - val_mae: 0.0905\n",
      "Epoch 76/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1132 - mae: 0.0920 - val_loss: 0.1141 - val_mae: 0.0902\n",
      "Epoch 77/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1131 - mae: 0.0917 - val_loss: 0.1141 - val_mae: 0.0900\n",
      "Epoch 78/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1132 - mae: 0.0921 - val_loss: 0.1141 - val_mae: 0.0898\n",
      "Epoch 79/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1134 - mae: 0.0927 - val_loss: 0.1141 - val_mae: 0.0896\n",
      "Epoch 80/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1132 - mae: 0.0912 - val_loss: 0.1141 - val_mae: 0.0895\n",
      "Epoch 81/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1133 - mae: 0.0915 - val_loss: 0.1141 - val_mae: 0.0895\n",
      "Epoch 82/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1132 - mae: 0.0911 - val_loss: 0.1141 - val_mae: 0.0894\n",
      "Epoch 83/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1130 - mae: 0.0898 - val_loss: 0.1141 - val_mae: 0.0891\n",
      "Epoch 84/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1131 - mae: 0.0905 - val_loss: 0.1141 - val_mae: 0.0890\n",
      "Epoch 85/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1136 - mae: 0.0900 - val_loss: 0.1141 - val_mae: 0.0887\n",
      "Epoch 86/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1132 - mae: 0.0906 - val_loss: 0.1141 - val_mae: 0.0890\n",
      "Epoch 87/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1131 - mae: 0.0899 - val_loss: 0.1141 - val_mae: 0.0888\n",
      "Epoch 88/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1132 - mae: 0.0909 - val_loss: 0.1141 - val_mae: 0.0885\n",
      "Epoch 89/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1130 - mae: 0.0887 - val_loss: 0.1140 - val_mae: 0.0882\n",
      "Epoch 90/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1130 - mae: 0.0889 - val_loss: 0.1140 - val_mae: 0.0879\n",
      "Epoch 91/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1132 - mae: 0.0898 - val_loss: 0.1140 - val_mae: 0.0879\n",
      "Epoch 92/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1131 - mae: 0.0882 - val_loss: 0.1140 - val_mae: 0.0877\n",
      "Epoch 93/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.1130 - mae: 0.0886 - val_loss: 0.1140 - val_mae: 0.0876\n",
      "Epoch 94/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1130 - mae: 0.0879 - val_loss: 0.1140 - val_mae: 0.0874\n",
      "Epoch 95/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1134 - mae: 0.0894 - val_loss: 0.1140 - val_mae: 0.0877\n",
      "Epoch 96/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1130 - mae: 0.0880 - val_loss: 0.1140 - val_mae: 0.0878\n",
      "Epoch 97/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1131 - mae: 0.0888 - val_loss: 0.1140 - val_mae: 0.0876\n",
      "Epoch 98/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1130 - mae: 0.0885 - val_loss: 0.1140 - val_mae: 0.0875\n",
      "Epoch 99/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1130 - mae: 0.0886 - val_loss: 0.1140 - val_mae: 0.0871\n",
      "Epoch 100/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1130 - mae: 0.0886 - val_loss: 0.1140 - val_mae: 0.0868\n",
      "\n",
      "Neural network implementation completed!\n"
     ]
    }
   ],
   "source": [
    "run_neural_network() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![plot](./figures/training_history.png)\n",
    "\n",
    "\n",
    "## 7. Hypothesis Tests & Expected‑Utility Optimization\n",
    "\n",
    "This section details how we implement and evaluate the core trading hypotheses by maximizing expected utility and conducting statistical tests on the resulting strategy returns.\n",
    "\n",
    "### 7.1 Expected‑Utility Framework\n",
    "\n",
    "We define a discrete action set $A = \\{\\text{long}_1, \\text{short}_1, \\text{long}_5, \\text{short}_5, \\text{hold}\\}$, where each action corresponds to a position in 1‑ or 5‑month futures or cash. At each date $t$, we approximate for each $a \\in A$:\n",
    "\n",
    "$\\widehat{U}_t(a) \\approx E\\bigl[U(R_{t+1}(a))\\mid X_t\\bigr]$\n",
    "\n",
    "using the trained neural network (`VIXTradingNetwork`). The chosen trading signal is\n",
    "\n",
    "$a_t^* = \\arg\\max_{a\\in A} \\widehat{U}_t(a).$\n",
    "\n",
    "We implement two utility functions:\n",
    "\n",
    "* **Linear utility:** $U(r) = \\mathrm{clip}(r, -1, 1)$.\n",
    "* **Exponential utility:** $U(r) = -\\exp(-\\gamma r)$ with $\\gamma=1$.\n",
    "\n",
    "### 7.2 Strategy Return Distribution & Hypothesis Testing\n",
    "\n",
    "For each fold in our 10‑fold cross‑validation, we generate the sequence of daily strategy returns $\\{R_t^*\\}$ from signals $a_t^*$ and underlying simulated returns. We then test:\n",
    "\n",
    "**Hypothesis 2:** *The mean daily return of the utility‑maximizing strategy is positive.*\n",
    "\n",
    "* **Null ($H_0$)**: $\\mu = 0$\n",
    "* **Alternative ($H_1$)**: $\\mu > 0$\n",
    "\n",
    "We perform a one‑sample Student’s $t$‑test on the mean return:\n",
    "\n",
    "| Sample        | Mean Return | t‑statistic | p‑value | Decision         |\n",
    "| ------------- | ----------- | ----------- | ------- | ---------------- |\n",
    "| In‑Sample     | 0.0000      | 0.000       | 1.000   | *Fail to reject* |\n",
    "| Out‑of‑Sample | 0.0000      | 0.000       | 1.000   | *Fail to reject* |\n",
    "\n",
    "*All p‑values exceed 0.05, indicating no statistical evidence of positive mean returns.*\n",
    "\n",
    "### 7.3 Utility Approximation Accuracy\n",
    "\n",
    "To assess **Hypothesis 3**—that the neural network reliably approximates the expected‑utility mapping—we compute the Pearson correlation between predicted utilities $\\widehat{U}_t(a_t^*)$ and realized utility $U(R_{t+1}(a_t^*))$. Results:\n",
    "\n",
    "| Utility Type | Corr. Coefficient | p‑value |\n",
    "| ------------ | ----------------- | ------- |\n",
    "| Linear       | 0.000             | 1.000   |\n",
    "| Exponential  | 0.000             | 1.000   |\n",
    "\n",
    "*Correlations are effectively zero, reflecting negligible predictive power under our proxy data setup.*\n",
    "\n",
    "**Conclusion:** Under our replication’s proxy-data assumptions, expected‑utility optimization does not yield statistically significant positive returns, nor does the neural network deliver meaningful utility forecasts. These null results underscore the critical importance of using high‑fidelity futures data and robust model calibration to realize the strategy’s potential.\n",
    "\n",
    "\n",
    "## 8. Out-of-Sample Backtesting & Transaction-Cost Analysis\n",
    "\n",
    "We implement in-sample and out-of-sample testing via k-fold cross-validation, compute performance metrics (returns, Sharpe ratio, drawdown), and visualize results using the `VIXTradingSignals` module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trading Signals Module for VIX Futures Trading (Phase 6)\n",
    "# Implements:\n",
    "# - In-sample and out-of-sample testing via k-fold cross-validation\n",
    "# - Performance metrics computation (returns, Sharpe ratio, drawdown)\n",
    "# - Results visualization and statistical analysis\n",
    "\n",
    "\n",
    "class VIXTradingSignals:\n",
    "    def __init__(self, n_folds=10, upper_threshold=0.5, lower_threshold=-0.5):\n",
    "        \"\"\"Initialize VIX trading signals generator.\n",
    "        \n",
    "        Args:\n",
    "            n_folds (int): Number of folds for cross-validation\n",
    "            upper_threshold (float): Upper threshold for long positions\n",
    "            lower_threshold (float): Lower threshold for short positions\n",
    "        \"\"\"\n",
    "        self.n_folds = n_folds\n",
    "        self.upper_threshold = upper_threshold\n",
    "        self.lower_threshold = lower_threshold\n",
    "        self.network = VIXTradingNetwork(input_dim=11, hidden_units=64, output_dim=1, use_prelu=True)\n",
    "        self.engine = VIXSimulationEngine()\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def generate_signals(self, state_vectors):\n",
    "        \"\"\"Generate trading signals from state vectors.\n",
    "        \n",
    "        Args:\n",
    "            state_vectors (np.ndarray): State vectors\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Trading signals (-1, 0, 1)\n",
    "        \"\"\"\n",
    "        # Get expected utilities from neural network\n",
    "        expected_utilities = self.network.predict(state_vectors)\n",
    "        \n",
    "        # Convert predictions to signals\n",
    "        signals = np.zeros_like(expected_utilities)\n",
    "        signals[expected_utilities > self.upper_threshold] = 1\n",
    "        signals[expected_utilities < self.lower_threshold] = -1\n",
    "        \n",
    "        return signals\n",
    "    \n",
    "    def compute_performance_metrics(self, signals, returns, transaction_cost=0.0):\n",
    "        \"\"\"Compute performance metrics for trading signals.\n",
    "        \n",
    "        Args:\n",
    "            signals (np.ndarray): Trading signals (-1, 0, 1)\n",
    "            returns (np.ndarray): Asset returns\n",
    "            transaction_cost (float): Transaction cost as decimal\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary of performance metrics\n",
    "        \"\"\"\n",
    "        # Debug: Print shapes\n",
    "        print(f\"[DEBUG] signals shape: {np.shape(signals)}, returns shape: {np.shape(returns)}\")\n",
    "        \n",
    "        # Ensure signals and returns are 1D arrays\n",
    "        signals = signals.flatten()\n",
    "        returns = returns.flatten()\n",
    "        \n",
    "        # Calculate strategy returns\n",
    "        strategy_returns = signals.reshape(-1) * returns\n",
    "        \n",
    "        # Apply transaction costs\n",
    "        if transaction_cost > 0:\n",
    "            # Calculate position changes\n",
    "            position_changes = np.abs(np.diff(signals))\n",
    "            # Add transaction costs\n",
    "            strategy_returns[1:] -= position_changes * transaction_cost\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_return = np.sum(strategy_returns)\n",
    "        sharpe_ratio = np.mean(strategy_returns) / np.std(strategy_returns) * np.sqrt(252) if np.std(strategy_returns) > 0 else 0\n",
    "        max_drawdown = self._calculate_max_drawdown(strategy_returns)\n",
    "        avg_return = np.mean(strategy_returns)\n",
    "        std_return = np.std(strategy_returns)\n",
    "        hit_ratio = np.mean(strategy_returns > 0)\n",
    "        annual_return = avg_return * 252\n",
    "        turnover = np.mean(np.abs(np.diff(signals)))\n",
    "        return {\n",
    "            'total_return': total_return,\n",
    "            'sharpe_ratio': sharpe_ratio,\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'avg_return': avg_return,\n",
    "            'std_return': std_return,\n",
    "            'hit_ratio': hit_ratio,\n",
    "            'annual_return': annual_return,\n",
    "            'turnover': turnover\n",
    "        }\n",
    "    \n",
    "    def run_cross_validation(self, n_steps=1000, time_series_split=True):\n",
    "        \"\"\"\n",
    "        Run k-fold cross-validation on the trading strategy.\n",
    "        \n",
    "        Args:\n",
    "            n_steps (int): Number of steps to simulate\n",
    "            time_series_split (bool): Whether to use time series split instead of random\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with cross-validation results\n",
    "        \"\"\"\n",
    "        # Generate simulation data\n",
    "        simulation_results = self.engine.simulate_trading_path(n_steps=n_steps)\n",
    "        state_vectors = simulation_results['state_vectors'].values\n",
    "        returns = simulation_results['returns']\n",
    "        \n",
    "        # Initialize results storage\n",
    "        cv_results = {\n",
    "            'in_sample': [],\n",
    "            'out_of_sample': [],\n",
    "            'fold_indices': []\n",
    "        }\n",
    "        \n",
    "        # Choose cross-validation method\n",
    "        if time_series_split:\n",
    "            cv = TimeSeriesSplit(n_splits=self.n_folds)\n",
    "        else:\n",
    "            cv = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n",
    "        \n",
    "        for fold, (train_idx, test_idx) in enumerate(cv.split(state_vectors)):\n",
    "            print(f\"\\nProcessing fold {fold + 1}/{self.n_folds}\")\n",
    "            \n",
    "            # Split data\n",
    "            X_train = state_vectors[train_idx]\n",
    "            X_test = state_vectors[test_idx]\n",
    "            \n",
    "            # Train neural network\n",
    "            self.network = VIXTradingNetwork()  # Reset network for each fold\n",
    "            X_train_scaled, _, y_train, _ = self.network.prepare_data(\n",
    "                X_train, \n",
    "                np.array(list(returns.values())).T[train_idx]\n",
    "            )\n",
    "            self.network.train(X_train_scaled, y_train, X_train_scaled, y_train)\n",
    "            \n",
    "            # Generate signals for both sets\n",
    "            train_signals = self.generate_signals(X_train)\n",
    "            test_signals = self.generate_signals(X_test)\n",
    "            \n",
    "            # Compute performance metrics\n",
    "            train_metrics = self.compute_performance_metrics(train_signals, returns[list(returns.keys())[0]][train_idx])\n",
    "            test_metrics = self.compute_performance_metrics(test_signals, returns[list(returns.keys())[0]][test_idx])\n",
    "            \n",
    "            cv_results['in_sample'].append(train_metrics)\n",
    "            cv_results['out_of_sample'].append(test_metrics)\n",
    "            cv_results['fold_indices'].append({'train': train_idx, 'test': test_idx})\n",
    "            \n",
    "            # Save fold results\n",
    "            self._save_fold_results(fold, train_metrics, test_metrics)\n",
    "            # Save signals for this fold\n",
    "            pd.DataFrame({'index': train_idx, 'signal': train_signals.flatten()}).to_csv(f'data/fold_output/fold_{fold+1}_train_signals.csv', index=False)\n",
    "            pd.DataFrame({'index': test_idx, 'signal': test_signals.flatten()}).to_csv(f'data/fold_output/fold_{fold+1}_test_signals.csv', index=False)\n",
    "        \n",
    "        # Generate summary report\n",
    "        self._generate_cv_summary(cv_results)\n",
    "        \n",
    "        return cv_results\n",
    "    \n",
    "    def _save_fold_results(self, fold, train_metrics, test_metrics):\n",
    "        \"\"\"Save results for each fold to a CSV file.\"\"\"\n",
    "        fold_df = pd.DataFrame({\n",
    "            'Metric': list(train_metrics.keys()),\n",
    "            'In-Sample': list(train_metrics.values()),\n",
    "            'Out-of-Sample': list(test_metrics.values())\n",
    "        })\n",
    "        fold_df.to_csv(f'data/fold_output/fold_{fold+1}_results.csv', index=False)\n",
    "    \n",
    "    def _generate_cv_summary(self, cv_results):\n",
    "        \"\"\"Generate summary statistics for cross-validation results.\"\"\"\n",
    "        # Convert results to DataFrames\n",
    "        in_sample_df = pd.DataFrame(cv_results['in_sample'])\n",
    "        out_sample_df = pd.DataFrame(cv_results['out_of_sample'])\n",
    "        \n",
    "        # Compute summary statistics\n",
    "        summary = pd.DataFrame({\n",
    "            'In-Sample Mean': in_sample_df.mean(),\n",
    "            'In-Sample Std': in_sample_df.std(),\n",
    "            'Out-of-Sample Mean': out_sample_df.mean(),\n",
    "            'Out-of-Sample Std': out_sample_df.std()\n",
    "        })\n",
    "        \n",
    "        # Save summary\n",
    "        summary.to_csv('data/fold_output/cross_validation_summary.csv')\n",
    "        \n",
    "        # Create visualization\n",
    "        self.plot_cross_validation_results(cv_results)\n",
    "    \n",
    "    def plot_cross_validation_results(self, cv_results):\n",
    "        \"\"\"\n",
    "        Create comprehensive visualization of cross-validation results.\n",
    "        \n",
    "        Args:\n",
    "            cv_results: Dictionary with cross-validation results\n",
    "        \"\"\"\n",
    "        metrics_to_plot = ['sharpe_ratio', 'avg_return', 'std_return', 'max_drawdown']\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Cross-Validation Results Analysis', fontsize=16)\n",
    "        \n",
    "        for idx, metric in enumerate(metrics_to_plot):\n",
    "            ax = axes[idx // 2, idx % 2]\n",
    "            \n",
    "            # Get data for plotting\n",
    "            in_sample_data = [result[metric] for result in cv_results['in_sample']]\n",
    "            out_sample_data = [result[metric] for result in cv_results['out_of_sample']]\n",
    "            \n",
    "            # Create violin plot\n",
    "            ax.violinplot([in_sample_data, out_sample_data])\n",
    "            ax.set_xticks([1, 2])\n",
    "            ax.set_xticklabels(['In-Sample', 'Out-of-Sample'])\n",
    "            ax.set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "            ax.grid(True)\n",
    "            \n",
    "            # Add mean and std annotations\n",
    "            ax.axhline(y=np.mean(in_sample_data), color='r', linestyle='--', alpha=0.5)\n",
    "            ax.axhline(y=np.mean(out_sample_data), color='b', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figures/cross_validation_analysis.png')\n",
    "        plt.close()\n",
    "\n",
    "    def _calculate_max_drawdown(self, returns):\n",
    "        \"\"\"Calculate maximum drawdown from returns.\n",
    "        \n",
    "        Args:\n",
    "            returns (np.ndarray): Array of returns\n",
    "            \n",
    "        Returns:\n",
    "            float: Maximum drawdown\n",
    "        \"\"\"\n",
    "        # Calculate cumulative returns\n",
    "        cumulative_returns = np.cumsum(returns)\n",
    "        \n",
    "        # Calculate running maximum\n",
    "        running_max = np.maximum.accumulate(cumulative_returns)\n",
    "        \n",
    "        # Calculate drawdowns\n",
    "        drawdowns = cumulative_returns - running_max\n",
    "        \n",
    "        # Return maximum drawdown\n",
    "        return np.min(drawdowns)\n",
    "\n",
    "    def analyze_transaction_costs(self, n_steps=1000, costs=[0.002, 0.003, 0.004]):\n",
    "        \"\"\"\n",
    "        Analyze sensitivity to transaction costs.\n",
    "        \n",
    "        Args:\n",
    "            n_steps (int): Number of steps to simulate\n",
    "            costs (list): List of transaction costs to analyze (in decimal)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with cost sensitivity results\n",
    "        \"\"\"\n",
    "        # Generate simulation data\n",
    "        simulation_results = self.engine.simulate_trading_path(n_steps=n_steps)\n",
    "        state_vectors = simulation_results['state_vectors'].values\n",
    "        returns = simulation_results['returns']\n",
    "        \n",
    "        # Initialize results storage\n",
    "        cost_results = []\n",
    "        \n",
    "        # Train model once\n",
    "        self.network = VIXTradingNetwork()\n",
    "        X_scaled, _, y, _ = self.network.prepare_data(\n",
    "            state_vectors,\n",
    "            np.array(list(returns.values())).T\n",
    "        )\n",
    "        self.network.train(X_scaled, y, X_scaled, y)\n",
    "        \n",
    "        # Generate signals\n",
    "        signals = self.generate_signals(state_vectors)\n",
    "        \n",
    "        # Analyze each cost level\n",
    "        for cost in costs:\n",
    "            metrics = self.compute_performance_metrics(signals, returns[list(returns.keys())[0]], transaction_cost=cost)\n",
    "            metrics['transaction_cost'] = cost * 10000  # Convert to basis points\n",
    "            cost_results.append(metrics)\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        results_df = pd.DataFrame(cost_results)\n",
    "        \n",
    "        # Save results\n",
    "        results_df.to_csv('data/fold_output/cost_sensitivity_summary.csv', index=False)\n",
    "        \n",
    "        # Create visualization\n",
    "        self.plot_cost_sensitivity(results_df)\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def plot_cost_sensitivity(self, results_df):\n",
    "        \"\"\"\n",
    "        Create visualization of cost sensitivity analysis.\n",
    "        \n",
    "        Args:\n",
    "            results_df (DataFrame): Results from cost sensitivity analysis\n",
    "        \"\"\"\n",
    "        metrics_to_plot = ['sharpe_ratio', 'total_return', 'hit_ratio']\n",
    "        fig, axes = plt.subplots(1, len(metrics_to_plot), figsize=(15, 5))\n",
    "        fig.suptitle('Transaction Cost Sensitivity Analysis', fontsize=16)\n",
    "        \n",
    "        for idx, metric in enumerate(metrics_to_plot):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Plot metric vs cost\n",
    "            ax.plot(results_df['transaction_cost'], results_df[metric], 'o-', label=metric)\n",
    "            ax.set_xlabel('Transaction Cost (bps)')\n",
    "            ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "            ax.grid(True)\n",
    "            ax.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figures/cost_sensitivity_analysis.png')\n",
    "        plt.close()\n",
    "\n",
    "    def run_non_contiguous_analysis(self, n_steps=1000):\n",
    "        \"\"\"\n",
    "        Run analysis with non-contiguous folds to test robustness.\n",
    "        \n",
    "        Args:\n",
    "            n_steps (int): Number of steps to simulate\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with non-contiguous analysis results\n",
    "        \"\"\"\n",
    "        # Generate simulation data\n",
    "        simulation_results = self.engine.simulate_trading_path(n_steps=n_steps)\n",
    "        state_vectors = simulation_results['state_vectors'].values\n",
    "        returns = simulation_results['returns']\n",
    "        \n",
    "        # Initialize results storage\n",
    "        non_contiguous_results = {\n",
    "            'in_sample': [],\n",
    "            'out_of_sample': [],\n",
    "            'fold_indices': []\n",
    "        }\n",
    "        \n",
    "        # Use KFold instead of TimeSeriesSplit for non-contiguous folds\n",
    "        cv = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n",
    "        \n",
    "        for fold, (train_idx, test_idx) in enumerate(cv.split(state_vectors)):\n",
    "            print(f\"\\nProcessing non-contiguous fold {fold + 1}/{self.n_folds}\")\n",
    "            \n",
    "            # Split data\n",
    "            X_train = state_vectors[train_idx]\n",
    "            X_test = state_vectors[test_idx]\n",
    "            \n",
    "            # Train neural network\n",
    "            self.network = VIXTradingNetwork()  # Reset network for each fold\n",
    "            X_train_scaled, _, y_train, _ = self.network.prepare_data(\n",
    "                X_train, \n",
    "                np.array(list(returns.values())).T[train_idx]\n",
    "            )\n",
    "            self.network.train(X_train_scaled, y_train, X_train_scaled, y_train)\n",
    "            \n",
    "            # Generate signals for both sets\n",
    "            train_signals = self.generate_signals(X_train)\n",
    "            test_signals = self.generate_signals(X_test)\n",
    "            \n",
    "            # Compute performance metrics\n",
    "            train_metrics = self.compute_performance_metrics(train_signals, returns[list(returns.keys())[0]][train_idx])\n",
    "            test_metrics = self.compute_performance_metrics(test_signals, returns[list(returns.keys())[0]][test_idx])\n",
    "            \n",
    "            non_contiguous_results['in_sample'].append(train_metrics)\n",
    "            non_contiguous_results['out_of_sample'].append(test_metrics)\n",
    "            non_contiguous_results['fold_indices'].append({'train': train_idx, 'test': test_idx})\n",
    "            \n",
    "            # Save fold results\n",
    "            self._save_non_contiguous_fold_results(fold, train_metrics, test_metrics)\n",
    "        \n",
    "        # Generate summary report\n",
    "        self._generate_non_contiguous_summary(non_contiguous_results)\n",
    "        \n",
    "        return non_contiguous_results\n",
    "    \n",
    "    def _save_non_contiguous_fold_results(self, fold, train_metrics, test_metrics):\n",
    "        \"\"\"Save results for each non-contiguous fold to a CSV file.\"\"\"\n",
    "        fold_df = pd.DataFrame({\n",
    "            'Metric': list(train_metrics.keys()),\n",
    "            'In-Sample': list(train_metrics.values()),\n",
    "            'Out-of-Sample': list(test_metrics.values())\n",
    "        })\n",
    "        fold_df.to_csv(f'data/raw/non_contiguous_fold_{fold+1}_results.csv', index=False)\n",
    "    \n",
    "    def _generate_non_contiguous_summary(self, results):\n",
    "        \"\"\"Generate summary statistics for non-contiguous analysis results.\"\"\"\n",
    "        # Convert results to DataFrames\n",
    "        in_sample_df = pd.DataFrame(results['in_sample'])\n",
    "        out_sample_df = pd.DataFrame(results['out_of_sample'])\n",
    "        \n",
    "        # Compute summary statistics\n",
    "        summary = pd.DataFrame({\n",
    "            'In-Sample Mean': in_sample_df.mean(),\n",
    "            'In-Sample Std': in_sample_df.std(),\n",
    "            'Out-of-Sample Mean': out_sample_df.mean(),\n",
    "            'Out-of-Sample Std': out_sample_df.std()\n",
    "        })\n",
    "        \n",
    "        # Save summary\n",
    "        summary.to_csv('data/fold_output/non_contiguous_summary.csv')\n",
    "        \n",
    "        # Create visualization\n",
    "        self.plot_non_contiguous_results(results)\n",
    "    \n",
    "    def plot_non_contiguous_results(self, results):\n",
    "        \"\"\"\n",
    "        Create visualization of non-contiguous analysis results.\n",
    "        \n",
    "        Args:\n",
    "            results: Dictionary with non-contiguous analysis results\n",
    "        \"\"\"\n",
    "        metrics_to_plot = ['sharpe_ratio', 'avg_return', 'std_return', 'max_drawdown']\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Non-Contiguous Fold Analysis Results', fontsize=16)\n",
    "        \n",
    "        for idx, metric in enumerate(metrics_to_plot):\n",
    "            ax = axes[idx // 2, idx % 2]\n",
    "            \n",
    "            # Get data for plotting\n",
    "            in_sample_data = [result[metric] for result in results['in_sample']]\n",
    "            out_sample_data = [result[metric] for result in results['out_of_sample']]\n",
    "            \n",
    "            # Create violin plot\n",
    "            ax.violinplot([in_sample_data, out_sample_data])\n",
    "            ax.set_xticks([1, 2])\n",
    "            ax.set_xticklabels(['In-Sample', 'Out-of-Sample'])\n",
    "            ax.set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "            ax.grid(True)\n",
    "            \n",
    "            # Add mean and std annotations\n",
    "            ax.axhline(y=np.mean(in_sample_data), color='r', linestyle='--', alpha=0.5)\n",
    "            ax.axhline(y=np.mean(out_sample_data), color='b', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figures/non_contiguous_analysis.png')\n",
    "        plt.close()\n",
    "\n",
    "def run_trading_signals():\n",
    "    \"\"\"Main function to run the trading signals implementation.\"\"\"\n",
    "    print(\"Starting Phase 6: In-Sample & Out-of-Sample Testing...\")\n",
    "    \n",
    "    # Initialize trading signals\n",
    "    signals = VIXTradingSignals(n_folds=10)\n",
    "    \n",
    "    # Run cross-validation with time series split\n",
    "    print(\"\\nRunning 10-fold cross-validation...\")\n",
    "    cv_results = signals.run_cross_validation(n_steps=1000, time_series_split=True)\n",
    "    \n",
    "    # Run transaction cost sensitivity analysis\n",
    "    print(\"\\nRunning transaction cost sensitivity analysis...\")\n",
    "    cost_results = signals.analyze_transaction_costs(n_steps=1000, costs=[0.002, 0.0025, 0.003, 0.0035, 0.004])\n",
    "    \n",
    "    # Run non-contiguous fold analysis\n",
    "    print(\"\\nRunning non-contiguous fold analysis...\")\n",
    "    non_contiguous_results = signals.run_non_contiguous_analysis(n_steps=1000)\n",
    "    \n",
    "    print(\"\\nResults have been saved to:\")\n",
    "    print(\"- Individual fold results: data/raw/fold_*_results.csv\")\n",
    "    print(\"- Summary statistics: data/raw/cross_validation_summary.csv\")\n",
    "    print(\"- Visualization: data/raw/cross_validation_analysis.png\")\n",
    "    print(\"- Cost sensitivity: data/raw/cost_sensitivity_summary.csv\")\n",
    "    print(\"- Cost sensitivity plot: data/raw/cost_sensitivity_analysis.png\")\n",
    "    print(\"- Non-contiguous fold results: data/raw/non_contiguous_fold_*_results.csv\")\n",
    "    print(\"- Non-contiguous summary: data/raw/non_contiguous_summary.csv\")\n",
    "    print(\"- Non-contiguous analysis plot: data/raw/non_contiguous_analysis.png\")\n",
    "    \n",
    "    print(\"\\nPhase 6 completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Phase 6: In-Sample & Out-of-Sample Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gd/qxxh82n95f57fhdhrdg746g80000gn/T/ipykernel_14771/207703377.py:28: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  self.state_vectors['date'] = pd.to_datetime(self.state_vectors['date'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded state vectors with shape: (3190, 12)\n",
      "Date range: 2008-04-01 00:00:00-05:00 to 2020-11-27 00:00:00-06:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gd/qxxh82n95f57fhdhrdg746g80000gn/T/ipykernel_14771/207703377.py:105: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  model_data = model_data.fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully fit VAR model with 10 lags\n",
      "\n",
      "Running 10-fold cross-validation...\n",
      "\n",
      "Processing fold 1/10\n",
      "Epoch 1/100\n",
      "3/3 [==============================] - 1s 69ms/step - loss: 1.3383 - mae: 0.9317 - val_loss: 0.7453 - val_mae: 0.7555\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.1466 - mae: 0.8995 - val_loss: 0.7433 - val_mae: 0.7516\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.1966 - mae: 0.8971 - val_loss: 0.7412 - val_mae: 0.7466\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.2311 - mae: 0.9137 - val_loss: 0.7405 - val_mae: 0.7441\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.1680 - mae: 0.9004 - val_loss: 0.7407 - val_mae: 0.7452\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.1956 - mae: 0.9066 - val_loss: 0.7407 - val_mae: 0.7452\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.1557 - mae: 0.9000 - val_loss: 0.7408 - val_mae: 0.7471\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.1928 - mae: 0.9005 - val_loss: 0.7411 - val_mae: 0.7483\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.1739 - mae: 0.8955 - val_loss: 0.7413 - val_mae: 0.7493\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.2050 - mae: 0.9138 - val_loss: 0.7417 - val_mae: 0.7508\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.0933 - mae: 0.8825 - val_loss: 0.7417 - val_mae: 0.7512\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.1737 - mae: 0.8970 - val_loss: 0.7417 - val_mae: 0.7505\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.1238 - mae: 0.8869 - val_loss: 0.7423 - val_mae: 0.7516\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.1181 - mae: 0.8783 - val_loss: 0.7433 - val_mae: 0.7551\n",
      "4/4 [==============================] - 0s 904us/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "[DEBUG] signals shape: (100, 1), returns shape: (100,)\n",
      "[DEBUG] signals shape: (90, 1), returns shape: (90,)\n",
      "\n",
      "Processing fold 2/10\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 2s 39ms/step - loss: 0.9489 - mae: 0.7950 - val_loss: 0.4058 - val_mae: 0.4162\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.8901 - mae: 0.7908 - val_loss: 0.3978 - val_mae: 0.4084\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.9020 - mae: 0.7643 - val_loss: 0.3958 - val_mae: 0.4027\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.8660 - mae: 0.7472 - val_loss: 0.3967 - val_mae: 0.4027\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.8457 - mae: 0.7549 - val_loss: 0.3984 - val_mae: 0.4050\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.8819 - mae: 0.7455 - val_loss: 0.4049 - val_mae: 0.4097\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.8664 - mae: 0.7452 - val_loss: 0.4144 - val_mae: 0.4197\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.8141 - mae: 0.7248 - val_loss: 0.4236 - val_mae: 0.4283\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.8233 - mae: 0.7168 - val_loss: 0.4261 - val_mae: 0.4309\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.7958 - mae: 0.7075 - val_loss: 0.4273 - val_mae: 0.4282\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.8245 - mae: 0.7357 - val_loss: 0.4294 - val_mae: 0.4243\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.7844 - mae: 0.7215 - val_loss: 0.4309 - val_mae: 0.4261\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.7636 - mae: 0.7033 - val_loss: 0.4306 - val_mae: 0.4383\n",
      "6/6 [==============================] - 0s 760us/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "[DEBUG] signals shape: (190, 1), returns shape: (190,)\n",
      "[DEBUG] signals shape: (90, 1), returns shape: (90,)\n",
      "\n",
      "Processing fold 3/10\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 1s 24ms/step - loss: 0.7273 - mae: 0.6871 - val_loss: 0.2594 - val_mae: 0.2876\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6715 - mae: 0.6618 - val_loss: 0.2642 - val_mae: 0.3098\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6352 - mae: 0.6001 - val_loss: 0.2653 - val_mae: 0.3084\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6655 - mae: 0.6484 - val_loss: 0.2643 - val_mae: 0.3052\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6160 - mae: 0.6102 - val_loss: 0.2608 - val_mae: 0.2934\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5866 - mae: 0.5854 - val_loss: 0.2572 - val_mae: 0.2798\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5671 - mae: 0.5826 - val_loss: 0.2552 - val_mae: 0.2777\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5476 - mae: 0.5670 - val_loss: 0.2541 - val_mae: 0.2656\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5974 - mae: 0.6000 - val_loss: 0.2539 - val_mae: 0.2595\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5752 - mae: 0.5783 - val_loss: 0.2544 - val_mae: 0.2603\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5631 - mae: 0.5650 - val_loss: 0.2545 - val_mae: 0.2578\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5326 - mae: 0.5418 - val_loss: 0.2553 - val_mae: 0.2605\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5124 - mae: 0.5196 - val_loss: 0.2563 - val_mae: 0.2804\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4902 - mae: 0.4916 - val_loss: 0.2600 - val_mae: 0.3163\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4876 - mae: 0.5206 - val_loss: 0.2637 - val_mae: 0.3326\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4761 - mae: 0.5029 - val_loss: 0.2580 - val_mae: 0.3054\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4856 - mae: 0.5032 - val_loss: 0.2561 - val_mae: 0.2905\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4956 - mae: 0.5344 - val_loss: 0.2568 - val_mae: 0.2894\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4584 - mae: 0.4984 - val_loss: 0.2579 - val_mae: 0.3021\n",
      "9/9 [==============================] - 0s 668us/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "[DEBUG] signals shape: (280, 1), returns shape: (280,)\n",
      "[DEBUG] signals shape: (90, 1), returns shape: (90,)\n",
      "\n",
      "Processing fold 4/10\n",
      "Epoch 1/100\n",
      "10/10 [==============================] - 2s 17ms/step - loss: 0.6786 - mae: 0.6869 - val_loss: 0.1935 - val_mae: 0.2131\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.6751 - mae: 0.6767 - val_loss: 0.1941 - val_mae: 0.2200\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6119 - mae: 0.6324 - val_loss: 0.1924 - val_mae: 0.2218\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.5773 - mae: 0.6121 - val_loss: 0.1871 - val_mae: 0.1976\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.5267 - mae: 0.5772 - val_loss: 0.1888 - val_mae: 0.2385\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.5220 - mae: 0.5575 - val_loss: 0.1914 - val_mae: 0.2544\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.5322 - mae: 0.5722 - val_loss: 0.1896 - val_mae: 0.2319\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.5109 - mae: 0.5592 - val_loss: 0.1901 - val_mae: 0.2411\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4856 - mae: 0.5350 - val_loss: 0.1988 - val_mae: 0.2830\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4800 - mae: 0.5377 - val_loss: 0.1952 - val_mae: 0.2547\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4796 - mae: 0.5136 - val_loss: 0.1931 - val_mae: 0.2181\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4361 - mae: 0.4903 - val_loss: 0.1926 - val_mae: 0.2214\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4676 - mae: 0.5147 - val_loss: 0.1941 - val_mae: 0.2110\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4153 - mae: 0.4867 - val_loss: 0.1939 - val_mae: 0.2264\n",
      "12/12 [==============================] - 0s 632us/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "[DEBUG] signals shape: (370, 1), returns shape: (370,)\n",
      "[DEBUG] signals shape: (90, 1), returns shape: (90,)\n",
      "\n",
      "Processing fold 5/10\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 17ms/step - loss: 0.6911 - mae: 0.7108 - val_loss: 0.1528 - val_mae: 0.1664\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.6361 - mae: 0.6656 - val_loss: 0.1532 - val_mae: 0.1630\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.5898 - mae: 0.6390 - val_loss: 0.1525 - val_mae: 0.1749\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5439 - mae: 0.6045 - val_loss: 0.1614 - val_mae: 0.2408\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5313 - mae: 0.5919 - val_loss: 0.1680 - val_mae: 0.2661\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.5217 - mae: 0.5746 - val_loss: 0.1623 - val_mae: 0.2423\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4807 - mae: 0.5488 - val_loss: 0.1639 - val_mae: 0.2507\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4671 - mae: 0.5399 - val_loss: 0.2069 - val_mae: 0.3635\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4355 - mae: 0.5118 - val_loss: 0.2156 - val_mae: 0.3803\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4446 - mae: 0.5309 - val_loss: 0.2035 - val_mae: 0.3565\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4170 - mae: 0.5002 - val_loss: 0.1715 - val_mae: 0.2675\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3884 - mae: 0.4714 - val_loss: 0.1666 - val_mae: 0.2528\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3965 - mae: 0.4771 - val_loss: 0.1643 - val_mae: 0.2351\n",
      "15/15 [==============================] - 0s 588us/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "[DEBUG] signals shape: (460, 1), returns shape: (460,)\n",
      "[DEBUG] signals shape: (90, 1), returns shape: (90,)\n",
      "\n",
      "Processing fold 6/10\n",
      "Epoch 1/100\n",
      "14/14 [==============================] - 1s 14ms/step - loss: 0.6819 - mae: 0.7142 - val_loss: 0.1235 - val_mae: 0.1288\n",
      "Epoch 2/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6078 - mae: 0.6562 - val_loss: 0.1243 - val_mae: 0.1337\n",
      "Epoch 3/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5641 - mae: 0.6236 - val_loss: 0.1239 - val_mae: 0.1256\n",
      "Epoch 4/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5351 - mae: 0.5959 - val_loss: 0.1285 - val_mae: 0.1819\n",
      "Epoch 5/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4952 - mae: 0.5698 - val_loss: 0.1222 - val_mae: 0.1524\n",
      "Epoch 6/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4861 - mae: 0.5743 - val_loss: 0.1214 - val_mae: 0.1240\n",
      "Epoch 7/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4694 - mae: 0.5483 - val_loss: 0.1227 - val_mae: 0.1428\n",
      "Epoch 8/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4241 - mae: 0.5080 - val_loss: 0.1366 - val_mae: 0.1998\n",
      "Epoch 9/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4187 - mae: 0.5017 - val_loss: 0.1332 - val_mae: 0.1684\n",
      "Epoch 10/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4311 - mae: 0.5242 - val_loss: 0.1288 - val_mae: 0.1725\n",
      "Epoch 11/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.3797 - mae: 0.4730 - val_loss: 0.1340 - val_mae: 0.1491\n",
      "Epoch 12/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3674 - mae: 0.4714 - val_loss: 0.1422 - val_mae: 0.1834\n",
      "Epoch 13/100\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 0.3520 - mae: 0.4488 - val_loss: 0.1376 - val_mae: 0.1318\n",
      "Epoch 14/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3462 - mae: 0.4389 - val_loss: 0.1266 - val_mae: 0.1742\n",
      "Epoch 15/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.3371 - mae: 0.4253 - val_loss: 0.1372 - val_mae: 0.1692\n",
      "Epoch 16/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.3126 - mae: 0.4088 - val_loss: 0.1530 - val_mae: 0.1387\n",
      "18/18 [==============================] - 0s 580us/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "[DEBUG] signals shape: (550, 1), returns shape: (550,)\n",
      "[DEBUG] signals shape: (90, 1), returns shape: (90,)\n",
      "\n",
      "Processing fold 7/10\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 1s 13ms/step - loss: 0.6145 - mae: 0.6654 - val_loss: 0.1292 - val_mae: 0.1238\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5165 - mae: 0.5892 - val_loss: 0.1107 - val_mae: 0.1369\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4893 - mae: 0.5697 - val_loss: 0.1091 - val_mae: 0.1359\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4295 - mae: 0.5149 - val_loss: 0.1157 - val_mae: 0.1841\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4131 - mae: 0.5049 - val_loss: 0.1125 - val_mae: 0.1132\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3885 - mae: 0.4877 - val_loss: 0.1121 - val_mae: 0.1237\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3519 - mae: 0.4458 - val_loss: 0.1079 - val_mae: 0.1467\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3345 - mae: 0.4347 - val_loss: 0.1081 - val_mae: 0.1428\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3165 - mae: 0.4257 - val_loss: 0.1074 - val_mae: 0.1181\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2960 - mae: 0.4055 - val_loss: 0.1111 - val_mae: 0.1227\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2930 - mae: 0.3938 - val_loss: 0.1063 - val_mae: 0.1262\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2612 - mae: 0.3500 - val_loss: 0.1110 - val_mae: 0.1285\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2447 - mae: 0.3437 - val_loss: 0.1110 - val_mae: 0.1321\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2577 - mae: 0.3474 - val_loss: 0.1327 - val_mae: 0.1610\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2241 - mae: 0.3175 - val_loss: 0.1141 - val_mae: 0.1303\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2238 - mae: 0.3158 - val_loss: 0.1119 - val_mae: 0.1204\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2084 - mae: 0.2889 - val_loss: 0.1095 - val_mae: 0.1237\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1993 - mae: 0.2897 - val_loss: 0.1090 - val_mae: 0.1114\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1930 - mae: 0.2681 - val_loss: 0.1173 - val_mae: 0.1504\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1918 - mae: 0.2629 - val_loss: 0.1092 - val_mae: 0.1266\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1856 - mae: 0.2650 - val_loss: 0.1156 - val_mae: 0.1258\n",
      "20/20 [==============================] - 0s 580us/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "[DEBUG] signals shape: (640, 1), returns shape: (640,)\n",
      "[DEBUG] signals shape: (90, 1), returns shape: (90,)\n",
      "\n",
      "Processing fold 8/10\n",
      "Epoch 1/100\n",
      "19/19 [==============================] - 2s 9ms/step - loss: 0.5882 - mae: 0.6529 - val_loss: 0.1150 - val_mae: 0.1280\n",
      "Epoch 2/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5287 - mae: 0.6094 - val_loss: 0.1160 - val_mae: 0.1765\n",
      "Epoch 3/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4555 - mae: 0.5589 - val_loss: 0.1122 - val_mae: 0.1607\n",
      "Epoch 4/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4234 - mae: 0.5267 - val_loss: 0.1005 - val_mae: 0.1223\n",
      "Epoch 5/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3977 - mae: 0.5035 - val_loss: 0.0998 - val_mae: 0.1326\n",
      "Epoch 6/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3641 - mae: 0.4698 - val_loss: 0.0991 - val_mae: 0.1367\n",
      "Epoch 7/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3217 - mae: 0.4334 - val_loss: 0.0998 - val_mae: 0.1290\n",
      "Epoch 8/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3376 - mae: 0.4510 - val_loss: 0.1014 - val_mae: 0.1076\n",
      "Epoch 9/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3104 - mae: 0.4150 - val_loss: 0.1048 - val_mae: 0.1517\n",
      "Epoch 10/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.2749 - mae: 0.3848 - val_loss: 0.1072 - val_mae: 0.1932\n",
      "Epoch 11/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.2666 - mae: 0.3740 - val_loss: 0.1152 - val_mae: 0.1404\n",
      "Epoch 12/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.2460 - mae: 0.3398 - val_loss: 0.1211 - val_mae: 0.1119\n",
      "Epoch 13/100\n",
      "19/19 [==============================] - 0s 12ms/step - loss: 0.2412 - mae: 0.3412 - val_loss: 0.1069 - val_mae: 0.1019\n",
      "Epoch 14/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.2056 - mae: 0.2939 - val_loss: 0.0991 - val_mae: 0.1101\n",
      "Epoch 15/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.2059 - mae: 0.2970 - val_loss: 0.0950 - val_mae: 0.1029\n",
      "Epoch 16/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.2072 - mae: 0.2984 - val_loss: 0.1010 - val_mae: 0.1247\n",
      "Epoch 17/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1913 - mae: 0.2737 - val_loss: 0.1000 - val_mae: 0.1253\n",
      "Epoch 18/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1930 - mae: 0.2677 - val_loss: 0.0961 - val_mae: 0.1010\n",
      "Epoch 19/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1829 - mae: 0.2618 - val_loss: 0.0998 - val_mae: 0.1179\n",
      "Epoch 20/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1622 - mae: 0.2428 - val_loss: 0.1028 - val_mae: 0.1276\n",
      "Epoch 21/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1603 - mae: 0.2394 - val_loss: 0.1119 - val_mae: 0.1102\n",
      "Epoch 22/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1577 - mae: 0.2267 - val_loss: 0.0975 - val_mae: 0.1166\n",
      "Epoch 23/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1627 - mae: 0.2308 - val_loss: 0.0996 - val_mae: 0.1167\n",
      "Epoch 24/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1465 - mae: 0.2054 - val_loss: 0.0980 - val_mae: 0.1131\n",
      "Epoch 25/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1446 - mae: 0.2025 - val_loss: 0.0997 - val_mae: 0.1167\n",
      "23/23 [==============================] - 0s 559us/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "[DEBUG] signals shape: (730, 1), returns shape: (730,)\n",
      "[DEBUG] signals shape: (90, 1), returns shape: (90,)\n",
      "\n",
      "Processing fold 9/10\n",
      "Epoch 1/100\n",
      "21/21 [==============================] - 1s 9ms/step - loss: 0.5629 - mae: 0.6479 - val_loss: 0.0891 - val_mae: 0.1244\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.4738 - mae: 0.5788 - val_loss: 0.0896 - val_mae: 0.1415\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.4534 - mae: 0.5584 - val_loss: 0.0970 - val_mae: 0.1806\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.3836 - mae: 0.4968 - val_loss: 0.0995 - val_mae: 0.1827\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.3842 - mae: 0.4908 - val_loss: 0.1042 - val_mae: 0.1910\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.3446 - mae: 0.4614 - val_loss: 0.1102 - val_mae: 0.1688\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.3288 - mae: 0.4567 - val_loss: 0.1042 - val_mae: 0.1918\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.2991 - mae: 0.4235 - val_loss: 0.0997 - val_mae: 0.1837\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.2888 - mae: 0.4096 - val_loss: 0.0991 - val_mae: 0.1813\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.2655 - mae: 0.3858 - val_loss: 0.1022 - val_mae: 0.1679\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.2434 - mae: 0.3637 - val_loss: 0.1032 - val_mae: 0.1934\n",
      "26/26 [==============================] - 0s 556us/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "[DEBUG] signals shape: (820, 1), returns shape: (820,)\n",
      "[DEBUG] signals shape: (90, 1), returns shape: (90,)\n",
      "\n",
      "Processing fold 10/10\n",
      "Epoch 1/100\n",
      "23/23 [==============================] - 1s 8ms/step - loss: 0.5273 - mae: 0.6159 - val_loss: 0.1253 - val_mae: 0.1099\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4734 - mae: 0.5774 - val_loss: 0.0963 - val_mae: 0.1216\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4387 - mae: 0.5503 - val_loss: 0.0809 - val_mae: 0.1214\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4214 - mae: 0.5384 - val_loss: 0.0814 - val_mae: 0.1033\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3420 - mae: 0.4671 - val_loss: 0.0791 - val_mae: 0.0958\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3187 - mae: 0.4403 - val_loss: 0.1025 - val_mae: 0.1881\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2988 - mae: 0.4282 - val_loss: 0.0840 - val_mae: 0.1092\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2731 - mae: 0.4014 - val_loss: 0.0868 - val_mae: 0.1078\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2523 - mae: 0.3852 - val_loss: 0.0878 - val_mae: 0.1242\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2453 - mae: 0.3682 - val_loss: 0.0915 - val_mae: 0.0913\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2261 - mae: 0.3519 - val_loss: 0.0846 - val_mae: 0.0966\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2014 - mae: 0.3168 - val_loss: 0.0813 - val_mae: 0.1150\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1827 - mae: 0.2953 - val_loss: 0.0799 - val_mae: 0.0959\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.1665 - mae: 0.2717 - val_loss: 0.0878 - val_mae: 0.1267\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1584 - mae: 0.2614 - val_loss: 0.0857 - val_mae: 0.0855\n",
      "29/29 [==============================] - 0s 549us/step\n",
      "3/3 [==============================] - 0s 984us/step\n",
      "[DEBUG] signals shape: (910, 1), returns shape: (910,)\n",
      "[DEBUG] signals shape: (90, 1), returns shape: (90,)\n",
      "\n",
      "Running transaction cost sensitivity analysis...\n",
      "Epoch 1/100\n",
      "25/25 [==============================] - 1s 7ms/step - loss: 0.5823 - mae: 0.6626 - val_loss: 0.0759 - val_mae: 0.1092\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4753 - mae: 0.5773 - val_loss: 0.0853 - val_mae: 0.1667\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4398 - mae: 0.5494 - val_loss: 0.0768 - val_mae: 0.0977\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.3959 - mae: 0.5151 - val_loss: 0.0825 - val_mae: 0.0954\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.3748 - mae: 0.4941 - val_loss: 0.0979 - val_mae: 0.1185\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.3345 - mae: 0.4618 - val_loss: 0.0942 - val_mae: 0.1045\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.3314 - mae: 0.4581 - val_loss: 0.0813 - val_mae: 0.1001\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2947 - mae: 0.4197 - val_loss: 0.0934 - val_mae: 0.2043\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2688 - mae: 0.3961 - val_loss: 0.0834 - val_mae: 0.1279\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2466 - mae: 0.3720 - val_loss: 0.0896 - val_mae: 0.1187\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2137 - mae: 0.3348 - val_loss: 0.0817 - val_mae: 0.1115\n",
      "32/32 [==============================] - 0s 580us/step\n",
      "[DEBUG] signals shape: (1000, 1), returns shape: (1000,)\n",
      "[DEBUG] signals shape: (1000, 1), returns shape: (1000,)\n",
      "[DEBUG] signals shape: (1000, 1), returns shape: (1000,)\n",
      "[DEBUG] signals shape: (1000, 1), returns shape: (1000,)\n",
      "[DEBUG] signals shape: (1000, 1), returns shape: (1000,)\n",
      "\n",
      "Running non-contiguous fold analysis...\n",
      "\n",
      "Processing non-contiguous fold 1/10\n",
      "Epoch 1/100\n",
      "23/23 [==============================] - 2s 8ms/step - loss: 0.5432 - mae: 0.6346 - val_loss: 0.0829 - val_mae: 0.0888\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4623 - mae: 0.5634 - val_loss: 0.0748 - val_mae: 0.0724\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4239 - mae: 0.5373 - val_loss: 0.0800 - val_mae: 0.1752\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3444 - mae: 0.4751 - val_loss: 0.0977 - val_mae: 0.2198\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3488 - mae: 0.4737 - val_loss: 0.0817 - val_mae: 0.1541\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2903 - mae: 0.4242 - val_loss: 0.0703 - val_mae: 0.0835\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2733 - mae: 0.3999 - val_loss: 0.0754 - val_mae: 0.1393\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2400 - mae: 0.3598 - val_loss: 0.0930 - val_mae: 0.1241\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2164 - mae: 0.3364 - val_loss: 0.1012 - val_mae: 0.1712\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1975 - mae: 0.3150 - val_loss: 0.0946 - val_mae: 0.1790\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1876 - mae: 0.3091 - val_loss: 0.0947 - val_mae: 0.1918\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1698 - mae: 0.2832 - val_loss: 0.0899 - val_mae: 0.1711\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1598 - mae: 0.2663 - val_loss: 0.0762 - val_mae: 0.0811\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1511 - mae: 0.2503 - val_loss: 0.0821 - val_mae: 0.1087\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1349 - mae: 0.2235 - val_loss: 0.0740 - val_mae: 0.1208\n",
      "Epoch 16/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1258 - mae: 0.2064 - val_loss: 0.0788 - val_mae: 0.0900\n",
      "29/29 [==============================] - 0s 558us/step\n",
      "4/4 [==============================] - 0s 823us/step\n",
      "[DEBUG] signals shape: (900, 1), returns shape: (900,)\n",
      "[DEBUG] signals shape: (100, 1), returns shape: (100,)\n",
      "\n",
      "Processing non-contiguous fold 2/10\n",
      "Epoch 1/100\n",
      "23/23 [==============================] - 2s 9ms/step - loss: 0.5383 - mae: 0.6378 - val_loss: 0.0749 - val_mae: 0.1056\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4722 - mae: 0.5783 - val_loss: 0.0893 - val_mae: 0.1940\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4364 - mae: 0.5521 - val_loss: 0.1298 - val_mae: 0.2988\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3761 - mae: 0.4949 - val_loss: 0.1056 - val_mae: 0.2471\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3514 - mae: 0.4743 - val_loss: 0.0897 - val_mae: 0.2016\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3220 - mae: 0.4500 - val_loss: 0.1158 - val_mae: 0.2226\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3209 - mae: 0.4487 - val_loss: 0.1452 - val_mae: 0.3091\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2832 - mae: 0.4123 - val_loss: 0.1384 - val_mae: 0.2916\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2577 - mae: 0.3834 - val_loss: 0.1403 - val_mae: 0.2568\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2449 - mae: 0.3701 - val_loss: 0.1201 - val_mae: 0.2174\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2284 - mae: 0.3470 - val_loss: 0.0967 - val_mae: 0.1770\n",
      "29/29 [==============================] - 0s 576us/step\n",
      "4/4 [==============================] - 0s 877us/step\n",
      "[DEBUG] signals shape: (900, 1), returns shape: (900,)\n",
      "[DEBUG] signals shape: (100, 1), returns shape: (100,)\n",
      "\n",
      "Processing non-contiguous fold 3/10\n",
      "Epoch 1/100\n",
      "23/23 [==============================] - 1s 8ms/step - loss: 0.5029 - mae: 0.6099 - val_loss: 0.0691 - val_mae: 0.1166\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4326 - mae: 0.5400 - val_loss: 0.0704 - val_mae: 0.1241\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3979 - mae: 0.5209 - val_loss: 0.0680 - val_mae: 0.0871\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3722 - mae: 0.4938 - val_loss: 0.0779 - val_mae: 0.1233\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3317 - mae: 0.4556 - val_loss: 0.0675 - val_mae: 0.0910\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3003 - mae: 0.4220 - val_loss: 0.0756 - val_mae: 0.0935\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2810 - mae: 0.4059 - val_loss: 0.0756 - val_mae: 0.0953\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2514 - mae: 0.3770 - val_loss: 0.0731 - val_mae: 0.1030\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2368 - mae: 0.3568 - val_loss: 0.0811 - val_mae: 0.0920\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2185 - mae: 0.3342 - val_loss: 0.0826 - val_mae: 0.1591\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2047 - mae: 0.3189 - val_loss: 0.0704 - val_mae: 0.0909\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1836 - mae: 0.2903 - val_loss: 0.0749 - val_mae: 0.1172\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1616 - mae: 0.2637 - val_loss: 0.0722 - val_mae: 0.1116\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1597 - mae: 0.2541 - val_loss: 0.0706 - val_mae: 0.0849\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1447 - mae: 0.2384 - val_loss: 0.0702 - val_mae: 0.1214\n",
      "29/29 [==============================] - 0s 586us/step\n",
      "4/4 [==============================] - 0s 946us/step\n",
      "[DEBUG] signals shape: (900, 1), returns shape: (900,)\n",
      "[DEBUG] signals shape: (100, 1), returns shape: (100,)\n",
      "\n",
      "Processing non-contiguous fold 4/10\n",
      "Epoch 1/100\n",
      "23/23 [==============================] - 1s 8ms/step - loss: 0.5444 - mae: 0.6300 - val_loss: 0.0740 - val_mae: 0.0984\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.4665 - mae: 0.5722 - val_loss: 0.0751 - val_mae: 0.1376\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3912 - mae: 0.5084 - val_loss: 0.0732 - val_mae: 0.1044\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3865 - mae: 0.5096 - val_loss: 0.1043 - val_mae: 0.2162\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3318 - mae: 0.4546 - val_loss: 0.1081 - val_mae: 0.2417\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3103 - mae: 0.4374 - val_loss: 0.0960 - val_mae: 0.2060\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2831 - mae: 0.4058 - val_loss: 0.0866 - val_mae: 0.1566\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2457 - mae: 0.3702 - val_loss: 0.1013 - val_mae: 0.2069\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2425 - mae: 0.3652 - val_loss: 0.1206 - val_mae: 0.2677\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2260 - mae: 0.3405 - val_loss: 0.0855 - val_mae: 0.1205\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2081 - mae: 0.3220 - val_loss: 0.0909 - val_mae: 0.1247\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1777 - mae: 0.2911 - val_loss: 0.0862 - val_mae: 0.1432\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1723 - mae: 0.2822 - val_loss: 0.0831 - val_mae: 0.1745\n",
      "29/29 [==============================] - 0s 556us/step\n",
      "4/4 [==============================] - 0s 864us/step\n",
      "[DEBUG] signals shape: (900, 1), returns shape: (900,)\n",
      "[DEBUG] signals shape: (100, 1), returns shape: (100,)\n",
      "\n",
      "Processing non-contiguous fold 5/10\n",
      "Epoch 1/100\n",
      "23/23 [==============================] - 2s 27ms/step - loss: 0.5622 - mae: 0.6514 - val_loss: 0.0741 - val_mae: 0.1338\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4973 - mae: 0.6017 - val_loss: 0.0738 - val_mae: 0.1120\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4368 - mae: 0.5485 - val_loss: 0.0712 - val_mae: 0.0787\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3714 - mae: 0.4908 - val_loss: 0.0782 - val_mae: 0.1007\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3599 - mae: 0.4864 - val_loss: 0.0749 - val_mae: 0.1419\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3350 - mae: 0.4561 - val_loss: 0.0702 - val_mae: 0.0946\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2819 - mae: 0.4079 - val_loss: 0.0729 - val_mae: 0.0851\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2614 - mae: 0.3846 - val_loss: 0.0763 - val_mae: 0.1436\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2417 - mae: 0.3713 - val_loss: 0.0680 - val_mae: 0.0756\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2346 - mae: 0.3597 - val_loss: 0.0772 - val_mae: 0.1543\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2048 - mae: 0.3264 - val_loss: 0.0721 - val_mae: 0.1313\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1882 - mae: 0.3077 - val_loss: 0.0841 - val_mae: 0.0929\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1739 - mae: 0.2850 - val_loss: 0.0765 - val_mae: 0.1085\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1481 - mae: 0.2470 - val_loss: 0.0765 - val_mae: 0.1473\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1477 - mae: 0.2497 - val_loss: 0.0688 - val_mae: 0.1045\n",
      "Epoch 16/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1436 - mae: 0.2347 - val_loss: 0.0707 - val_mae: 0.0910\n",
      "Epoch 17/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1317 - mae: 0.2174 - val_loss: 0.0770 - val_mae: 0.1328\n",
      "Epoch 18/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1237 - mae: 0.1952 - val_loss: 0.0680 - val_mae: 0.0870\n",
      "Epoch 19/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1150 - mae: 0.1874 - val_loss: 0.0692 - val_mae: 0.1066\n",
      "29/29 [==============================] - 0s 544us/step\n",
      "4/4 [==============================] - 0s 893us/step\n",
      "[DEBUG] signals shape: (900, 1), returns shape: (900,)\n",
      "[DEBUG] signals shape: (100, 1), returns shape: (100,)\n",
      "\n",
      "Processing non-contiguous fold 6/10\n",
      "Epoch 1/100\n",
      "23/23 [==============================] - 2s 9ms/step - loss: 0.4926 - mae: 0.5967 - val_loss: 0.0708 - val_mae: 0.0777\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4326 - mae: 0.5392 - val_loss: 0.0735 - val_mae: 0.1066\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3950 - mae: 0.5147 - val_loss: 0.0821 - val_mae: 0.1735\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3359 - mae: 0.4596 - val_loss: 0.0821 - val_mae: 0.1137\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2675 - mae: 0.3876 - val_loss: 0.0968 - val_mae: 0.1345\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2800 - mae: 0.3968 - val_loss: 0.1196 - val_mae: 0.1408\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2481 - mae: 0.3681 - val_loss: 0.1053 - val_mae: 0.1086\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2205 - mae: 0.3330 - val_loss: 0.0974 - val_mae: 0.1282\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1979 - mae: 0.3128 - val_loss: 0.0819 - val_mae: 0.1416\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1970 - mae: 0.3048 - val_loss: 0.0790 - val_mae: 0.1102\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1757 - mae: 0.2713 - val_loss: 0.0801 - val_mae: 0.1038\n",
      "29/29 [==============================] - 0s 566us/step\n",
      "4/4 [==============================] - 0s 879us/step\n",
      "[DEBUG] signals shape: (900, 1), returns shape: (900,)\n",
      "[DEBUG] signals shape: (100, 1), returns shape: (100,)\n",
      "\n",
      "Processing non-contiguous fold 7/10\n",
      "Epoch 1/100\n",
      "23/23 [==============================] - 1s 8ms/step - loss: 0.5927 - mae: 0.6743 - val_loss: 0.0953 - val_mae: 0.1144\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5056 - mae: 0.6051 - val_loss: 0.1013 - val_mae: 0.0931\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4567 - mae: 0.5634 - val_loss: 0.0955 - val_mae: 0.1004\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4046 - mae: 0.5204 - val_loss: 0.1113 - val_mae: 0.1906\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3485 - mae: 0.4744 - val_loss: 0.1122 - val_mae: 0.2227\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3054 - mae: 0.4310 - val_loss: 0.1184 - val_mae: 0.2282\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2935 - mae: 0.4268 - val_loss: 0.1256 - val_mae: 0.2131\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2583 - mae: 0.3863 - val_loss: 0.1284 - val_mae: 0.2558\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2319 - mae: 0.3606 - val_loss: 0.1030 - val_mae: 0.2357\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2292 - mae: 0.3512 - val_loss: 0.0975 - val_mae: 0.1588\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2057 - mae: 0.3252 - val_loss: 0.0805 - val_mae: 0.1274\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1850 - mae: 0.2949 - val_loss: 0.0808 - val_mae: 0.1509\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1810 - mae: 0.2964 - val_loss: 0.0759 - val_mae: 0.1173\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1648 - mae: 0.2680 - val_loss: 0.0715 - val_mae: 0.0741\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1490 - mae: 0.2440 - val_loss: 0.0866 - val_mae: 0.1433\n",
      "Epoch 16/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1375 - mae: 0.2300 - val_loss: 0.0717 - val_mae: 0.0953\n",
      "Epoch 17/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.1329 - mae: 0.2116 - val_loss: 0.0804 - val_mae: 0.0812\n",
      "Epoch 18/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1232 - mae: 0.2036 - val_loss: 0.0742 - val_mae: 0.1078\n",
      "Epoch 19/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1235 - mae: 0.2070 - val_loss: 0.0703 - val_mae: 0.0771\n",
      "Epoch 20/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1133 - mae: 0.1825 - val_loss: 0.0706 - val_mae: 0.0852\n",
      "Epoch 21/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1143 - mae: 0.1845 - val_loss: 0.0733 - val_mae: 0.0953\n",
      "Epoch 22/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0995 - mae: 0.1622 - val_loss: 0.0724 - val_mae: 0.0731\n",
      "Epoch 23/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1029 - mae: 0.1631 - val_loss: 0.0701 - val_mae: 0.0843\n",
      "Epoch 24/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0990 - mae: 0.1540 - val_loss: 0.0745 - val_mae: 0.0864\n",
      "Epoch 25/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0934 - mae: 0.1455 - val_loss: 0.0776 - val_mae: 0.0987\n",
      "Epoch 26/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0924 - mae: 0.1385 - val_loss: 0.0769 - val_mae: 0.1146\n",
      "Epoch 27/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0908 - mae: 0.1335 - val_loss: 0.0706 - val_mae: 0.0889\n",
      "Epoch 28/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0884 - mae: 0.1310 - val_loss: 0.0699 - val_mae: 0.0809\n",
      "Epoch 29/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0867 - mae: 0.1252 - val_loss: 0.0693 - val_mae: 0.0715\n",
      "Epoch 30/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0875 - mae: 0.1311 - val_loss: 0.0723 - val_mae: 0.0821\n",
      "Epoch 31/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0853 - mae: 0.1273 - val_loss: 0.0693 - val_mae: 0.0779\n",
      "Epoch 32/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0818 - mae: 0.1165 - val_loss: 0.0696 - val_mae: 0.0879\n",
      "Epoch 33/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0783 - mae: 0.1127 - val_loss: 0.0714 - val_mae: 0.0776\n",
      "Epoch 34/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0801 - mae: 0.1108 - val_loss: 0.0700 - val_mae: 0.0844\n",
      "Epoch 35/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0806 - mae: 0.1122 - val_loss: 0.0696 - val_mae: 0.0730\n",
      "Epoch 36/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0780 - mae: 0.1092 - val_loss: 0.0695 - val_mae: 0.0743\n",
      "Epoch 37/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0741 - mae: 0.0998 - val_loss: 0.0693 - val_mae: 0.0764\n",
      "Epoch 38/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0751 - mae: 0.1020 - val_loss: 0.0692 - val_mae: 0.0720\n",
      "Epoch 39/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0746 - mae: 0.0962 - val_loss: 0.0691 - val_mae: 0.0742\n",
      "Epoch 40/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0730 - mae: 0.0963 - val_loss: 0.0692 - val_mae: 0.0703\n",
      "Epoch 41/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0737 - mae: 0.0916 - val_loss: 0.0691 - val_mae: 0.0716\n",
      "Epoch 42/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0717 - mae: 0.0886 - val_loss: 0.0690 - val_mae: 0.0714\n",
      "Epoch 43/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0715 - mae: 0.0877 - val_loss: 0.0693 - val_mae: 0.0821\n",
      "Epoch 44/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0717 - mae: 0.0885 - val_loss: 0.0691 - val_mae: 0.0705\n",
      "Epoch 45/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0717 - mae: 0.0854 - val_loss: 0.0691 - val_mae: 0.0753\n",
      "Epoch 46/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0703 - mae: 0.0836 - val_loss: 0.0691 - val_mae: 0.0743\n",
      "Epoch 47/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0702 - mae: 0.0828 - val_loss: 0.0691 - val_mae: 0.0705\n",
      "Epoch 48/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0699 - mae: 0.0801 - val_loss: 0.0690 - val_mae: 0.0722\n",
      "Epoch 49/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0696 - mae: 0.0789 - val_loss: 0.0690 - val_mae: 0.0726\n",
      "Epoch 50/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0698 - mae: 0.0791 - val_loss: 0.0690 - val_mae: 0.0726\n",
      "Epoch 51/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0695 - mae: 0.0791 - val_loss: 0.0690 - val_mae: 0.0729\n",
      "Epoch 52/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0694 - mae: 0.0779 - val_loss: 0.0690 - val_mae: 0.0698\n",
      "Epoch 53/100\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0695 - mae: 0.0768 - val_loss: 0.0690 - val_mae: 0.0710\n",
      "Epoch 54/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0692 - mae: 0.0754 - val_loss: 0.0690 - val_mae: 0.0737\n",
      "Epoch 55/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0692 - mae: 0.0748 - val_loss: 0.0690 - val_mae: 0.0703\n",
      "Epoch 56/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0691 - mae: 0.0736 - val_loss: 0.0690 - val_mae: 0.0696\n",
      "Epoch 57/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0691 - mae: 0.0729 - val_loss: 0.0690 - val_mae: 0.0693\n",
      "Epoch 58/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0691 - mae: 0.0732 - val_loss: 0.0690 - val_mae: 0.0710\n",
      "Epoch 59/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0690 - mae: 0.0726 - val_loss: 0.0690 - val_mae: 0.0694\n",
      "Epoch 60/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0717 - val_loss: 0.0690 - val_mae: 0.0694\n",
      "Epoch 61/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0690 - mae: 0.0715 - val_loss: 0.0690 - val_mae: 0.0701\n",
      "Epoch 62/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0690 - mae: 0.0715 - val_loss: 0.0690 - val_mae: 0.0702\n",
      "Epoch 63/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0690 - mae: 0.0710 - val_loss: 0.0690 - val_mae: 0.0695\n",
      "Epoch 64/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0709 - val_loss: 0.0690 - val_mae: 0.0693\n",
      "Epoch 65/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0690 - mae: 0.0706 - val_loss: 0.0690 - val_mae: 0.0695\n",
      "Epoch 66/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0704 - val_loss: 0.0690 - val_mae: 0.0697\n",
      "Epoch 67/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0703 - val_loss: 0.0690 - val_mae: 0.0693\n",
      "Epoch 68/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0702 - val_loss: 0.0690 - val_mae: 0.0695\n",
      "Epoch 69/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0700 - val_loss: 0.0690 - val_mae: 0.0697\n",
      "Epoch 70/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0698 - val_loss: 0.0690 - val_mae: 0.0694\n",
      "Epoch 71/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0697 - val_loss: 0.0690 - val_mae: 0.0692\n",
      "Epoch 72/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0690 - mae: 0.0696 - val_loss: 0.0690 - val_mae: 0.0694\n",
      "Epoch 73/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0696 - val_loss: 0.0690 - val_mae: 0.0693\n",
      "Epoch 74/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0695 - val_loss: 0.0690 - val_mae: 0.0693\n",
      "Epoch 75/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0695 - val_loss: 0.0690 - val_mae: 0.0692\n",
      "Epoch 76/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0695 - val_loss: 0.0690 - val_mae: 0.0693\n",
      "Epoch 77/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0694 - val_loss: 0.0690 - val_mae: 0.0694\n",
      "Epoch 78/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0694 - val_loss: 0.0690 - val_mae: 0.0695\n",
      "Epoch 79/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0693 - val_loss: 0.0690 - val_mae: 0.0693\n",
      "Epoch 80/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0693 - val_loss: 0.0690 - val_mae: 0.0692\n",
      "Epoch 81/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0693 - val_loss: 0.0690 - val_mae: 0.0692\n",
      "Epoch 82/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0693 - val_loss: 0.0690 - val_mae: 0.0692\n",
      "Epoch 83/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0692 - val_loss: 0.0690 - val_mae: 0.0692\n",
      "Epoch 84/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0692 - val_loss: 0.0690 - val_mae: 0.0692\n",
      "Epoch 85/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0692 - val_loss: 0.0690 - val_mae: 0.0692\n",
      "Epoch 86/100\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0690 - mae: 0.0692 - val_loss: 0.0690 - val_mae: 0.0692\n",
      "Epoch 87/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0692 - val_loss: 0.0690 - val_mae: 0.0692\n",
      "Epoch 88/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0692 - val_loss: 0.0690 - val_mae: 0.0692\n",
      "Epoch 89/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0692 - val_loss: 0.0690 - val_mae: 0.0692\n",
      "Epoch 90/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0692 - val_loss: 0.0690 - val_mae: 0.0692\n",
      "Epoch 91/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0692 - val_loss: 0.0690 - val_mae: 0.0692\n",
      "Epoch 92/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0692 - val_loss: 0.0690 - val_mae: 0.0692\n",
      "Epoch 93/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0692 - val_loss: 0.0690 - val_mae: 0.0692\n",
      "Epoch 94/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0692 - val_loss: 0.0690 - val_mae: 0.0692\n",
      "Epoch 95/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0692 - val_loss: 0.0690 - val_mae: 0.0692\n",
      "Epoch 96/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0692 - val_loss: 0.0690 - val_mae: 0.0692\n",
      "Epoch 97/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0690 - mae: 0.0692 - val_loss: 0.0690 - val_mae: 0.0692\n",
      "Epoch 98/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0692 - val_loss: 0.0690 - val_mae: 0.0692\n",
      "29/29 [==============================] - 0s 556us/step\n",
      "4/4 [==============================] - 0s 951us/step\n",
      "[DEBUG] signals shape: (900, 1), returns shape: (900,)\n",
      "[DEBUG] signals shape: (100, 1), returns shape: (100,)\n",
      "\n",
      "Processing non-contiguous fold 8/10\n",
      "Epoch 1/100\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 0.5442 - mae: 0.6285 - val_loss: 0.0841 - val_mae: 0.1138\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4828 - mae: 0.5872 - val_loss: 0.0947 - val_mae: 0.1328\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4413 - mae: 0.5516 - val_loss: 0.1017 - val_mae: 0.1934\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3791 - mae: 0.4995 - val_loss: 0.1074 - val_mae: 0.2117\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3523 - mae: 0.4712 - val_loss: 0.0975 - val_mae: 0.1948\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3084 - mae: 0.4250 - val_loss: 0.0928 - val_mae: 0.1848\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3071 - mae: 0.4313 - val_loss: 0.0916 - val_mae: 0.1711\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2597 - mae: 0.3800 - val_loss: 0.1116 - val_mae: 0.2096\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2417 - mae: 0.3674 - val_loss: 0.0917 - val_mae: 0.1466\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2265 - mae: 0.3505 - val_loss: 0.1025 - val_mae: 0.1274\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2084 - mae: 0.3237 - val_loss: 0.0937 - val_mae: 0.1439\n",
      "29/29 [==============================] - 0s 560us/step\n",
      "4/4 [==============================] - 0s 839us/step\n",
      "[DEBUG] signals shape: (900, 1), returns shape: (900,)\n",
      "[DEBUG] signals shape: (100, 1), returns shape: (100,)\n",
      "\n",
      "Processing non-contiguous fold 9/10\n",
      "Epoch 1/100\n",
      "23/23 [==============================] - 1s 8ms/step - loss: 0.5326 - mae: 0.6284 - val_loss: 0.0831 - val_mae: 0.0834\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4462 - mae: 0.5585 - val_loss: 0.0807 - val_mae: 0.0806\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3635 - mae: 0.4870 - val_loss: 0.0764 - val_mae: 0.0803\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3626 - mae: 0.4812 - val_loss: 0.0885 - val_mae: 0.1807\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3161 - mae: 0.4398 - val_loss: 0.0772 - val_mae: 0.1481\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3002 - mae: 0.4234 - val_loss: 0.0743 - val_mae: 0.1143\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2780 - mae: 0.4069 - val_loss: 0.0842 - val_mae: 0.1813\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2533 - mae: 0.3791 - val_loss: 0.1032 - val_mae: 0.2150\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2334 - mae: 0.3488 - val_loss: 0.0800 - val_mae: 0.1376\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2124 - mae: 0.3321 - val_loss: 0.0922 - val_mae: 0.1791\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1960 - mae: 0.3087 - val_loss: 0.0783 - val_mae: 0.1315\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1792 - mae: 0.2928 - val_loss: 0.0770 - val_mae: 0.1076\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1735 - mae: 0.2823 - val_loss: 0.0757 - val_mae: 0.1222\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1647 - mae: 0.2737 - val_loss: 0.0754 - val_mae: 0.1110\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1419 - mae: 0.2306 - val_loss: 0.0756 - val_mae: 0.1178\n",
      "Epoch 16/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1437 - mae: 0.2388 - val_loss: 0.0756 - val_mae: 0.1163\n",
      "29/29 [==============================] - 0s 551us/step\n",
      "4/4 [==============================] - 0s 799us/step\n",
      "[DEBUG] signals shape: (900, 1), returns shape: (900,)\n",
      "[DEBUG] signals shape: (100, 1), returns shape: (100,)\n",
      "\n",
      "Processing non-contiguous fold 10/10\n",
      "Epoch 1/100\n",
      "23/23 [==============================] - 2s 8ms/step - loss: 0.5441 - mae: 0.6383 - val_loss: 0.0798 - val_mae: 0.0926\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4365 - mae: 0.5498 - val_loss: 0.0752 - val_mae: 0.0843\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4115 - mae: 0.5241 - val_loss: 0.0779 - val_mae: 0.0778\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3661 - mae: 0.4847 - val_loss: 0.0771 - val_mae: 0.0849\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3221 - mae: 0.4383 - val_loss: 0.0912 - val_mae: 0.1529\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3205 - mae: 0.4364 - val_loss: 0.0815 - val_mae: 0.1254\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2872 - mae: 0.4001 - val_loss: 0.0765 - val_mae: 0.0824\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2728 - mae: 0.3926 - val_loss: 0.1230 - val_mae: 0.1725\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2392 - mae: 0.3505 - val_loss: 0.0955 - val_mae: 0.0901\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.2221 - mae: 0.3283 - val_loss: 0.0826 - val_mae: 0.1036\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1946 - mae: 0.3032 - val_loss: 0.0773 - val_mae: 0.1216\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1926 - mae: 0.3073 - val_loss: 0.0963 - val_mae: 0.0904\n",
      "29/29 [==============================] - 0s 585us/step\n",
      "4/4 [==============================] - 0s 915us/step\n",
      "[DEBUG] signals shape: (900, 1), returns shape: (900,)\n",
      "[DEBUG] signals shape: (100, 1), returns shape: (100,)\n",
      "\n",
      "Results have been saved to:\n",
      "- Individual fold results: data/raw/fold_*_results.csv\n",
      "- Summary statistics: data/raw/cross_validation_summary.csv\n",
      "- Visualization: data/raw/cross_validation_analysis.png\n",
      "- Cost sensitivity: data/raw/cost_sensitivity_summary.csv\n",
      "- Cost sensitivity plot: data/raw/cost_sensitivity_analysis.png\n",
      "- Non-contiguous fold results: data/raw/non_contiguous_fold_*_results.csv\n",
      "- Non-contiguous summary: data/raw/non_contiguous_summary.csv\n",
      "- Non-contiguous analysis plot: data/raw/non_contiguous_analysis.png\n",
      "\n",
      "Phase 6 completed successfully!\n"
     ]
    }
   ],
   "source": [
    "run_trading_signals() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![plot](./figures/cross_validation_analysis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 8.2 Transaction Cost Analysis Module\n",
    "\n",
    "The `VIXTransactionCosts` module isolates transaction-cost modeling, cost-adjusted returns, and sensitivity analysis. Note that some functionality overlaps with cost-penalty logic in the trading-signals module (e.g., adjusting returns for position changes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIXTransactionCosts:\n",
    "    def __init__(self, base_cost=0.0020):  # 20 bps base cost\n",
    "        \"\"\"\n",
    "        Initialize the transaction costs module.\n",
    "        \n",
    "        Args:\n",
    "            base_cost (float): Base transaction cost in decimal (e.g., 0.0020 for 20 bps)\n",
    "        \"\"\"\n",
    "        self.base_cost = base_cost\n",
    "        self.signals = VIXTradingSignals()\n",
    "        self.engine = VIXSimulationEngine()\n",
    "        \n",
    "    def compute_transaction_costs(self, signals, cost_bps):\n",
    "        \"\"\"\n",
    "        Compute transaction costs for a sequence of trading signals.\n",
    "        \n",
    "        Args:\n",
    "            signals: Array of trading signals\n",
    "            cost_bps: Transaction cost in basis points\n",
    "            \n",
    "        Returns:\n",
    "            Array of transaction costs\n",
    "        \"\"\"\n",
    "        costs = np.zeros(len(signals))\n",
    "        cost_decimal = cost_bps / 10000  # Convert bps to decimal\n",
    "        \n",
    "        # Compute costs for each trade\n",
    "        for i in range(1, len(signals)):\n",
    "            if signals[i] != signals[i-1]:  # Position change\n",
    "                costs[i] = cost_decimal\n",
    "        \n",
    "        return costs\n",
    "    \n",
    "    def compute_cost_adjusted_returns(self, returns, signals, cost_bps):\n",
    "        \"\"\"\n",
    "        Compute returns adjusted for transaction costs.\n",
    "        \n",
    "        Args:\n",
    "            returns: Dictionary of strategy returns\n",
    "            signals: Array of trading signals\n",
    "            cost_bps: Transaction cost in basis points\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with cost-adjusted returns and metrics\n",
    "        \"\"\"\n",
    "        # Get raw returns\n",
    "        raw_returns = np.zeros(len(signals))\n",
    "        for i, signal in enumerate(signals):\n",
    "            action_name = list(self.engine.actions.keys())[int(signal)]\n",
    "            raw_returns[i] = returns[action_name][i]\n",
    "        \n",
    "        # Compute transaction costs\n",
    "        costs = self.compute_transaction_costs(signals, cost_bps)\n",
    "        \n",
    "        # Compute cost-adjusted returns\n",
    "        adjusted_returns = raw_returns - costs\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics = {\n",
    "            'raw_returns': raw_returns,\n",
    "            'costs': costs,\n",
    "            'adjusted_returns': adjusted_returns,\n",
    "            'total_cost': np.sum(costs),\n",
    "            'cost_ratio': np.sum(costs) / np.sum(np.abs(raw_returns)) if np.sum(np.abs(raw_returns)) > 0 else 0,\n",
    "            'turnover': np.sum(np.diff(signals) != 0) / len(signals)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def run_cost_sensitivity_analysis(self, n_steps=1000, cost_levels=None):\n",
    "        \"\"\"\n",
    "        Run sensitivity analysis for different cost levels.\n",
    "        \n",
    "        Args:\n",
    "            n_steps (int): Number of steps to simulate\n",
    "            cost_levels (list): List of cost levels in bps to analyze\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with sensitivity analysis results\n",
    "        \"\"\"\n",
    "        if cost_levels is None:\n",
    "            cost_levels = [20, 25, 30, 35, 40]  # Default cost levels in bps\n",
    "        \n",
    "        # Generate simulation data\n",
    "        simulation_results = self.engine.simulate_trading_path(n_steps=n_steps)\n",
    "        state_vectors = simulation_results['state_vectors'].values\n",
    "        returns = simulation_results['returns']\n",
    "        \n",
    "        # Initialize results storage\n",
    "        sensitivity_results = {\n",
    "            'cost_levels': cost_levels,\n",
    "            'metrics': []\n",
    "        }\n",
    "        \n",
    "        # Train neural network\n",
    "        print(\"\\nTraining neural network...\")\n",
    "        X_train_scaled, _, y_train, _ = self.signals.network.prepare_data(\n",
    "            state_vectors,\n",
    "            np.array(list(returns.values())).T\n",
    "        )\n",
    "        self.signals.network.train(X_train_scaled, y_train, X_train_scaled, y_train)\n",
    "        \n",
    "        # Generate trading signals\n",
    "        print(\"\\nGenerating trading signals...\")\n",
    "        signals = self.signals.generate_signals(state_vectors)\n",
    "        \n",
    "        # Run analysis for each cost level\n",
    "        for cost_bps in cost_levels:\n",
    "            print(f\"\\nAnalyzing cost level: {cost_bps} bps\")\n",
    "            \n",
    "            # Compute cost-adjusted returns\n",
    "            cost_metrics = self.compute_cost_adjusted_returns(\n",
    "                returns, signals.flatten(), cost_bps\n",
    "            )\n",
    "            \n",
    "            # Compute performance metrics\n",
    "            metrics = {\n",
    "                'cost_bps': cost_bps,\n",
    "                'total_return': np.sum(cost_metrics['adjusted_returns']),\n",
    "                'annual_return': np.mean(cost_metrics['adjusted_returns']) * 252,\n",
    "                'annual_volatility': np.std(cost_metrics['adjusted_returns']) * np.sqrt(252),\n",
    "                'sharpe_ratio': np.mean(cost_metrics['adjusted_returns']) / np.std(cost_metrics['adjusted_returns']) * np.sqrt(252) if np.std(cost_metrics['adjusted_returns']) > 0 else 0,\n",
    "                'total_cost': cost_metrics['total_cost'],\n",
    "                'cost_ratio': cost_metrics['cost_ratio'],\n",
    "                'turnover': cost_metrics['turnover']\n",
    "            }\n",
    "            \n",
    "            sensitivity_results['metrics'].append(metrics)\n",
    "            \n",
    "            # Save results for this cost level\n",
    "            self._save_cost_level_results(cost_bps, metrics)\n",
    "        \n",
    "        # Generate summary report\n",
    "        self._generate_sensitivity_summary(sensitivity_results)\n",
    "        \n",
    "        return sensitivity_results\n",
    "    \n",
    "    def _save_cost_level_results(self, cost_bps, metrics):\n",
    "        \"\"\"Save results for a specific cost level to a CSV file.\"\"\"\n",
    "        metrics_df = pd.DataFrame([metrics])\n",
    "        metrics_df.to_csv(f'data/tran_cost/cost_{cost_bps}bps_results.csv', index=False)\n",
    "    \n",
    "    def _generate_sensitivity_summary(self, sensitivity_results):\n",
    "        \"\"\"Generate summary statistics for sensitivity analysis.\"\"\"\n",
    "        # Convert results to DataFrame\n",
    "        summary_df = pd.DataFrame(sensitivity_results['metrics'])\n",
    "        \n",
    "        # Save summary\n",
    "        summary_df.to_csv('data/tran_cost/cost_sensitivity_summary.csv', index=False)\n",
    "        \n",
    "        # Create visualization\n",
    "        self.plot_sensitivity_analysis(sensitivity_results)\n",
    "    \n",
    "    def plot_sensitivity_analysis(self, sensitivity_results):\n",
    "        \"\"\"\n",
    "        Create comprehensive visualization of sensitivity analysis results.\n",
    "        \n",
    "        Args:\n",
    "            sensitivity_results: Dictionary with sensitivity analysis results\n",
    "        \"\"\"\n",
    "        # Convert results to DataFrame\n",
    "        df = pd.DataFrame(sensitivity_results['metrics'])\n",
    "        \n",
    "        # Create figure\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Transaction Cost Sensitivity Analysis', fontsize=16)\n",
    "        \n",
    "        # Plot 1: Annual Return vs Cost Level\n",
    "        ax = axes[0, 0]\n",
    "        ax.plot(df['cost_bps'], df['annual_return'], 'b-o')\n",
    "        ax.set_xlabel('Transaction Cost (bps)')\n",
    "        ax.set_ylabel('Annual Return')\n",
    "        ax.set_title('Annual Return vs Transaction Cost')\n",
    "        ax.grid(True)\n",
    "        \n",
    "        # Plot 2: Sharpe Ratio vs Cost Level\n",
    "        ax = axes[0, 1]\n",
    "        ax.plot(df['cost_bps'], df['sharpe_ratio'], 'g-o')\n",
    "        ax.set_xlabel('Transaction Cost (bps)')\n",
    "        ax.set_ylabel('Sharpe Ratio')\n",
    "        ax.set_title('Sharpe Ratio vs Transaction Cost')\n",
    "        ax.grid(True)\n",
    "        \n",
    "        # Plot 3: Cost Ratio vs Cost Level\n",
    "        ax = axes[1, 0]\n",
    "        ax.plot(df['cost_bps'], df['cost_ratio'], 'r-o')\n",
    "        ax.set_xlabel('Transaction Cost (bps)')\n",
    "        ax.set_ylabel('Cost Ratio')\n",
    "        ax.set_title('Cost Ratio vs Transaction Cost')\n",
    "        ax.grid(True)\n",
    "        \n",
    "        # Plot 4: Turnover vs Cost Level\n",
    "        ax = axes[1, 1]\n",
    "        ax.plot(df['cost_bps'], df['turnover'], 'm-o')\n",
    "        ax.set_xlabel('Transaction Cost (bps)')\n",
    "        ax.set_ylabel('Turnover')\n",
    "        ax.set_title('Turnover vs Transaction Cost')\n",
    "        ax.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figures/cost_sensitivity_analysis.png')\n",
    "        plt.close()\n",
    "\n",
    "def run_transaction_costs():\n",
    "    \"\"\"Main function to run the transaction costs analysis.\"\"\"\n",
    "    print(\"Starting Phase 7: Transaction Cost Analysis...\")\n",
    "    \n",
    "    # Initialize transaction costs module\n",
    "    costs = VIXTransactionCosts()\n",
    "    \n",
    "    # Run sensitivity analysis\n",
    "    print(\"\\nRunning transaction cost sensitivity analysis...\")\n",
    "    sensitivity_results = costs.run_cost_sensitivity_analysis(\n",
    "        n_steps=1000,\n",
    "        cost_levels=[20, 25, 30, 35, 40]  # Cost levels in bps\n",
    "    )\n",
    "    \n",
    "    print(\"\\nResults have been saved to:\")\n",
    "    print(\"- Individual cost level results: data/tran_cost/cost_*bps_results.csv\")\n",
    "    print(\"- Summary statistics: data/tran_cost/cost_sensitivity_summary.csv\")\n",
    "    print(\"- Visualization: figures/cost_sensitivity_analysis.png\")\n",
    "    \n",
    "    print(\"\\nPhase 7 completed successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Phase 7: Transaction Cost Analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gd/qxxh82n95f57fhdhrdg746g80000gn/T/ipykernel_14771/207703377.py:28: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  self.state_vectors['date'] = pd.to_datetime(self.state_vectors['date'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded state vectors with shape: (3190, 12)\n",
      "Date range: 2008-04-01 00:00:00-05:00 to 2020-11-27 00:00:00-06:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gd/qxxh82n95f57fhdhrdg746g80000gn/T/ipykernel_14771/207703377.py:105: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  model_data = model_data.fillna(method='ffill').fillna(method='bfill')\n",
      "/var/folders/gd/qxxh82n95f57fhdhrdg746g80000gn/T/ipykernel_14771/207703377.py:28: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  self.state_vectors['date'] = pd.to_datetime(self.state_vectors['date'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully fit VAR model with 10 lags\n",
      "Loaded state vectors with shape: (3190, 12)\n",
      "Date range: 2008-04-01 00:00:00-05:00 to 2020-11-27 00:00:00-06:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gd/qxxh82n95f57fhdhrdg746g80000gn/T/ipykernel_14771/207703377.py:105: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  model_data = model_data.fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully fit VAR model with 10 lags\n",
      "\n",
      "Running transaction cost sensitivity analysis...\n",
      "\n",
      "Training neural network...\n",
      "Epoch 1/100\n",
      "25/25 [==============================] - 2s 9ms/step - loss: 0.5715 - mae: 0.6644 - val_loss: 0.0798 - val_mae: 0.0750\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4804 - mae: 0.5921 - val_loss: 0.0751 - val_mae: 0.0883\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4103 - mae: 0.5334 - val_loss: 0.0781 - val_mae: 0.1329\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3542 - mae: 0.4815 - val_loss: 0.0761 - val_mae: 0.1137\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3311 - mae: 0.4557 - val_loss: 0.0739 - val_mae: 0.0952\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.3178 - mae: 0.4467 - val_loss: 0.0714 - val_mae: 0.1048\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2964 - mae: 0.4264 - val_loss: 0.0772 - val_mae: 0.0821\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2749 - mae: 0.4076 - val_loss: 0.0958 - val_mae: 0.2146\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2399 - mae: 0.3761 - val_loss: 0.0929 - val_mae: 0.1969\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2079 - mae: 0.3375 - val_loss: 0.0969 - val_mae: 0.2260\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1990 - mae: 0.3322 - val_loss: 0.0764 - val_mae: 0.1318\n",
      "Epoch 12/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1797 - mae: 0.3016 - val_loss: 0.0912 - val_mae: 0.2130\n",
      "Epoch 13/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.1683 - mae: 0.2849 - val_loss: 0.0922 - val_mae: 0.1836\n",
      "Epoch 14/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.1558 - mae: 0.2711 - val_loss: 0.0952 - val_mae: 0.2072\n",
      "Epoch 15/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1390 - mae: 0.2521 - val_loss: 0.0749 - val_mae: 0.1460\n",
      "Epoch 16/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.1260 - mae: 0.2261 - val_loss: 0.0747 - val_mae: 0.1283\n",
      "\n",
      "Generating trading signals...\n",
      "32/32 [==============================] - 0s 632us/step\n",
      "\n",
      "Analyzing cost level: 20 bps\n",
      "\n",
      "Analyzing cost level: 25 bps\n",
      "\n",
      "Analyzing cost level: 30 bps\n",
      "\n",
      "Analyzing cost level: 35 bps\n",
      "\n",
      "Analyzing cost level: 40 bps\n",
      "\n",
      "Results have been saved to:\n",
      "- Individual cost level results: data/tran_cost/cost_*bps_results.csv\n",
      "- Summary statistics: data/tran_cost/cost_sensitivity_summary.csv\n",
      "- Visualization: figures/cost_sensitivity_analysis.png\n",
      "\n",
      "Phase 7 completed successfully!\n"
     ]
    }
   ],
   "source": [
    "run_transaction_costs() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Cost Level (bps) | Annual Return | Annual Volatility | Sharpe Ratio | Total Cost | Cost Ratio | Turnover |\n",
    "|-----------------|---------------|-------------------|--------------|------------|------------|----------|\n",
    "| 20 | 47.67% | 465.72% | 0.1024 | 0.004 | 0.0046% | 0.2% |\n",
    "| 25 | 47.64% | 465.72% | 0.1023 | 0.005 | 0.0058% | 0.2% |\n",
    "| 30 | 47.62% | 465.71% | 0.1022 | 0.006 | 0.0069% | 0.2% |\n",
    "| 35 | 47.59% | 465.71% | 0.1022 | 0.007 | 0.0081% | 0.2% |\n",
    "| 40 | 47.57% | 465.70% | 0.1021 | 0.008 | 0.0093% | 0.2% |\n",
    "\n",
    "\n",
    "![plot](./figures/cost_sensitivity_analysis.png)\n",
    "\n",
    "\n",
    "*Scripts and results (fold-level CSV, summary) are saved to* `data/tran_cost/`, and plots saved to* `figures/*`*for further analysis.*\n",
    "\n",
    "## 9. Comparison to Original Results\n",
    "\n",
    "We compare our replication metrics against those reported by Avellaneda et al. (2020) to assess fidelity and identify key divergences.\n",
    "\n",
    "| **Metric**              | **Original Paper**                                      | **Replication**                                        |\n",
    "| ----------------------- | ------------------------------------------------------- | ------------------------------------------------------ |\n",
    "| Annualized Sharpe Ratio | Oftentimes above **3.0** across folds                   | Approximately **–0.04** (Figure 4)                     |\n",
    "| Annualized Return       | Positive and statistically significant (\\~50–100% p.a.) | Approximately **–20.8%** (Figure 5)                    |\n",
    "| Maximum Drawdown        | Moderate drawdowns (<30%)                               | Severe: **–137.7%** on non-contiguous folds (Figure 6) |\n",
    "\n",
    "**Key Differences & Implications:**\n",
    "\n",
    "1. **Data Proxy vs. True Futures:** We used the VIX index as a stand-in, whereas the original uses full CBOE futures term-structure. This simplification likely obscures real arbitrage signals.\n",
    "2. **Constant‐Maturity Construction:** Our linear and stochastic interpolation of futures may not capture subtle curve dynamics present in actual market data.\n",
    "3. **Transaction-Cost Modeling:** Even at 20 bps, costs reverse the positive original returns into large losses (Figure 5), highlighting sensitivity to realistic spread modeling.\n",
    "4. **Model Calibration & Complexity:** Hyperparameter choices (NN architecture, VAR lags, utility functions) differ from the original’s finely tuned setup, contributing to performance gaps.\n",
    "\n",
    "**Conclusion:** Although we faithfully reimplemented the methodology, the quantitative gap underscores the critical importance of high-fidelity data and careful calibration. While the structural robustness (consistent in- vs. out-of-sample behavior) aligns with the original findings, the actual economic profitability vanishes under our simplified assumptions and proxy data usage.\n",
    "\n",
    "\n",
    "## 10. Extensions: More Recent Data & Additional Asset Classes\n",
    "\n",
    "For now, I just used data range similar to what the original auther used. But I think this method would be also helpful.\n",
    "\n",
    "## 11. Summary Statistics\n",
    "\n",
    "\n",
    "### 11.1 Transaction Cost Sensitivity Summary\n",
    "\n",
    "\n",
    "| Transaction Cost (bps) | Total Return | Sharpe Ratio | Max Drawdown | Avg Return | Std Return | Hit Ratio |\n",
    "|----------------------|--------------|--------------|--------------|------------|------------|-----------|\n",
    "| 20 | -1.422 | -0.659 | -1.422 | -0.00142 | 0.0343 | 0.0 |\n",
    "| 25 | -1.423 | -0.659 | -1.423 | -0.00142 | 0.0343 | 0.0 |\n",
    "| 30 | -1.424 | -0.659 | -1.424 | -0.00142 | 0.0343 | 0.0 |\n",
    "| 35 | -1.425 | -0.660 | -1.425 | -0.00143 | 0.0343 | 0.0 |\n",
    "| 40 | -1.426 | -0.660 | -1.426 | -0.00143 | 0.0343 | 0.0 |\n",
    "\n",
    "\n",
    "## 11.1 Non-Contiguous Fold Summary\n",
    "\n",
    "| Metric | In-Sample | Out-of-Sample |\n",
    "|--------|------------|---------------|\n",
    "| Total Return | -5.55% (σ=50.14%) | 10.62% (σ=40.76%) |\n",
    "| Sharpe Ratio | 0.03 (σ=0.44) | 0.17 (σ=2.50) |\n",
    "| Max Drawdown | -50.73% (σ=45.70%) | 0% (σ=0%) |\n",
    "| Avg Return | -0.006% (σ=0.056%) | 0.11% (σ=0.41%) |\n",
    "| Std Return | 2.38% (σ=1.55%) | 1.21% (σ=3.23%) |\n",
    "| Hit Ratio | 0.11% (σ=0.07%) | 0.20% (σ=0.63%) |\n",
    "\n",
    "\n",
    "\n",
    "## 12. Replication of Extended Techniques\n",
    "\n",
    "In **Phase 8**, we perform a suite of robustness checks to extend the original methodology, including non-contiguous fold testing, alternative neural-activation benchmarking, and strategy comparisons against constant benchmarks and market ETFs. Some of these analyses (e.g., cost-sensitivity) overlap with transaction-cost logic already in Section 8.2 and activation variants in Section 6.3; you may choose to consolidate shared helper functions to avoid redundancy.\n",
    "\n",
    "### 12.1 Robustness Checks Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robustness Checks Module for VIX Futures Trading (Phase 8)\n",
    "# Implements:\n",
    "# - Non-contiguous fold testing\n",
    "# - Alternative activation functions testing\n",
    "# - Benchmark comparisons with constant strategies and SPY ETF\n",
    "\n",
    "\n",
    "class VIXRobustnessChecks:\n",
    "    def __init__(self, signals, engine):\n",
    "        \"\"\"Initialize robustness checks.\n",
    "        \n",
    "        Args:\n",
    "            signals (VIXTradingSignals): Trading signals object\n",
    "            engine (SimulationEngine): Simulation engine object\n",
    "        \"\"\"\n",
    "        self.signals = signals\n",
    "        self.engine = engine\n",
    "        \n",
    "        # Load and prepare data\n",
    "        simulation_results = self.engine.simulate_trading_path(n_steps=1000)\n",
    "        state_vectors = simulation_results['state_vectors'].values\n",
    "        returns = simulation_results['returns']\n",
    "        \n",
    "        # Split data for training and testing\n",
    "        n_samples = len(state_vectors)\n",
    "        train_size = int(0.8 * n_samples)\n",
    "        self.X_train = state_vectors[:train_size]\n",
    "        self.X_test = state_vectors[train_size:]\n",
    "        self.y_train = np.array(list(returns.values())).T[:train_size]\n",
    "        self.y_test = np.array(list(returns.values())).T[train_size:]\n",
    "        \n",
    "    def run_non_contiguous_folds(self, n_folds=10, fold_size=0.1):\n",
    "        \"\"\"\n",
    "        Run cross-validation with non-contiguous folds.\n",
    "        \n",
    "        Args:\n",
    "            n_folds (int): Number of folds\n",
    "            fold_size (float): Size of each fold as a fraction of total data\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with non-contiguous fold results\n",
    "        \"\"\"\n",
    "        # Initialize results storage\n",
    "        results = {\n",
    "            'fold_results': [],\n",
    "            'summary_metrics': []\n",
    "        }\n",
    "        \n",
    "        # Create non-contiguous folds\n",
    "        n_samples = len(self.X_train)\n",
    "        fold_length = int(n_samples * fold_size)\n",
    "        \n",
    "        for fold in range(n_folds):\n",
    "            print(f\"\\nRunning non-contiguous fold {fold + 1}/{n_folds}\")\n",
    "            \n",
    "            # Create non-contiguous test indices\n",
    "            test_indices = np.array([], dtype=int)\n",
    "            for i in range(fold_length):\n",
    "                test_indices = np.append(test_indices, (fold + i * n_folds) % n_samples)\n",
    "            \n",
    "            train_indices = np.setdiff1d(np.arange(n_samples), test_indices)\n",
    "            \n",
    "            # Split data\n",
    "            X_train, X_test = self.X_train[train_indices], self.X_train[test_indices]\n",
    "            y_train, y_test = self.y_train[train_indices], self.y_train[test_indices]\n",
    "            \n",
    "            # Train neural network\n",
    "            network = VIXTradingNetwork()\n",
    "            X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = network.prepare_data(\n",
    "                X_train, y_train, utility_type='linear'\n",
    "            )\n",
    "            network.train(X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "            \n",
    "            # Update signals network with trained network\n",
    "            self.signals.network = network\n",
    "            \n",
    "            # Generate signals and compute metrics\n",
    "            signals = network.predict(X_test_scaled)\n",
    "            # Ensure signals and returns have compatible shapes\n",
    "            if signals.ndim == 2:\n",
    "                signals = signals[:, 0]\n",
    "            # Ensure returns matches signals length\n",
    "            returns = y_test_scaled[:, 0] if y_test_scaled.ndim > 1 else y_test_scaled\n",
    "            print(f\"[DEBUG] run_non_contiguous_folds: signals shape: {signals.shape}, returns shape: {returns.shape}\")\n",
    "            metrics = self.signals.compute_performance_metrics(signals, returns)\n",
    "            \n",
    "            # Store results\n",
    "            results['fold_results'].append({\n",
    "                'fold': fold + 1,\n",
    "                'test_indices': test_indices,\n",
    "                'metrics': metrics\n",
    "            })\n",
    "            \n",
    "            # Save individual fold results\n",
    "            self._save_fold_results(fold + 1, metrics)\n",
    "        \n",
    "        # Generate summary report\n",
    "        self._generate_non_contiguous_summary(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def test_alternative_activations(self):\n",
    "        \"\"\"Test different activation functions and compare their performance.\"\"\"\n",
    "        activations = ['relu', 'tanh', 'sigmoid']\n",
    "        results = {}\n",
    "        \n",
    "        # Test each activation function\n",
    "        for activation in activations:\n",
    "            network = VIXTradingNetwork()\n",
    "            network.model = tf.keras.Sequential([\n",
    "                Dense(128, activation=activation),\n",
    "                BatchNormalization(),\n",
    "                Dropout(0.2),\n",
    "                Dense(64, activation=activation),\n",
    "                BatchNormalization(),\n",
    "                Dropout(0.2),\n",
    "                Dense(32, activation=activation),\n",
    "                BatchNormalization(),\n",
    "                Dropout(0.2),\n",
    "                Dense(16, activation=activation),\n",
    "                BatchNormalization(),\n",
    "                Dense(5)  # Output layer\n",
    "            ])\n",
    "            \n",
    "            # Prepare and train data\n",
    "            X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = network.prepare_data(\n",
    "                self.X_train, self.y_train, utility_type='linear'\n",
    "            )\n",
    "            network.train(X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "            \n",
    "            # Update signals network with trained network\n",
    "            self.signals.network = network\n",
    "            \n",
    "            # Generate signals and compute metrics\n",
    "            signals = network.predict(X_test_scaled)\n",
    "            # Ensure signals and returns have compatible shapes\n",
    "            if signals.ndim == 2:\n",
    "                signals = signals[:, 0]\n",
    "            # Ensure returns matches signals length\n",
    "            returns = y_test_scaled[:, 0] if y_test_scaled.ndim > 1 else y_test_scaled\n",
    "            print(f\"[DEBUG] test_alternative_activations: signals shape: {signals.shape}, returns shape: {returns.shape}\")\n",
    "            metrics = self.signals.compute_performance_metrics(signals, returns)\n",
    "            \n",
    "            # Store results\n",
    "            results[activation] = metrics\n",
    "            \n",
    "            # Save individual activation results\n",
    "            self._save_activation_results(activation, metrics)\n",
    "        \n",
    "        # Test PReLU separately with proper configuration\n",
    "        network = VIXTradingNetwork()\n",
    "        network.model = tf.keras.Sequential([\n",
    "            Dense(128, activation=PReLU()),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            Dense(64, activation=PReLU()),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation=PReLU()),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            Dense(16, activation=PReLU()),\n",
    "            BatchNormalization(),\n",
    "            Dense(5)  # Output layer\n",
    "        ])\n",
    "        \n",
    "        # Prepare and train data\n",
    "        X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = network.prepare_data(\n",
    "            self.X_train, self.y_train, utility_type='linear'\n",
    "        )\n",
    "        network.train(X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "        \n",
    "        # Update signals network with trained network\n",
    "        self.signals.network = network\n",
    "        \n",
    "        # Generate signals and compute metrics\n",
    "        signals = network.predict(X_test_scaled)\n",
    "        # Ensure signals and returns have compatible shapes\n",
    "        if signals.ndim == 2:\n",
    "            signals = signals[:, 0]\n",
    "        # Ensure returns matches signals length\n",
    "        returns = y_test_scaled[:, 0] if y_test_scaled.ndim > 1 else y_test_scaled\n",
    "        print(f\"[DEBUG] test_alternative_activations: signals shape: {signals.shape}, returns shape: {returns.shape}\")\n",
    "        metrics = self.signals.compute_performance_metrics(signals, returns)\n",
    "        \n",
    "        # Store results\n",
    "        results['prelu'] = metrics\n",
    "        \n",
    "        # Save individual activation results\n",
    "        self._save_activation_results('prelu', metrics)\n",
    "        \n",
    "        # Generate summary report\n",
    "        self._generate_activation_summary(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_benchmark_comparison(self, n_steps=1000):\n",
    "        \"\"\"\n",
    "        Compare strategy performance with benchmarks.\n",
    "        \n",
    "        Args:\n",
    "            n_steps (int): Number of steps to simulate\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with benchmark comparison results\n",
    "        \"\"\"\n",
    "        # Initialize results storage\n",
    "        results = {\n",
    "            'benchmark_results': []\n",
    "        }\n",
    "\n",
    "        # Get simulation data\n",
    "        simulation_results = self.engine.simulate_trading_path(n_steps=n_steps)\n",
    "        state_vectors = simulation_results['state_vectors'].values\n",
    "        returns = simulation_results['returns']\n",
    "        \n",
    "        # Test constant strategies\n",
    "        for action_name, action in self.engine.actions.items():\n",
    "            print(f\"\\nTesting constant {action_name} strategy\")\n",
    "            \n",
    "           # Create constant signals based on position\n",
    "            signals = np.full(len(state_vectors), action['position'])\n",
    "            \n",
    "            # Get returns for this action\n",
    "            action_returns = returns[action_name]\n",
    "        \n",
    "            # Compute metrics\n",
    "            metrics = self.signals.compute_performance_metrics(\n",
    "                signals,\n",
    "                action_returns\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            results['benchmark_results'].append({\n",
    "                'strategy': f'constant_{action_name}',\n",
    "                'metrics': metrics\n",
    "            })\n",
    "            \n",
    "            # Save individual benchmark results\n",
    "            self._save_benchmark_results(f'constant_{action_name}', metrics)\n",
    "        \n",
    "        # Generate summary report\n",
    "        self._generate_benchmark_summary(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_cost_sensitivity_analysis(self, cost_levels=None):\n",
    "        \"\"\"Run cost sensitivity analysis.\n",
    "        \n",
    "        Args:\n",
    "            cost_levels (list): List of cost levels to test (in basis points)\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary with cost sensitivity results\n",
    "        \"\"\"\n",
    "        if cost_levels is None:\n",
    "            cost_levels = [20, 25, 30, 35, 40]  # Default cost levels in basis points\n",
    "            \n",
    "        results = {\n",
    "            'cost_levels': cost_levels,\n",
    "            'metrics': []\n",
    "        }\n",
    "        \n",
    "        for cost_level in cost_levels:\n",
    "            print(f\"\\nAnalyzing cost level: {cost_level} bps\")\n",
    "            \n",
    "            # Train network with transaction costs\n",
    "            network = VIXTradingNetwork(\n",
    "                input_dim=self.X_train.shape[1],\n",
    "                hidden_units=64,\n",
    "                output_dim=1,\n",
    "                use_prelu=True\n",
    "            )\n",
    "            network.train(self.X_train, self.y_train, transaction_cost=cost_level/10000)\n",
    "            \n",
    "            # Generate signals\n",
    "            signals = network.predict(self.X_test)\n",
    "            \n",
    "            # Ensure signals and returns have compatible shapes\n",
    "            signals = signals.flatten()\n",
    "            returns = self.y_test[:, 0]  # Use first return series\n",
    "            \n",
    "            # Compute metrics with transaction costs\n",
    "            metrics = self.signals.compute_performance_metrics(\n",
    "                signals=signals,\n",
    "                returns=returns,\n",
    "                transaction_cost=cost_level/10000\n",
    "            )\n",
    "            \n",
    "            results['metrics'].append(metrics)\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def _save_fold_results(self, fold, metrics):\n",
    "        \"\"\"Save results for a specific fold to a CSV file.\"\"\"\n",
    "        metrics_df = pd.DataFrame([metrics])\n",
    "        metrics_df.to_csv(f'data/robust_output/non_contiguous_fold_{fold}_results.csv', index=False)\n",
    "    \n",
    "    def _save_activation_results(self, activation, metrics):\n",
    "        \"\"\"Save results for a specific activation function to a CSV file.\"\"\"\n",
    "        metrics_df = pd.DataFrame([metrics])\n",
    "        metrics_df.to_csv(f'data/robust_output/activation_{activation}_results.csv', index=False)\n",
    "    \n",
    "    def _save_benchmark_results(self, strategy, metrics):\n",
    "        \"\"\"Save results for a specific benchmark strategy to a CSV file.\"\"\"\n",
    "        metrics_df = pd.DataFrame([metrics])\n",
    "        metrics_df.to_csv(f'data/robust_output/benchmark_{strategy}_results.csv', index=False)\n",
    "    \n",
    "    def _generate_non_contiguous_summary(self, results):\n",
    "        \"\"\"Generate summary statistics for non-contiguous fold testing.\"\"\"\n",
    "        # Convert results to DataFrame\n",
    "        summary_df = pd.DataFrame([r['metrics'] for r in results['fold_results']])\n",
    "        \n",
    "        # Save summary\n",
    "        summary_df.to_csv('data/robust_output/non_contiguous_summary.csv', index=False)\n",
    "        \n",
    "        # Create visualization\n",
    "        self._plot_non_contiguous_results(summary_df)\n",
    "    \n",
    "    def _generate_activation_summary(self, results):\n",
    "        \"\"\"Generate summary statistics for activation function testing.\"\"\"\n",
    "        # Convert results to DataFrame\n",
    "        summary_df = pd.DataFrame(list(results.values()))\n",
    "        summary_df['activation'] = list(results.keys())\n",
    "        \n",
    "        # Save summary\n",
    "        summary_df.to_csv('data/robust_output/activation_summary.csv', index=False)\n",
    "        \n",
    "        # Create visualization\n",
    "        self._plot_activation_results(summary_df)\n",
    "    \n",
    "    def _generate_benchmark_summary(self, results):\n",
    "        \"\"\"Generate summary statistics for benchmark comparison.\"\"\"\n",
    "        # Convert results to DataFrame\n",
    "        summary_df = pd.DataFrame([r['metrics'] for r in results['benchmark_results']])\n",
    "        summary_df['strategy'] = [r['strategy'] for r in results['benchmark_results']]\n",
    "        \n",
    "        # Save summary\n",
    "        summary_df.to_csv('data/robust_output/benchmark_summary.csv', index=False)\n",
    "        \n",
    "        # Create visualization\n",
    "        self._plot_benchmark_results(summary_df)\n",
    "    \n",
    "    def _plot_non_contiguous_results(self, df):\n",
    "        \"\"\"Create visualization for non-contiguous fold results.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Non-Contiguous Fold Analysis', fontsize=16)\n",
    "        \n",
    "        # Plot 1: Annual Return by Fold\n",
    "        axes[0, 0].plot(df.index + 1, df['annual_return'], 'b-o')\n",
    "        axes[0, 0].set_xlabel('Fold')\n",
    "        axes[0, 0].set_ylabel('Annual Return')\n",
    "        axes[0, 0].set_title('Annual Return by Fold')\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Plot 2: Sharpe Ratio by Fold\n",
    "        axes[0, 1].plot(df.index + 1, df['sharpe_ratio'], 'g-o')\n",
    "        axes[0, 1].set_xlabel('Fold')\n",
    "        axes[0, 1].set_ylabel('Sharpe Ratio')\n",
    "        axes[0, 1].set_title('Sharpe Ratio by Fold')\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # Plot 3: Maximum Drawdown by Fold\n",
    "        axes[1, 0].plot(df.index + 1, df['max_drawdown'], 'r-o')\n",
    "        axes[1, 0].set_xlabel('Fold')\n",
    "        axes[1, 0].set_ylabel('Maximum Drawdown')\n",
    "        axes[1, 0].set_title('Maximum Drawdown by Fold')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Plot 4: Turnover by Fold\n",
    "        axes[1, 1].plot(df.index + 1, df['turnover'], 'm-o')\n",
    "        axes[1, 1].set_xlabel('Fold')\n",
    "        axes[1, 1].set_ylabel('Turnover')\n",
    "        axes[1, 1].set_title('Turnover by Fold')\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figures/non_contiguous_analysis.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_activation_results(self, df):\n",
    "        \"\"\"Create visualization for activation function results.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Activation Function Comparison', fontsize=16)\n",
    "        \n",
    "        # Plot 1: Annual Return by Activation\n",
    "        axes[0, 0].bar(df['activation'], df['annual_return'])\n",
    "        axes[0, 0].set_xlabel('Activation Function')\n",
    "        axes[0, 0].set_ylabel('Annual Return')\n",
    "        axes[0, 0].set_title('Annual Return by Activation')\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Plot 2: Sharpe Ratio by Activation\n",
    "        axes[0, 1].bar(df['activation'], df['sharpe_ratio'])\n",
    "        axes[0, 1].set_xlabel('Activation Function')\n",
    "        axes[0, 1].set_ylabel('Sharpe Ratio')\n",
    "        axes[0, 1].set_title('Sharpe Ratio by Activation')\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # Plot 3: Maximum Drawdown by Activation\n",
    "        axes[1, 0].bar(df['activation'], df['max_drawdown'])\n",
    "        axes[1, 0].set_xlabel('Activation Function')\n",
    "        axes[1, 0].set_ylabel('Maximum Drawdown')\n",
    "        axes[1, 0].set_title('Maximum Drawdown by Activation')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Plot 4: Turnover by Activation\n",
    "        axes[1, 1].bar(df['activation'], df['turnover'])\n",
    "        axes[1, 1].set_xlabel('Activation Function')\n",
    "        axes[1, 1].set_ylabel('Turnover')\n",
    "        axes[1, 1].set_title('Turnover by Activation')\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figures/activation_comparison.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_benchmark_results(self, df):\n",
    "        \"\"\"Create visualization for benchmark comparison.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Benchmark Strategy Comparison', fontsize=16)\n",
    "        \n",
    "        # Plot 1: Annual Return by Strategy\n",
    "        axes[0, 0].bar(df['strategy'], df['annual_return'])\n",
    "        axes[0, 0].set_xlabel('Strategy')\n",
    "        axes[0, 0].set_ylabel('Annual Return')\n",
    "        axes[0, 0].set_title('Annual Return by Strategy')\n",
    "        axes[0, 0].grid(True)\n",
    "        plt.setp(axes[0, 0].xaxis.get_majorticklabels(), rotation=45)\n",
    "        \n",
    "        # Plot 2: Sharpe Ratio by Strategy\n",
    "        axes[0, 1].bar(df['strategy'], df['sharpe_ratio'])\n",
    "        axes[0, 1].set_xlabel('Strategy')\n",
    "        axes[0, 1].set_ylabel('Sharpe Ratio')\n",
    "        axes[0, 1].set_title('Sharpe Ratio by Strategy')\n",
    "        axes[0, 1].grid(True)\n",
    "        plt.setp(axes[0, 1].xaxis.get_majorticklabels(), rotation=45)\n",
    "        \n",
    "        # Plot 3: Maximum Drawdown by Strategy\n",
    "        axes[1, 0].bar(df['strategy'], df['max_drawdown'])\n",
    "        axes[1, 0].set_xlabel('Strategy')\n",
    "        axes[1, 0].set_ylabel('Maximum Drawdown')\n",
    "        axes[1, 0].set_title('Maximum Drawdown by Strategy')\n",
    "        axes[1, 0].grid(True)\n",
    "        plt.setp(axes[1, 0].xaxis.get_majorticklabels(), rotation=45)\n",
    "        \n",
    "        # Plot 4: Turnover by Strategy\n",
    "        axes[1, 1].bar(df['strategy'], df['turnover'])\n",
    "        axes[1, 1].set_xlabel('Strategy')\n",
    "        axes[1, 1].set_ylabel('Turnover')\n",
    "        axes[1, 1].set_title('Turnover by Strategy')\n",
    "        axes[1, 1].grid(True)\n",
    "        plt.setp(axes[1, 1].xaxis.get_majorticklabels(), rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figures/benchmark_comparison.png')\n",
    "        plt.close()\n",
    "\n",
    "def run_robustness_checks():\n",
    "    \"\"\"Main function to run all robustness checks.\"\"\"\n",
    "    print(\"Starting Phase 8: Robustness Checks...\")\n",
    "    \n",
    "    # Initialize robustness checks module\n",
    "    signals = VIXTradingSignals()\n",
    "    engine = VIXSimulationEngine()\n",
    "    checks = VIXRobustnessChecks(signals, engine)\n",
    "    \n",
    "    # Run non-contiguous fold testing\n",
    "    print(\"\\nRunning non-contiguous fold testing...\")\n",
    "    checks.run_non_contiguous_folds()\n",
    "    \n",
    "    # Test alternative activation functions\n",
    "    print(\"\\nTesting alternative activation functions...\")\n",
    "    checks.test_alternative_activations()\n",
    "    \n",
    "    # Run benchmark comparison\n",
    "    print(\"\\nRunning benchmark comparison...\")\n",
    "    checks.run_benchmark_comparison()\n",
    "    \n",
    "    # Run cost sensitivity analysis\n",
    "    print(\"\\nRunning cost sensitivity analysis...\")\n",
    "    checks.run_cost_sensitivity_analysis()\n",
    "    \n",
    "    print(\"\\nResults have been saved to:\")\n",
    "    print(\"- Non-contiguous fold results: data/robust_output/non_contiguous_*\")\n",
    "    print(\"- Activation function results: data/robust_output/activation_*\")\n",
    "    print(\"- Benchmark comparison results: data/robust_output/benchmark_*\")\n",
    "    print(\"- Cost sensitivity results: data/robust_output/cost_sensitivity_*\")\n",
    "    \n",
    "    print(\"\\nPhase 8 completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Phase 8: Robustness Checks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gd/qxxh82n95f57fhdhrdg746g80000gn/T/ipykernel_14771/207703377.py:28: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  self.state_vectors['date'] = pd.to_datetime(self.state_vectors['date'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded state vectors with shape: (3190, 12)\n",
      "Date range: 2008-04-01 00:00:00-05:00 to 2020-11-27 00:00:00-06:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gd/qxxh82n95f57fhdhrdg746g80000gn/T/ipykernel_14771/207703377.py:105: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  model_data = model_data.fillna(method='ffill').fillna(method='bfill')\n",
      "/var/folders/gd/qxxh82n95f57fhdhrdg746g80000gn/T/ipykernel_14771/207703377.py:28: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  self.state_vectors['date'] = pd.to_datetime(self.state_vectors['date'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully fit VAR model with 10 lags\n",
      "Loaded state vectors with shape: (3190, 12)\n",
      "Date range: 2008-04-01 00:00:00-05:00 to 2020-11-27 00:00:00-06:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gd/qxxh82n95f57fhdhrdg746g80000gn/T/ipykernel_14771/207703377.py:105: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  model_data = model_data.fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully fit VAR model with 10 lags\n",
      "\n",
      "Running non-contiguous fold testing...\n",
      "\n",
      "Running non-contiguous fold 1/10\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 1s 10ms/step - loss: 0.5755 - mae: 0.6416 - val_loss: 0.1605 - val_mae: 0.1865\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4991 - mae: 0.5761 - val_loss: 0.1701 - val_mae: 0.2631\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4542 - mae: 0.5500 - val_loss: 0.1684 - val_mae: 0.2031\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4110 - mae: 0.5111 - val_loss: 0.1872 - val_mae: 0.2459\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3860 - mae: 0.4842 - val_loss: 0.1992 - val_mae: 0.3357\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3777 - mae: 0.4726 - val_loss: 0.1502 - val_mae: 0.1889\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3412 - mae: 0.4410 - val_loss: 0.1648 - val_mae: 0.2657\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3053 - mae: 0.4072 - val_loss: 0.2027 - val_mae: 0.2879\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3026 - mae: 0.4068 - val_loss: 0.2435 - val_mae: 0.3112\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2848 - mae: 0.3787 - val_loss: 0.1720 - val_mae: 0.2238\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.2764 - mae: 0.3695 - val_loss: 0.1543 - val_mae: 0.1921\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2511 - mae: 0.3430 - val_loss: 0.1943 - val_mae: 0.1774\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2325 - mae: 0.3184 - val_loss: 0.2159 - val_mae: 0.2023\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2362 - mae: 0.3216 - val_loss: 0.1848 - val_mae: 0.2113\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2206 - mae: 0.2951 - val_loss: 0.1869 - val_mae: 0.1794\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2186 - mae: 0.3011 - val_loss: 0.1734 - val_mae: 0.1850\n",
      "5/5 [==============================] - 0s 789us/step\n",
      "[DEBUG] run_non_contiguous_folds: signals shape: (144,), returns shape: (144,)\n",
      "[DEBUG] signals shape: (144,), returns shape: (144,)\n",
      "\n",
      "Running non-contiguous fold 2/10\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 1s 9ms/step - loss: 0.5951 - mae: 0.6559 - val_loss: 0.1824 - val_mae: 0.1879\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5302 - mae: 0.6049 - val_loss: 0.1799 - val_mae: 0.2258\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4759 - mae: 0.5524 - val_loss: 0.1639 - val_mae: 0.2244\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4301 - mae: 0.5282 - val_loss: 0.1531 - val_mae: 0.1976\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4115 - mae: 0.5142 - val_loss: 0.1582 - val_mae: 0.2389\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3686 - mae: 0.4667 - val_loss: 0.1487 - val_mae: 0.1955\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3654 - mae: 0.4682 - val_loss: 0.1554 - val_mae: 0.1983\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3099 - mae: 0.4167 - val_loss: 0.1580 - val_mae: 0.1645\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.3133 - mae: 0.4168 - val_loss: 0.1696 - val_mae: 0.2154\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3006 - mae: 0.4060 - val_loss: 0.1592 - val_mae: 0.1932\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2688 - mae: 0.3755 - val_loss: 0.1524 - val_mae: 0.1764\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2505 - mae: 0.3508 - val_loss: 0.1552 - val_mae: 0.1715\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2414 - mae: 0.3359 - val_loss: 0.1625 - val_mae: 0.1923\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2300 - mae: 0.3122 - val_loss: 0.1893 - val_mae: 0.1819\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2203 - mae: 0.3075 - val_loss: 0.1854 - val_mae: 0.2898\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2071 - mae: 0.2860 - val_loss: 0.1749 - val_mae: 0.1847\n",
      "5/5 [==============================] - 0s 893us/step\n",
      "[DEBUG] run_non_contiguous_folds: signals shape: (144,), returns shape: (144,)\n",
      "[DEBUG] signals shape: (144,), returns shape: (144,)\n",
      "\n",
      "Running non-contiguous fold 3/10\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 1s 10ms/step - loss: 0.5587 - mae: 0.6289 - val_loss: 0.1453 - val_mae: 0.1704\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4895 - mae: 0.5739 - val_loss: 0.1496 - val_mae: 0.2100\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4452 - mae: 0.5445 - val_loss: 0.1594 - val_mae: 0.2455\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4425 - mae: 0.5377 - val_loss: 0.1636 - val_mae: 0.2335\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3756 - mae: 0.4738 - val_loss: 0.1730 - val_mae: 0.2799\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3604 - mae: 0.4654 - val_loss: 0.1650 - val_mae: 0.2578\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3381 - mae: 0.4463 - val_loss: 0.1639 - val_mae: 0.2609\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3165 - mae: 0.4291 - val_loss: 0.1689 - val_mae: 0.2624\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.2836 - mae: 0.3908 - val_loss: 0.1577 - val_mae: 0.2402\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2746 - mae: 0.3895 - val_loss: 0.1516 - val_mae: 0.2123\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.2542 - mae: 0.3662 - val_loss: 0.1671 - val_mae: 0.2313\n",
      "5/5 [==============================] - 0s 911us/step\n",
      "[DEBUG] run_non_contiguous_folds: signals shape: (144,), returns shape: (144,)\n",
      "[DEBUG] signals shape: (144,), returns shape: (144,)\n",
      "\n",
      "Running non-contiguous fold 4/10\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 2s 10ms/step - loss: 0.5367 - mae: 0.6053 - val_loss: 0.1571 - val_mae: 0.1905\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4813 - mae: 0.5630 - val_loss: 0.1518 - val_mae: 0.1897\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4345 - mae: 0.5361 - val_loss: 0.1509 - val_mae: 0.1809\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4066 - mae: 0.5013 - val_loss: 0.1528 - val_mae: 0.1831\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3817 - mae: 0.4885 - val_loss: 0.1504 - val_mae: 0.1847\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3780 - mae: 0.4812 - val_loss: 0.1458 - val_mae: 0.1679\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3126 - mae: 0.4092 - val_loss: 0.1439 - val_mae: 0.1875\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3014 - mae: 0.4074 - val_loss: 0.1494 - val_mae: 0.2139\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.2974 - mae: 0.4074 - val_loss: 0.1565 - val_mae: 0.2546\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2751 - mae: 0.3793 - val_loss: 0.1472 - val_mae: 0.1565\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2609 - mae: 0.3615 - val_loss: 0.1580 - val_mae: 0.2269\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2334 - mae: 0.3298 - val_loss: 0.1649 - val_mae: 0.2537\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2291 - mae: 0.3267 - val_loss: 0.1506 - val_mae: 0.1485\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2160 - mae: 0.2979 - val_loss: 0.1546 - val_mae: 0.2089\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2064 - mae: 0.2955 - val_loss: 0.1573 - val_mae: 0.2497\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2091 - mae: 0.2953 - val_loss: 0.1637 - val_mae: 0.2170\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1913 - mae: 0.2729 - val_loss: 0.1543 - val_mae: 0.1709\n",
      "5/5 [==============================] - 0s 810us/step\n",
      "[DEBUG] run_non_contiguous_folds: signals shape: (144,), returns shape: (144,)\n",
      "[DEBUG] signals shape: (144,), returns shape: (144,)\n",
      "\n",
      "Running non-contiguous fold 5/10\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 1s 20ms/step - loss: 0.5890 - mae: 0.6529 - val_loss: 0.1398 - val_mae: 0.1598\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4917 - mae: 0.5681 - val_loss: 0.1444 - val_mae: 0.1622\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4786 - mae: 0.5649 - val_loss: 0.1756 - val_mae: 0.1865\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4071 - mae: 0.5123 - val_loss: 0.2066 - val_mae: 0.1776\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3867 - mae: 0.4811 - val_loss: 0.2033 - val_mae: 0.1692\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3724 - mae: 0.4575 - val_loss: 0.2138 - val_mae: 0.2880\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3299 - mae: 0.4280 - val_loss: 0.2261 - val_mae: 0.3070\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3000 - mae: 0.4035 - val_loss: 0.1848 - val_mae: 0.2524\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2872 - mae: 0.3851 - val_loss: 0.1666 - val_mae: 0.2610\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2862 - mae: 0.3818 - val_loss: 0.1799 - val_mae: 0.2807\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2468 - mae: 0.3436 - val_loss: 0.1517 - val_mae: 0.1645\n",
      "5/5 [==============================] - 0s 813us/step\n",
      "[DEBUG] run_non_contiguous_folds: signals shape: (144,), returns shape: (144,)\n",
      "[DEBUG] signals shape: (144,), returns shape: (144,)\n",
      "\n",
      "Running non-contiguous fold 6/10\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 1s 10ms/step - loss: 0.5793 - mae: 0.6400 - val_loss: 0.1500 - val_mae: 0.1593\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5219 - mae: 0.5973 - val_loss: 0.1555 - val_mae: 0.1952\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4935 - mae: 0.5840 - val_loss: 0.1508 - val_mae: 0.2037\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4797 - mae: 0.5609 - val_loss: 0.1710 - val_mae: 0.2287\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4152 - mae: 0.5100 - val_loss: 0.1706 - val_mae: 0.1976\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4076 - mae: 0.5067 - val_loss: 0.1538 - val_mae: 0.1919\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3898 - mae: 0.4846 - val_loss: 0.1457 - val_mae: 0.1861\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3590 - mae: 0.4533 - val_loss: 0.1670 - val_mae: 0.2119\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3379 - mae: 0.4429 - val_loss: 0.1803 - val_mae: 0.2650\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3279 - mae: 0.4223 - val_loss: 0.1446 - val_mae: 0.1621\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3205 - mae: 0.4051 - val_loss: 0.1478 - val_mae: 0.1683\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2714 - mae: 0.3747 - val_loss: 0.1753 - val_mae: 0.1803\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2676 - mae: 0.3601 - val_loss: 0.1741 - val_mae: 0.1654\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2728 - mae: 0.3602 - val_loss: 0.1631 - val_mae: 0.1771\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2416 - mae: 0.3310 - val_loss: 0.1691 - val_mae: 0.1682\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2290 - mae: 0.3059 - val_loss: 0.1530 - val_mae: 0.1593\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2430 - mae: 0.3315 - val_loss: 0.1460 - val_mae: 0.1483\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2116 - mae: 0.2947 - val_loss: 0.1407 - val_mae: 0.1533\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2071 - mae: 0.2827 - val_loss: 0.1646 - val_mae: 0.1548\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2144 - mae: 0.2946 - val_loss: 0.1911 - val_mae: 0.1614\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1950 - mae: 0.2728 - val_loss: 0.1694 - val_mae: 0.1652\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1871 - mae: 0.2446 - val_loss: 0.1577 - val_mae: 0.1815\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1935 - mae: 0.2544 - val_loss: 0.1660 - val_mae: 0.1904\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1811 - mae: 0.2427 - val_loss: 0.1466 - val_mae: 0.1506\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1747 - mae: 0.2346 - val_loss: 0.1424 - val_mae: 0.1634\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1739 - mae: 0.2281 - val_loss: 0.1473 - val_mae: 0.1570\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1677 - mae: 0.2213 - val_loss: 0.1602 - val_mae: 0.1583\n",
      "Epoch 28/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1640 - mae: 0.2142 - val_loss: 0.1500 - val_mae: 0.1673\n",
      "5/5 [==============================] - 0s 863us/step\n",
      "[DEBUG] run_non_contiguous_folds: signals shape: (144,), returns shape: (144,)\n",
      "[DEBUG] signals shape: (144,), returns shape: (144,)\n",
      "\n",
      "Running non-contiguous fold 7/10\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 1s 10ms/step - loss: 0.5580 - mae: 0.6278 - val_loss: 0.1445 - val_mae: 0.1922\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5215 - mae: 0.6013 - val_loss: 0.1550 - val_mae: 0.1705\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.4415 - mae: 0.5336 - val_loss: 0.1545 - val_mae: 0.2073\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4174 - mae: 0.5141 - val_loss: 0.1525 - val_mae: 0.1889\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3879 - mae: 0.4905 - val_loss: 0.1549 - val_mae: 0.1763\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3605 - mae: 0.4572 - val_loss: 0.1636 - val_mae: 0.1915\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3273 - mae: 0.4426 - val_loss: 0.1620 - val_mae: 0.1892\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3156 - mae: 0.4216 - val_loss: 0.1544 - val_mae: 0.2152\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3020 - mae: 0.4116 - val_loss: 0.1563 - val_mae: 0.1679\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2637 - mae: 0.3702 - val_loss: 0.1481 - val_mae: 0.1860\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.2751 - mae: 0.3679 - val_loss: 0.1446 - val_mae: 0.1565\n",
      "5/5 [==============================] - 0s 855us/step\n",
      "[DEBUG] run_non_contiguous_folds: signals shape: (144,), returns shape: (144,)\n",
      "[DEBUG] signals shape: (144,), returns shape: (144,)\n",
      "\n",
      "Running non-contiguous fold 8/10\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 1s 12ms/step - loss: 0.5427 - mae: 0.6013 - val_loss: 0.1595 - val_mae: 0.2487\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5590 - mae: 0.6313 - val_loss: 0.1808 - val_mae: 0.2844\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5023 - mae: 0.5810 - val_loss: 0.1887 - val_mae: 0.2817\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4618 - mae: 0.5603 - val_loss: 0.2002 - val_mae: 0.3311\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4235 - mae: 0.5299 - val_loss: 0.1755 - val_mae: 0.2587\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3829 - mae: 0.4865 - val_loss: 0.1536 - val_mae: 0.1956\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3572 - mae: 0.4612 - val_loss: 0.1781 - val_mae: 0.1899\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3460 - mae: 0.4528 - val_loss: 0.1902 - val_mae: 0.2211\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3432 - mae: 0.4427 - val_loss: 0.1862 - val_mae: 0.2881\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3054 - mae: 0.4073 - val_loss: 0.1610 - val_mae: 0.2607\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2752 - mae: 0.3838 - val_loss: 0.1558 - val_mae: 0.2311\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2839 - mae: 0.3838 - val_loss: 0.1548 - val_mae: 0.2468\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2486 - mae: 0.3489 - val_loss: 0.1496 - val_mae: 0.2252\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2354 - mae: 0.3357 - val_loss: 0.1434 - val_mae: 0.1633\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2250 - mae: 0.3293 - val_loss: 0.1463 - val_mae: 0.1716\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2093 - mae: 0.3056 - val_loss: 0.1482 - val_mae: 0.1651\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2007 - mae: 0.2923 - val_loss: 0.1482 - val_mae: 0.1941\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1967 - mae: 0.2785 - val_loss: 0.1514 - val_mae: 0.1940\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1833 - mae: 0.2619 - val_loss: 0.1602 - val_mae: 0.1563\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1841 - mae: 0.2742 - val_loss: 0.1461 - val_mae: 0.2021\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1735 - mae: 0.2469 - val_loss: 0.1438 - val_mae: 0.1691\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1645 - mae: 0.2409 - val_loss: 0.1465 - val_mae: 0.2030\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1611 - mae: 0.2418 - val_loss: 0.1414 - val_mae: 0.1822\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1551 - mae: 0.2301 - val_loss: 0.1440 - val_mae: 0.1797\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1518 - mae: 0.2171 - val_loss: 0.1409 - val_mae: 0.1763\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1512 - mae: 0.2171 - val_loss: 0.1409 - val_mae: 0.1699\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1433 - mae: 0.2005 - val_loss: 0.1408 - val_mae: 0.1706\n",
      "Epoch 28/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1394 - mae: 0.2031 - val_loss: 0.1448 - val_mae: 0.1803\n",
      "Epoch 29/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1367 - mae: 0.1933 - val_loss: 0.1406 - val_mae: 0.1486\n",
      "Epoch 30/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1336 - mae: 0.1809 - val_loss: 0.1597 - val_mae: 0.2037\n",
      "Epoch 31/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1320 - mae: 0.1818 - val_loss: 0.1397 - val_mae: 0.1497\n",
      "Epoch 32/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1236 - mae: 0.1736 - val_loss: 0.1440 - val_mae: 0.1809\n",
      "Epoch 33/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1306 - mae: 0.1752 - val_loss: 0.1398 - val_mae: 0.1578\n",
      "Epoch 34/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1203 - mae: 0.1640 - val_loss: 0.1396 - val_mae: 0.1496\n",
      "Epoch 35/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1251 - mae: 0.1664 - val_loss: 0.1465 - val_mae: 0.1815\n",
      "Epoch 36/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1231 - mae: 0.1555 - val_loss: 0.1392 - val_mae: 0.1452\n",
      "Epoch 37/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1189 - mae: 0.1528 - val_loss: 0.1392 - val_mae: 0.1464\n",
      "Epoch 38/100\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.1180 - mae: 0.1575 - val_loss: 0.1393 - val_mae: 0.1479\n",
      "Epoch 39/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1164 - mae: 0.1449 - val_loss: 0.1399 - val_mae: 0.1581\n",
      "Epoch 40/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1142 - mae: 0.1422 - val_loss: 0.1411 - val_mae: 0.1484\n",
      "Epoch 41/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1151 - mae: 0.1457 - val_loss: 0.1413 - val_mae: 0.1603\n",
      "Epoch 42/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1138 - mae: 0.1389 - val_loss: 0.1410 - val_mae: 0.1587\n",
      "Epoch 43/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1123 - mae: 0.1374 - val_loss: 0.1392 - val_mae: 0.1411\n",
      "Epoch 44/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1104 - mae: 0.1331 - val_loss: 0.1391 - val_mae: 0.1456\n",
      "Epoch 45/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1101 - mae: 0.1309 - val_loss: 0.1392 - val_mae: 0.1462\n",
      "Epoch 46/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1110 - mae: 0.1307 - val_loss: 0.1394 - val_mae: 0.1548\n",
      "Epoch 47/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1094 - mae: 0.1287 - val_loss: 0.1394 - val_mae: 0.1457\n",
      "Epoch 48/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1083 - mae: 0.1273 - val_loss: 0.1392 - val_mae: 0.1520\n",
      "Epoch 49/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1079 - mae: 0.1253 - val_loss: 0.1394 - val_mae: 0.1424\n",
      "Epoch 50/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1074 - mae: 0.1215 - val_loss: 0.1401 - val_mae: 0.1561\n",
      "Epoch 51/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1072 - mae: 0.1214 - val_loss: 0.1394 - val_mae: 0.1429\n",
      "Epoch 52/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1077 - mae: 0.1230 - val_loss: 0.1399 - val_mae: 0.1517\n",
      "Epoch 53/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1068 - mae: 0.1186 - val_loss: 0.1391 - val_mae: 0.1429\n",
      "Epoch 54/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1067 - mae: 0.1180 - val_loss: 0.1391 - val_mae: 0.1439\n",
      "Epoch 55/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1067 - mae: 0.1178 - val_loss: 0.1391 - val_mae: 0.1405\n",
      "Epoch 56/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1063 - mae: 0.1172 - val_loss: 0.1391 - val_mae: 0.1422\n",
      "Epoch 57/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1065 - mae: 0.1182 - val_loss: 0.1391 - val_mae: 0.1449\n",
      "Epoch 58/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1063 - mae: 0.1153 - val_loss: 0.1393 - val_mae: 0.1445\n",
      "Epoch 59/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1061 - mae: 0.1146 - val_loss: 0.1392 - val_mae: 0.1456\n",
      "Epoch 60/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1061 - mae: 0.1128 - val_loss: 0.1390 - val_mae: 0.1398\n",
      "Epoch 61/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1060 - mae: 0.1136 - val_loss: 0.1392 - val_mae: 0.1461\n",
      "Epoch 62/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1059 - mae: 0.1126 - val_loss: 0.1391 - val_mae: 0.1415\n",
      "Epoch 63/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1058 - mae: 0.1122 - val_loss: 0.1391 - val_mae: 0.1434\n",
      "Epoch 64/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1058 - mae: 0.1117 - val_loss: 0.1391 - val_mae: 0.1467\n",
      "Epoch 65/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1057 - mae: 0.1106 - val_loss: 0.1391 - val_mae: 0.1402\n",
      "Epoch 66/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1057 - mae: 0.1105 - val_loss: 0.1390 - val_mae: 0.1415\n",
      "Epoch 67/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1057 - mae: 0.1099 - val_loss: 0.1392 - val_mae: 0.1457\n",
      "Epoch 68/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1095 - val_loss: 0.1391 - val_mae: 0.1402\n",
      "Epoch 69/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1090 - val_loss: 0.1390 - val_mae: 0.1401\n",
      "Epoch 70/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1084 - val_loss: 0.1390 - val_mae: 0.1409\n",
      "Epoch 71/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1080 - val_loss: 0.1390 - val_mae: 0.1409\n",
      "Epoch 72/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1082 - val_loss: 0.1390 - val_mae: 0.1404\n",
      "Epoch 73/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1080 - val_loss: 0.1390 - val_mae: 0.1398\n",
      "Epoch 74/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1074 - val_loss: 0.1391 - val_mae: 0.1414\n",
      "Epoch 75/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1071 - val_loss: 0.1390 - val_mae: 0.1397\n",
      "Epoch 76/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1073 - val_loss: 0.1390 - val_mae: 0.1399\n",
      "Epoch 77/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1069 - val_loss: 0.1390 - val_mae: 0.1401\n",
      "Epoch 78/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1067 - val_loss: 0.1390 - val_mae: 0.1399\n",
      "Epoch 79/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1066 - val_loss: 0.1390 - val_mae: 0.1399\n",
      "Epoch 80/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1066 - val_loss: 0.1390 - val_mae: 0.1397\n",
      "Epoch 81/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1065 - val_loss: 0.1390 - val_mae: 0.1396\n",
      "Epoch 82/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1064 - val_loss: 0.1390 - val_mae: 0.1396\n",
      "Epoch 83/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1064 - val_loss: 0.1390 - val_mae: 0.1397\n",
      "Epoch 84/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1062 - val_loss: 0.1390 - val_mae: 0.1401\n",
      "Epoch 85/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.1056 - mae: 0.1062 - val_loss: 0.1390 - val_mae: 0.1398\n",
      "Epoch 86/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.1056 - mae: 0.1060 - val_loss: 0.1390 - val_mae: 0.1396\n",
      "Epoch 87/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1060 - val_loss: 0.1390 - val_mae: 0.1395\n",
      "Epoch 88/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1059 - val_loss: 0.1390 - val_mae: 0.1398\n",
      "Epoch 89/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1060 - val_loss: 0.1390 - val_mae: 0.1395\n",
      "Epoch 90/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.1056 - mae: 0.1058 - val_loss: 0.1390 - val_mae: 0.1396\n",
      "Epoch 91/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1058 - val_loss: 0.1390 - val_mae: 0.1396\n",
      "Epoch 92/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1058 - val_loss: 0.1390 - val_mae: 0.1395\n",
      "Epoch 93/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1058 - val_loss: 0.1390 - val_mae: 0.1396\n",
      "Epoch 94/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1058 - val_loss: 0.1390 - val_mae: 0.1396\n",
      "Epoch 95/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1057 - val_loss: 0.1390 - val_mae: 0.1396\n",
      "Epoch 96/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1057 - val_loss: 0.1390 - val_mae: 0.1396\n",
      "Epoch 97/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1057 - val_loss: 0.1390 - val_mae: 0.1395\n",
      "Epoch 98/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1057 - val_loss: 0.1390 - val_mae: 0.1396\n",
      "Epoch 99/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1057 - val_loss: 0.1390 - val_mae: 0.1396\n",
      "Epoch 100/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1056 - mae: 0.1057 - val_loss: 0.1390 - val_mae: 0.1395\n",
      "5/5 [==============================] - 0s 860us/step\n",
      "[DEBUG] run_non_contiguous_folds: signals shape: (144,), returns shape: (144,)\n",
      "[DEBUG] signals shape: (144,), returns shape: (144,)\n",
      "\n",
      "Running non-contiguous fold 9/10\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 2s 9ms/step - loss: 0.5940 - mae: 0.6566 - val_loss: 0.1490 - val_mae: 0.2059\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5360 - mae: 0.6096 - val_loss: 0.1450 - val_mae: 0.1950\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4811 - mae: 0.5659 - val_loss: 0.1509 - val_mae: 0.1622\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4634 - mae: 0.5511 - val_loss: 0.1503 - val_mae: 0.1540\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4303 - mae: 0.5317 - val_loss: 0.1562 - val_mae: 0.1689\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3990 - mae: 0.4921 - val_loss: 0.1613 - val_mae: 0.2225\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3666 - mae: 0.4825 - val_loss: 0.1533 - val_mae: 0.1826\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3793 - mae: 0.4827 - val_loss: 0.1398 - val_mae: 0.1639\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3440 - mae: 0.4578 - val_loss: 0.1434 - val_mae: 0.1927\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3237 - mae: 0.4395 - val_loss: 0.1403 - val_mae: 0.1644\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3052 - mae: 0.4253 - val_loss: 0.1433 - val_mae: 0.1509\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2902 - mae: 0.4051 - val_loss: 0.1400 - val_mae: 0.1459\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2741 - mae: 0.3812 - val_loss: 0.1491 - val_mae: 0.2306\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2593 - mae: 0.3654 - val_loss: 0.1500 - val_mae: 0.2290\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2349 - mae: 0.3436 - val_loss: 0.1453 - val_mae: 0.1705\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2347 - mae: 0.3380 - val_loss: 0.1557 - val_mae: 0.2568\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2155 - mae: 0.3170 - val_loss: 0.1483 - val_mae: 0.2122\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2012 - mae: 0.3011 - val_loss: 0.1461 - val_mae: 0.1892\n",
      "5/5 [==============================] - 0s 833us/step\n",
      "[DEBUG] run_non_contiguous_folds: signals shape: (144,), returns shape: (144,)\n",
      "[DEBUG] signals shape: (144,), returns shape: (144,)\n",
      "\n",
      "Running non-contiguous fold 10/10\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 1s 21ms/step - loss: 0.6477 - mae: 0.6887 - val_loss: 0.1610 - val_mae: 0.2297\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5330 - mae: 0.6068 - val_loss: 0.1482 - val_mae: 0.1601\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5084 - mae: 0.5882 - val_loss: 0.1437 - val_mae: 0.1578\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4504 - mae: 0.5369 - val_loss: 0.1418 - val_mae: 0.1780\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4286 - mae: 0.5256 - val_loss: 0.1599 - val_mae: 0.1699\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3969 - mae: 0.4920 - val_loss: 0.1778 - val_mae: 0.1717\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3367 - mae: 0.4385 - val_loss: 0.1733 - val_mae: 0.1531\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3325 - mae: 0.4327 - val_loss: 0.1532 - val_mae: 0.1693\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3181 - mae: 0.4210 - val_loss: 0.1589 - val_mae: 0.1567\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2900 - mae: 0.3877 - val_loss: 0.1618 - val_mae: 0.1577\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2791 - mae: 0.3692 - val_loss: 0.1722 - val_mae: 0.1775\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2690 - mae: 0.3675 - val_loss: 0.1677 - val_mae: 0.1653\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2594 - mae: 0.3515 - val_loss: 0.1894 - val_mae: 0.2281\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.2470 - mae: 0.3480 - val_loss: 0.1521 - val_mae: 0.2086\n",
      "5/5 [==============================] - 0s 808us/step\n",
      "[DEBUG] run_non_contiguous_folds: signals shape: (144,), returns shape: (144,)\n",
      "[DEBUG] signals shape: (144,), returns shape: (144,)\n",
      "\n",
      "Testing alternative activation functions...\n",
      "Epoch 1/100\n",
      "20/20 [==============================] - 1s 8ms/step - loss: 1.3087 - mae: 0.8331 - val_loss: 0.1088 - val_mae: 0.1410\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.9178 - mae: 0.7011 - val_loss: 0.0925 - val_mae: 0.1513\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6469 - mae: 0.5872 - val_loss: 0.0739 - val_mae: 0.1390\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4935 - mae: 0.5153 - val_loss: 0.0636 - val_mae: 0.1326\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4155 - mae: 0.4661 - val_loss: 0.0591 - val_mae: 0.1277\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2871 - mae: 0.3887 - val_loss: 0.0541 - val_mae: 0.1274\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2444 - mae: 0.3617 - val_loss: 0.0519 - val_mae: 0.1189\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2332 - mae: 0.3495 - val_loss: 0.0505 - val_mae: 0.1227\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1612 - mae: 0.2952 - val_loss: 0.0485 - val_mae: 0.1247\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1607 - mae: 0.2824 - val_loss: 0.0464 - val_mae: 0.1315\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1536 - mae: 0.2759 - val_loss: 0.0317 - val_mae: 0.1127\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1357 - mae: 0.2577 - val_loss: 0.0248 - val_mae: 0.0941\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1307 - mae: 0.2531 - val_loss: 0.0193 - val_mae: 0.0766\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1210 - mae: 0.2431 - val_loss: 0.0176 - val_mae: 0.0683\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0873 - mae: 0.2067 - val_loss: 0.0165 - val_mae: 0.0683\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0965 - mae: 0.2095 - val_loss: 0.0134 - val_mae: 0.0626\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0863 - mae: 0.1963 - val_loss: 0.0119 - val_mae: 0.0538\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1024 - mae: 0.2171 - val_loss: 0.0184 - val_mae: 0.0759\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0706 - mae: 0.1826 - val_loss: 0.0142 - val_mae: 0.0706\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0764 - mae: 0.1868 - val_loss: 0.0123 - val_mae: 0.0669\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0528 - mae: 0.1496 - val_loss: 0.0087 - val_mae: 0.0562\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0651 - mae: 0.1728 - val_loss: 0.0079 - val_mae: 0.0543\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0524 - mae: 0.1541 - val_loss: 0.0079 - val_mae: 0.0568\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0596 - mae: 0.1570 - val_loss: 0.0079 - val_mae: 0.0581\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0503 - mae: 0.1468 - val_loss: 0.0075 - val_mae: 0.0568\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 0s 10ms/step - loss: 0.0465 - mae: 0.1455 - val_loss: 0.0070 - val_mae: 0.0531\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0509 - mae: 0.1487 - val_loss: 0.0062 - val_mae: 0.0469\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0492 - mae: 0.1460 - val_loss: 0.0067 - val_mae: 0.0508\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0437 - mae: 0.1304 - val_loss: 0.0058 - val_mae: 0.0446\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0609 - mae: 0.1644 - val_loss: 0.0067 - val_mae: 0.0474\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.0330 - mae: 0.1192 - val_loss: 0.0063 - val_mae: 0.0444\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0392 - mae: 0.1277 - val_loss: 0.0059 - val_mae: 0.0429\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0500 - mae: 0.1413 - val_loss: 0.0066 - val_mae: 0.0449\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0308 - mae: 0.1152 - val_loss: 0.0071 - val_mae: 0.0493\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0443 - mae: 0.1335 - val_loss: 0.0069 - val_mae: 0.0492\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0355 - mae: 0.1166 - val_loss: 0.0065 - val_mae: 0.0465\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0324 - mae: 0.1118 - val_loss: 0.0062 - val_mae: 0.0437\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0430 - mae: 0.1287 - val_loss: 0.0060 - val_mae: 0.0418\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0312 - mae: 0.1084 - val_loss: 0.0059 - val_mae: 0.0391\n",
      "5/5 [==============================] - 0s 831us/step\n",
      "[DEBUG] test_alternative_activations: signals shape: (160,), returns shape: (160,)\n",
      "[DEBUG] signals shape: (160,), returns shape: (160,)\n",
      "Epoch 1/100\n",
      "20/20 [==============================] - 1s 8ms/step - loss: 0.8729 - mae: 0.7181 - val_loss: 0.0838 - val_mae: 0.1311\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5234 - mae: 0.5541 - val_loss: 0.0601 - val_mae: 0.1165\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3697 - mae: 0.4648 - val_loss: 0.0440 - val_mae: 0.1080\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3024 - mae: 0.4207 - val_loss: 0.0460 - val_mae: 0.1001\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2281 - mae: 0.3454 - val_loss: 0.0419 - val_mae: 0.1026\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2038 - mae: 0.3515 - val_loss: 0.0359 - val_mae: 0.1135\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1660 - mae: 0.3008 - val_loss: 0.0302 - val_mae: 0.1006\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1557 - mae: 0.2977 - val_loss: 0.0249 - val_mae: 0.0890\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1258 - mae: 0.2615 - val_loss: 0.0208 - val_mae: 0.0873\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1206 - mae: 0.2693 - val_loss: 0.0186 - val_mae: 0.0714\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1034 - mae: 0.2431 - val_loss: 0.0192 - val_mae: 0.0823\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0897 - mae: 0.2242 - val_loss: 0.0183 - val_mae: 0.0725\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0932 - mae: 0.2318 - val_loss: 0.0169 - val_mae: 0.0675\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0876 - mae: 0.2267 - val_loss: 0.0156 - val_mae: 0.0710\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0815 - mae: 0.2142 - val_loss: 0.0159 - val_mae: 0.0667\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0675 - mae: 0.1979 - val_loss: 0.0142 - val_mae: 0.0557\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0600 - mae: 0.1816 - val_loss: 0.0128 - val_mae: 0.0521\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0637 - mae: 0.1959 - val_loss: 0.0130 - val_mae: 0.0537\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0643 - mae: 0.1868 - val_loss: 0.0137 - val_mae: 0.0652\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0590 - mae: 0.1856 - val_loss: 0.0255 - val_mae: 0.1352\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0489 - mae: 0.1626 - val_loss: 0.0172 - val_mae: 0.1082\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0504 - mae: 0.1666 - val_loss: 0.0120 - val_mae: 0.0701\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0517 - mae: 0.1673 - val_loss: 0.0132 - val_mae: 0.0775\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0455 - mae: 0.1581 - val_loss: 0.0130 - val_mae: 0.0739\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0440 - mae: 0.1571 - val_loss: 0.0102 - val_mae: 0.0547\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0450 - mae: 0.1514 - val_loss: 0.0110 - val_mae: 0.0625\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0382 - mae: 0.1418 - val_loss: 0.0111 - val_mae: 0.0719\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0409 - mae: 0.1531 - val_loss: 0.0121 - val_mae: 0.0809\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0363 - mae: 0.1412 - val_loss: 0.0104 - val_mae: 0.0696\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0322 - mae: 0.1301 - val_loss: 0.0091 - val_mae: 0.0616\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0409 - mae: 0.1480 - val_loss: 0.0095 - val_mae: 0.0581\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0266 - mae: 0.1160 - val_loss: 0.0104 - val_mae: 0.0665\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0355 - mae: 0.1416 - val_loss: 0.0140 - val_mae: 0.0934\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0295 - mae: 0.1231 - val_loss: 0.0130 - val_mae: 0.0871\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0290 - mae: 0.1197 - val_loss: 0.0162 - val_mae: 0.1043\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0316 - mae: 0.1268 - val_loss: 0.0243 - val_mae: 0.1343\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0266 - mae: 0.1179 - val_loss: 0.0130 - val_mae: 0.0871\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0301 - mae: 0.1209 - val_loss: 0.0126 - val_mae: 0.0828\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0249 - mae: 0.1119 - val_loss: 0.0086 - val_mae: 0.0587\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0232 - mae: 0.1044 - val_loss: 0.0107 - val_mae: 0.0739\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0324 - mae: 0.1333 - val_loss: 0.0064 - val_mae: 0.0353\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1112 - val_loss: 0.0073 - val_mae: 0.0368\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0247 - mae: 0.1132 - val_loss: 0.0064 - val_mae: 0.0321\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0254 - mae: 0.1178 - val_loss: 0.0059 - val_mae: 0.0307\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0274 - mae: 0.1173 - val_loss: 0.0055 - val_mae: 0.0250\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0262 - mae: 0.1165 - val_loss: 0.0057 - val_mae: 0.0269\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0251 - mae: 0.1134 - val_loss: 0.0066 - val_mae: 0.0312\n",
      "Epoch 48/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0219 - mae: 0.1085 - val_loss: 0.0065 - val_mae: 0.0424\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0239 - mae: 0.1128 - val_loss: 0.0057 - val_mae: 0.0327\n",
      "Epoch 50/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0261 - mae: 0.1184 - val_loss: 0.0062 - val_mae: 0.0341\n",
      "Epoch 51/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0273 - mae: 0.1202 - val_loss: 0.0060 - val_mae: 0.0309\n",
      "Epoch 52/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0230 - mae: 0.1111 - val_loss: 0.0062 - val_mae: 0.0302\n",
      "Epoch 53/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0214 - mae: 0.1032 - val_loss: 0.0055 - val_mae: 0.0300\n",
      "Epoch 54/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0216 - mae: 0.1003 - val_loss: 0.0066 - val_mae: 0.0420\n",
      "Epoch 55/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0223 - mae: 0.1038 - val_loss: 0.0053 - val_mae: 0.0336\n",
      "Epoch 56/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0229 - mae: 0.1088 - val_loss: 0.0054 - val_mae: 0.0354\n",
      "Epoch 57/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0221 - mae: 0.1031 - val_loss: 0.0057 - val_mae: 0.0345\n",
      "Epoch 58/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0225 - mae: 0.1069 - val_loss: 0.0048 - val_mae: 0.0239\n",
      "Epoch 59/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0224 - mae: 0.1064 - val_loss: 0.0051 - val_mae: 0.0251\n",
      "Epoch 60/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0183 - mae: 0.0956 - val_loss: 0.0053 - val_mae: 0.0269\n",
      "Epoch 61/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0219 - mae: 0.1073 - val_loss: 0.0051 - val_mae: 0.0309\n",
      "Epoch 62/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0182 - mae: 0.0952 - val_loss: 0.0060 - val_mae: 0.0367\n",
      "Epoch 63/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0218 - mae: 0.1064 - val_loss: 0.0054 - val_mae: 0.0350\n",
      "Epoch 64/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0222 - mae: 0.1053 - val_loss: 0.0054 - val_mae: 0.0281\n",
      "Epoch 65/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0169 - mae: 0.0892 - val_loss: 0.0048 - val_mae: 0.0238\n",
      "Epoch 66/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0199 - mae: 0.0978 - val_loss: 0.0045 - val_mae: 0.0211\n",
      "Epoch 67/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0186 - mae: 0.0959 - val_loss: 0.0053 - val_mae: 0.0267\n",
      "Epoch 68/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.0183 - mae: 0.0946 - val_loss: 0.0052 - val_mae: 0.0266\n",
      "Epoch 69/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0177 - mae: 0.0922 - val_loss: 0.0052 - val_mae: 0.0314\n",
      "Epoch 70/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0146 - mae: 0.0856 - val_loss: 0.0049 - val_mae: 0.0286\n",
      "Epoch 71/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0185 - mae: 0.0968 - val_loss: 0.0047 - val_mae: 0.0280\n",
      "Epoch 72/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0170 - mae: 0.0868 - val_loss: 0.0049 - val_mae: 0.0255\n",
      "Epoch 73/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0188 - mae: 0.0964 - val_loss: 0.0048 - val_mae: 0.0301\n",
      "Epoch 74/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0178 - mae: 0.0941 - val_loss: 0.0046 - val_mae: 0.0279\n",
      "Epoch 75/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0151 - mae: 0.0841 - val_loss: 0.0047 - val_mae: 0.0280\n",
      "Epoch 76/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0165 - mae: 0.0894 - val_loss: 0.0044 - val_mae: 0.0219\n",
      "Epoch 77/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0128 - mae: 0.0754 - val_loss: 0.0044 - val_mae: 0.0223\n",
      "Epoch 78/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0170 - mae: 0.0926 - val_loss: 0.0044 - val_mae: 0.0211\n",
      "Epoch 79/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0142 - mae: 0.0835 - val_loss: 0.0043 - val_mae: 0.0175\n",
      "Epoch 80/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0138 - mae: 0.0809 - val_loss: 0.0044 - val_mae: 0.0188\n",
      "Epoch 81/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0163 - mae: 0.0861 - val_loss: 0.0042 - val_mae: 0.0165\n",
      "Epoch 82/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0166 - mae: 0.0870 - val_loss: 0.0042 - val_mae: 0.0172\n",
      "Epoch 83/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0173 - mae: 0.0903 - val_loss: 0.0044 - val_mae: 0.0182\n",
      "Epoch 84/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0175 - mae: 0.0931 - val_loss: 0.0045 - val_mae: 0.0227\n",
      "Epoch 85/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0131 - mae: 0.0786 - val_loss: 0.0045 - val_mae: 0.0196\n",
      "Epoch 86/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0165 - mae: 0.0883 - val_loss: 0.0044 - val_mae: 0.0216\n",
      "Epoch 87/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0134 - mae: 0.0782 - val_loss: 0.0044 - val_mae: 0.0217\n",
      "Epoch 88/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0152 - mae: 0.0852 - val_loss: 0.0044 - val_mae: 0.0172\n",
      "Epoch 89/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0129 - mae: 0.0790 - val_loss: 0.0042 - val_mae: 0.0165\n",
      "Epoch 90/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0165 - mae: 0.0911 - val_loss: 0.0041 - val_mae: 0.0180\n",
      "Epoch 91/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0129 - mae: 0.0767 - val_loss: 0.0043 - val_mae: 0.0208\n",
      "Epoch 92/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0134 - mae: 0.0767 - val_loss: 0.0045 - val_mae: 0.0241\n",
      "Epoch 93/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0126 - mae: 0.0756 - val_loss: 0.0042 - val_mae: 0.0208\n",
      "Epoch 94/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0109 - mae: 0.0687 - val_loss: 0.0043 - val_mae: 0.0207\n",
      "Epoch 95/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0117 - mae: 0.0671 - val_loss: 0.0042 - val_mae: 0.0191\n",
      "Epoch 96/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0124 - mae: 0.0765 - val_loss: 0.0043 - val_mae: 0.0207\n",
      "Epoch 97/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0094 - mae: 0.0640 - val_loss: 0.0040 - val_mae: 0.0123\n",
      "Epoch 98/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0114 - mae: 0.0690 - val_loss: 0.0041 - val_mae: 0.0153\n",
      "Epoch 99/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0137 - mae: 0.0812 - val_loss: 0.0042 - val_mae: 0.0180\n",
      "Epoch 100/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0118 - mae: 0.0754 - val_loss: 0.0042 - val_mae: 0.0190\n",
      "5/5 [==============================] - 0s 856us/step\n",
      "[DEBUG] test_alternative_activations: signals shape: (160,), returns shape: (160,)\n",
      "[DEBUG] signals shape: (160,), returns shape: (160,)\n",
      "Epoch 1/100\n",
      "20/20 [==============================] - 1s 8ms/step - loss: 0.9786 - mae: 0.6743 - val_loss: 0.3046 - val_mae: 0.4435\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5274 - mae: 0.5103 - val_loss: 0.2551 - val_mae: 0.3971\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3671 - mae: 0.4192 - val_loss: 0.2235 - val_mae: 0.3657\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2431 - mae: 0.3155 - val_loss: 0.2032 - val_mae: 0.3428\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2148 - mae: 0.3173 - val_loss: 0.1821 - val_mae: 0.3160\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1560 - mae: 0.2656 - val_loss: 0.1606 - val_mae: 0.2905\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1070 - mae: 0.2280 - val_loss: 0.1445 - val_mae: 0.2706\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0861 - mae: 0.1997 - val_loss: 0.1286 - val_mae: 0.2495\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.0936 - mae: 0.2115 - val_loss: 0.1114 - val_mae: 0.2269\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0646 - mae: 0.1757 - val_loss: 0.0993 - val_mae: 0.2100\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0678 - mae: 0.1786 - val_loss: 0.0835 - val_mae: 0.1907\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.0624 - mae: 0.1770 - val_loss: 0.0690 - val_mae: 0.1706\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0499 - mae: 0.1533 - val_loss: 0.0579 - val_mae: 0.1536\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0379 - mae: 0.1409 - val_loss: 0.0474 - val_mae: 0.1375\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0429 - mae: 0.1484 - val_loss: 0.0382 - val_mae: 0.1238\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0417 - mae: 0.1402 - val_loss: 0.0307 - val_mae: 0.1102\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0380 - mae: 0.1385 - val_loss: 0.0256 - val_mae: 0.0986\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0414 - mae: 0.1381 - val_loss: 0.0209 - val_mae: 0.0880\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0357 - mae: 0.1323 - val_loss: 0.0166 - val_mae: 0.0777\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0284 - mae: 0.1214 - val_loss: 0.0124 - val_mae: 0.0649\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0254 - mae: 0.1087 - val_loss: 0.0101 - val_mae: 0.0562\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0365 - mae: 0.1343 - val_loss: 0.0078 - val_mae: 0.0461\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0256 - mae: 0.1077 - val_loss: 0.0078 - val_mae: 0.0419\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0260 - mae: 0.1054 - val_loss: 0.0056 - val_mae: 0.0310\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0271 - mae: 0.1082 - val_loss: 0.0062 - val_mae: 0.0324\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0261 - mae: 0.1165 - val_loss: 0.0049 - val_mae: 0.0290\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0262 - mae: 0.1157 - val_loss: 0.0049 - val_mae: 0.0291\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0226 - mae: 0.1020 - val_loss: 0.0046 - val_mae: 0.0317\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0212 - mae: 0.1033 - val_loss: 0.0043 - val_mae: 0.0231\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.0257 - mae: 0.1142 - val_loss: 0.0046 - val_mae: 0.0325\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0200 - mae: 0.0990 - val_loss: 0.0048 - val_mae: 0.0347\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0250 - mae: 0.1075 - val_loss: 0.0043 - val_mae: 0.0238\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 0s 10ms/step - loss: 0.0259 - mae: 0.1053 - val_loss: 0.0043 - val_mae: 0.0218\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.0191 - mae: 0.1011 - val_loss: 0.0042 - val_mae: 0.0176\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0207 - mae: 0.1042 - val_loss: 0.0041 - val_mae: 0.0143\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0189 - mae: 0.0980 - val_loss: 0.0044 - val_mae: 0.0201\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0238 - mae: 0.1096 - val_loss: 0.0042 - val_mae: 0.0152\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0191 - mae: 0.0887 - val_loss: 0.0043 - val_mae: 0.0206\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0234 - mae: 0.1003 - val_loss: 0.0041 - val_mae: 0.0148\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0190 - mae: 0.0937 - val_loss: 0.0042 - val_mae: 0.0157\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0167 - mae: 0.0891 - val_loss: 0.0039 - val_mae: 0.0157\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0220 - mae: 0.1015 - val_loss: 0.0040 - val_mae: 0.0159\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0229 - mae: 0.1055 - val_loss: 0.0040 - val_mae: 0.0118\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0137 - mae: 0.0787 - val_loss: 0.0040 - val_mae: 0.0151\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0194 - mae: 0.0959 - val_loss: 0.0040 - val_mae: 0.0152\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0119 - mae: 0.0730 - val_loss: 0.0041 - val_mae: 0.0156\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0168 - mae: 0.0844 - val_loss: 0.0040 - val_mae: 0.0133\n",
      "Epoch 48/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0150 - mae: 0.0796 - val_loss: 0.0040 - val_mae: 0.0138\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0128 - mae: 0.0694 - val_loss: 0.0040 - val_mae: 0.0112\n",
      "Epoch 50/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0127 - mae: 0.0656 - val_loss: 0.0042 - val_mae: 0.0124\n",
      "Epoch 51/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0168 - mae: 0.0840 - val_loss: 0.0039 - val_mae: 0.0108\n",
      "5/5 [==============================] - 0s 887us/step\n",
      "[DEBUG] test_alternative_activations: signals shape: (160,), returns shape: (160,)\n",
      "[DEBUG] signals shape: (160,), returns shape: (160,)\n",
      "Epoch 1/100\n",
      "20/20 [==============================] - 2s 9ms/step - loss: 1.1791 - mae: 0.7664 - val_loss: 0.0749 - val_mae: 0.1271\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6667 - mae: 0.5881 - val_loss: 0.0718 - val_mae: 0.1468\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4824 - mae: 0.4959 - val_loss: 0.0734 - val_mae: 0.1630\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3987 - mae: 0.4521 - val_loss: 0.0546 - val_mae: 0.1547\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2937 - mae: 0.3840 - val_loss: 0.0443 - val_mae: 0.1483\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2615 - mae: 0.3560 - val_loss: 0.0343 - val_mae: 0.1380\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1932 - mae: 0.2968 - val_loss: 0.0285 - val_mae: 0.1259\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1287 - mae: 0.2391 - val_loss: 0.0308 - val_mae: 0.1293\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1077 - mae: 0.2205 - val_loss: 0.0276 - val_mae: 0.1188\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1172 - mae: 0.2305 - val_loss: 0.0263 - val_mae: 0.1144\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.1050 - mae: 0.2120 - val_loss: 0.0238 - val_mae: 0.1047\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0976 - mae: 0.2061 - val_loss: 0.0191 - val_mae: 0.0945\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 0s 10ms/step - loss: 0.1000 - mae: 0.2069 - val_loss: 0.0167 - val_mae: 0.0921\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.0682 - mae: 0.1693 - val_loss: 0.0177 - val_mae: 0.0891\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0631 - mae: 0.1597 - val_loss: 0.0177 - val_mae: 0.0844\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0722 - mae: 0.1746 - val_loss: 0.0158 - val_mae: 0.0772\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0583 - mae: 0.1522 - val_loss: 0.0091 - val_mae: 0.0659\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0747 - mae: 0.1714 - val_loss: 0.0075 - val_mae: 0.0580\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0536 - mae: 0.1437 - val_loss: 0.0074 - val_mae: 0.0541\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0390 - mae: 0.1229 - val_loss: 0.0074 - val_mae: 0.0544\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0376 - mae: 0.1174 - val_loss: 0.0063 - val_mae: 0.0482\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0424 - mae: 0.1279 - val_loss: 0.0065 - val_mae: 0.0542\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0451 - mae: 0.1284 - val_loss: 0.0078 - val_mae: 0.0619\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0475 - mae: 0.1315 - val_loss: 0.0078 - val_mae: 0.0614\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0355 - mae: 0.1110 - val_loss: 0.0070 - val_mae: 0.0545\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0344 - mae: 0.1120 - val_loss: 0.0060 - val_mae: 0.0459\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0409 - mae: 0.1223 - val_loss: 0.0058 - val_mae: 0.0445\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0490 - mae: 0.1352 - val_loss: 0.0062 - val_mae: 0.0472\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0396 - mae: 0.1198 - val_loss: 0.0062 - val_mae: 0.0481\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0321 - mae: 0.1089 - val_loss: 0.0052 - val_mae: 0.0403\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0384 - mae: 0.1152 - val_loss: 0.0055 - val_mae: 0.0413\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0410 - mae: 0.1190 - val_loss: 0.0058 - val_mae: 0.0417\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0356 - mae: 0.1119 - val_loss: 0.0059 - val_mae: 0.0411\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0558 - mae: 0.1432 - val_loss: 0.0071 - val_mae: 0.0446\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0319 - mae: 0.1045 - val_loss: 0.0056 - val_mae: 0.0346\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0406 - mae: 0.1161 - val_loss: 0.0075 - val_mae: 0.0418\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0332 - mae: 0.1034 - val_loss: 0.0056 - val_mae: 0.0333\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0364 - mae: 0.1083 - val_loss: 0.0057 - val_mae: 0.0304\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0256 - mae: 0.0891 - val_loss: 0.0055 - val_mae: 0.0295\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0295 - mae: 0.0975 - val_loss: 0.0057 - val_mae: 0.0299\n",
      "5/5 [==============================] - 0s 846us/step\n",
      "[DEBUG] test_alternative_activations: signals shape: (160,), returns shape: (160,)\n",
      "[DEBUG] signals shape: (160,), returns shape: (160,)\n",
      "\n",
      "Running benchmark comparison...\n",
      "\n",
      "Testing constant long_1m strategy\n",
      "[DEBUG] signals shape: (1000,), returns shape: (1000,)\n",
      "\n",
      "Testing constant short_1m strategy\n",
      "[DEBUG] signals shape: (1000,), returns shape: (1000,)\n",
      "\n",
      "Testing constant long_5m strategy\n",
      "[DEBUG] signals shape: (1000,), returns shape: (1000,)\n",
      "\n",
      "Testing constant short_5m strategy\n",
      "[DEBUG] signals shape: (1000,), returns shape: (1000,)\n",
      "\n",
      "Testing constant hold strategy\n",
      "[DEBUG] signals shape: (1000,), returns shape: (1000,)\n",
      "\n",
      "Running cost sensitivity analysis...\n",
      "\n",
      "Analyzing cost level: 20 bps\n",
      "Epoch 1/100\n",
      "25/25 [==============================] - 2s 8ms/step - loss: nan - mae: 0.6264\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: nan - mae: 0.5639\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.5148\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.4625\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.4516\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.4045\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.3720\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.3427\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.3204\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.2990\n",
      "7/7 [==============================] - 0s 803us/step\n",
      "[DEBUG] signals shape: (200,), returns shape: (200,)\n",
      "\n",
      "Analyzing cost level: 25 bps\n",
      "Epoch 1/100\n",
      "25/25 [==============================] - 1s 2ms/step - loss: nan - mae: 0.6703\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.6193\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.5636\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.5333\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.5024\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.4779\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.4402\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.3959\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.3755\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.3570\n",
      "7/7 [==============================] - 0s 790us/step\n",
      "[DEBUG] signals shape: (200,), returns shape: (200,)\n",
      "\n",
      "Analyzing cost level: 30 bps\n",
      "Epoch 1/100\n",
      "25/25 [==============================] - 1s 2ms/step - loss: nan - mae: 0.6353\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.5787\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.5354\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: nan - mae: 0.4843\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.4502\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.4326\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.3974\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.3864\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: nan - mae: 0.3422\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.3353\n",
      "7/7 [==============================] - 0s 835us/step\n",
      "[DEBUG] signals shape: (200,), returns shape: (200,)\n",
      "\n",
      "Analyzing cost level: 35 bps\n",
      "Epoch 1/100\n",
      "25/25 [==============================] - 2s 2ms/step - loss: nan - mae: 0.6631\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.6181\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.5432\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.5057\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.4832\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.4352\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.4190\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.3979\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.3642\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.3501\n",
      "7/7 [==============================] - 0s 830us/step\n",
      "[DEBUG] signals shape: (200,), returns shape: (200,)\n",
      "\n",
      "Analyzing cost level: 40 bps\n",
      "Epoch 1/100\n",
      "25/25 [==============================] - 1s 2ms/step - loss: nan - mae: 0.6586\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.5732\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.5269\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.4843\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: nan - mae: 0.4620\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.4231\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.3934\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.3690\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.3423\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 1ms/step - loss: nan - mae: 0.3345\n",
      "7/7 [==============================] - 0s 757us/step\n",
      "[DEBUG] signals shape: (200,), returns shape: (200,)\n",
      "\n",
      "Results have been saved to:\n",
      "- Non-contiguous fold results: data/robust_output/non_contiguous_*\n",
      "- Activation function results: data/robust_output/activation_*\n",
      "- Benchmark comparison results: data/robust_output/benchmark_*\n",
      "- Cost sensitivity results: data/robust_output/cost_sensitivity_*\n",
      "\n",
      "Phase 8 completed successfully!\n"
     ]
    }
   ],
   "source": [
    "run_robustness_checks() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![plot](./figures/non_contiguous_analysis.png)\n",
    "\n",
    "![plot](./figures/benchmark_comparison.png)\n",
    "\n",
    "![plot](./figures/activation_comparison.png)\n",
    "## 13. Overfitting Assessment\n",
    "\n",
    "To evaluate the potential for overfitting in our replication, we compare in-sample and out-of-sample performance metrics from the 10-fold cross-validation (Figure 4). Key observations:\n",
    "\n",
    "* **Sharpe Ratio Stability:** The average in-sample Sharpe ratio (~~–0.042) closely matches the out-of-sample Sharpe (~~–0.042) across all folds, indicating the model’s predictive power generalizes rather than collapsing outside the training set.\n",
    "* **Return and Risk Consistency:** Average returns, standard deviation of returns, and maximum drawdown metrics remain nearly identical in- and out-of-sample, further suggesting minimal data snooping or parameter overfitting.\n",
    "* **Neural Training Behavior:** The training history (Figure 3) shows gradual decline in loss and MAE without severe divergence between training and validation curves until late epochs, implying robust model fitting without large generalization gaps.\n",
    "\n",
    "**Conclusion:** Despite overall negative performance (reflecting the proxy data and simplified assumptions), the consistency of metrics across folds and epochs suggests that the implementation does not suffer from overfitting. Any further performance degradation is more likely driven by structural data limitations (e.g., using VIX index instead of true futures) rather than over-parameterization.\n",
    "\n",
    "## 14. Conclusions & Opportunities for Further Research\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "* We successfully replicated the core methodological pipeline: term-structure VAR modeling, simulated signal generation, deep‐learning approximation, cross-validation backtests, and transaction‐cost sensitivity.\n",
    "* Our results—while quantitatively different from Avellaneda et al. (e.g., negative Sharpe due to proxy data)—exhibit the same qualitative behavior: signal consistency, robustness to cross-validation folds, and sensitivity to transaction costs.\n",
    "* The replication confirms the feasibility of the original framework and highlights the critical importance of using accurate futures data and realistic cost assumptions.\n",
    "\n",
    "**Opportunities for Further Research:**\n",
    "\n",
    "1. **Use Actual CBOE Futures Data:** Replace the VIX index proxy with full term-structure data to capture realistic yield curves and improve model fidelity.\n",
    "2. **Enhanced Utility Specifications:** Explore alternative utility functions (e.g., mean–variance, prospect theory–inspired) and dynamic risk-aversion parameters.\n",
    "3. **Alternative Neural Architectures:** Test recurrent architectures (LSTM/GRU) or attention-based models that can better capture temporal dependencies in the term structure.\n",
    "4. **Regime-Switching Extensions:** Integrate macroeconomic or sentiment indicators to allow the VAR and neural networks to adapt to volatility regime changes.\n",
    "5. **Intraday and High-Frequency Signals:** Extend the framework to intraday futures tick data for more granular signal timing and tighter cost modeling.\n",
    "6. **Portfolio-Level Applications:** Incorporate signals into multi-asset portfolios—e.g., combining VIX signals with equity or commodity volatility strategies—and study diversification benefits.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
